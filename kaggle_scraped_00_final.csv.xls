title,price,launch,close,tags,teams,competitors,entries,description,files,size,points,tiers,difficulty
1st and Future - Player Contact Detection,"$100,000 ",2 days ago,,"['health', 'tabular', 'football', 'video data', 'matthewscorrelationcoefficient']",32,32,77,"Goal of the Competition
The goal of this competition is to detect external contact experienced by players during an NFL football game. You will use video and player tracking data to identify moments with contact to help improve player safety.
Context
The National Football League (NFL) has teamed up with Amazon Web Services (AWS) to strengthen its commitment to predict player injuries. The NFL aspires to have the best injury surveillance and mitigation program in any sport. With your machine learning and computer vision skills, you can help the NFL accurately identify when players experience contact throughout a football play.
In prior years, the NFL challenged the Kaggle community to create helmet impact detection and identification algorithms. This year the NFL looks to automatically identify all moments when players experience contact. This competition will be successful if we can reliably detect moments when players are in contact with one another and when a player’s body is in contact with the ground.
Currently, the NFL uses its tracking system to monitor a large number of statistics about players’ load during the season. The league has a solution that predicts contact between players, but it only leverages the player tracking data. This competition hopes to improve the predictive power by including video in addition to tracking data. Categorizing ground contact will also provide a more comprehensive view of impacts, improving analysis for player health and safety.
More accurate data is an important step toward the NFL’s injury surveillance and mitigation goals. With complete contact detection, the league can identify correlations between certain types of contact and injury, a contributor to future prevention. Your efforts could help mitigate unsafe situations to reduce injury to all players.
The National Football League is America's most popular sports league. Founded in 1920, the NFL developed the model for the successful modern sports league and is committed to advancing progress in the diagnosis, prevention, and treatment of sports-related injuries. This competition is part of the Digital Athlete, a joint effort between the NFL and AWS to build a virtual, 360-degree representation of an NFL player’s experience. The Digital Athlete hopes to generate a precise picture of what they need when it comes to preventing and recovering from injuries while performing at their best. Health and safety efforts include support for independent medical research and engineering advancements as well as a commitment to work to better protect players and make the game safer, including enhancements to medical protocols and improvements to how our game is taught and played. For more information about the NFL's health and safety efforts, please visit the NFL Player Health and Safety website.

This is a Code Competition. Refer to Code Requirements for details.",734 files,5.01 GB,This competition awards ranking points,This competition counts towards tiers,Featured Code Competition
Santa 2022 - The Christmas Card Conundrum,"$50,000 ",7 days ago,,"['optimization', 'holidays and cultural events', 'custom metric']",277,300,"1,078","This Christmas season, we’re printing in house.
Our vendor’s supply eaten by a field mouse!
The paper was laid on the printer with care
In hopes that the arms soon would reach there;
The elves are nestled all snug in their pods;
In hopes that the picture would earn some applause.
Can you help move the arms and put them in place?
Please optimize this configuration space!
Santa’s elves relied on the same vendor every year to print the annual Christmas card. But a mouse ate through the printer cables and the elves are forced to print at the North Pole Workshop this year! They managed to create their own giant printer with an 8-axis robotic arm that can print one pixel of the card at a time. But moving the arm and changing the color is not only expensive, the elves need to spend the least amount of time possible making the card so they can get back to making toys!
Your job is to determine the most optimal way to craft this year’s Christmas card, by selecting the most efficient path of both moving the robotic arm and changing the print color to craft this year’s image. Each link of the printer arm can be moved independently each step, but you'll also need to account for the time needed to change the printing color.
Can you save Christmas by finding the most efficient way to print Santa’s Christmas cards?
Photos by Kelly Sikkema and Eric Prouzet on Unsplash",3 files,10.04 MB,This competition awards ranking points,This competition counts towards tiers,Featured Prediction Competition
RSNA Screening Mammography Breast Cancer Detection,"$50,000 ",9 days ago,,"['binary classification', 'image classification', 'probfscorebetamicro']",259,280,"1,524","Goal of the Competition
The goal of this competition is to identify breast cancer. You'll train your model with screening mammograms obtained from regular screening.
Your work improving the automation of detection in screening mammography may enable radiologists to be more accurate and efficient, improving the quality and safety of patient care. It could also help reduce costs and unnecessary medical procedures.
Context
According to the WHO, breast cancer is the most commonly occurring cancer worldwide. In 2020 alone, there were 2.3 million new breast cancer diagnoses and 685,000 deaths. Yet breast cancer mortality in high-income countries has dropped by 40% since the 1980s when health authorities implemented regular mammography screening in age groups considered at risk. Early detection and treatment are critical to reducing cancer fatalities, and your machine learning skills could help streamline the process radiologists use to evaluate screening mammograms.
Currently, early detection of breast cancer requires the expertise of highly-trained human observers, making screening mammography programs expensive to conduct. A looming shortage of radiologists in several countries will likely worsen this problem. Mammography screening also leads to a high incidence of false positive results. This can result in unnecessary anxiety, inconvenient follow-up care, extra imaging tests, and sometimes a need for tissue sampling (often a needle biopsy).
The competition host, the Radiological Society of North America (RSNA) is a non-profit organization that represents 31 radiologic subspecialties from 145 countries around the world. RSNA promotes excellence in patient care and health care delivery through education, research, and technological innovation.
Your efforts in this competition could help extend the benefits of early detection to a broader population. Greater access could further reduce breast cancer mortality worldwide.

This is a Code Competition. Refer to Code Requirements for details.",54713 files,314.72 GB,This competition awards ranking points,This competition counts towards tiers,Featured Code Competition
OTTO – Multi-Objective Recommender System,"$30,000 ",a month ago,,"['retail and shopping', 'recommender systems', 'weightedrecall@{k}']","1,249","1,361","7,526","Goal of the Competition
The goal of this competition is to predict e-commerce clicks, cart additions, and orders. You'll build a multi-objective recommender system based on previous events in a user session.
Your work will help improve the shopping experience for everyone involved. Customers will receive more tailored recommendations while online retailers may increase their sales.
Context
Online shoppers have their pick of millions of products from large retailers. While such variety may be impressive, having so many options to explore can be overwhelming, resulting in shoppers leaving with empty carts. This neither benefits shoppers seeking to make a purchase nor retailers that missed out on sales. This is one reason online retailers rely on recommender systems to guide shoppers to products that best match their interests and motivations. Using data science to enhance retailers' ability to predict which products each customer actually wants to see, add to their cart, and order at any given moment of their visit in real-time could improve your customer experience the next time you shop online with your favorite retailer.
Current recommender systems consist of various models with different approaches, ranging from simple matrix factorization to a transformer-type deep neural network. However, no single model exists that can simultaneously optimize multiple objectives. In this competition, you’ll build a single entry to predict click-through, add-to-cart, and conversion rates based on previous same-session events.
With more than 10 million products from over 19,000 brands, OTTO is the largest German online shop. OTTO is a member of the Hamburg-based, multi-national Otto Group, which also subsidizes Crate & Barrel (USA) and 3 Suisses (France).
Your work will help online retailers select more relevant items from a vast range to recommend to their customers based on their real-time behavior. Improving recommendations will ensure navigating through seemingly endless options is more effortless and engaging for shoppers.",3 files,11.89 GB,This competition awards ranking points,This competition counts towards tiers,Featured Prediction Competition
Tabular Playground Series - Nov 2022,,a month ago,7 days ago,"['tabular', 'ensembling', 'logloss']",689,717,"7,260","You may have heard that blending predictions from model predictions can give better results than using the output of a single model. There are many different strategies that can be employed for this, and they are great to learn if you're looking for an effectively free boost in model scores. The November Tabular Playground is the chance to practice this skill!
About the Tabular Playground Series
Kaggle competitions are incredibly fun and rewarding, but they can also be intimidating for people who are relatively new in their data science journey. In the past, we've launched many Playground competitions that are more approachable than our Featured competitions and thus, more beginner-friendly.
The goal of these competitions is to provide a fun and approachable-for-anyone tabular dataset to model. These competitions are a great choice for people looking for something in between the Titanic Getting Started competition and the Featured competitions. If you're an established competitions master or grandmaster, these probably won't be much of a challenge for you; thus, we encourage you to avoid saturating the leaderboard.
For each monthly competition, we'll be offering Kaggle Merchandise for the top three teams. And finally, because we want these competitions to be more about learning, we're limiting team sizes to 3 individuals.
Getting Started
For ideas on how to improve your score, check out the Intro to Machine Learning and Intermediate Machine Learning courses on Kaggle Learn.
Good luck and have fun!
Photo by RhondaK Native Florida Folk Artist on Unsplash",5002 files,2.9 GB,This competition did not award ranking points,This competition did not count towards tiers,Playground Prediction Competition
Scrabble Player Rating,,a month ago,8 days,"['tabular', 'rmse']",180,194,883,"Are you a Kaggle Scrabble Grandmaster? In the second edition of this Competition featuring data from Woogles.io, participants are challenged to predict the ratings of players based on Scrabble gameplay. The evaluation algorithm is RMSE.
Find inspiration from the first Competition based on Woogles.io data where participants were challenged to predict the point value of the 20th turn from Scrabble games.
How to participate
Log in or create a Kaggle account
Accept the competition's rules
Download the data (click on the ""Data"" tab) or create a notebook directly on Kaggle (click on the ""Code"" tab)
Train a model on train.csv, games.csv, and turns.csv predicting the missing (non-bot) player rating in test.csv
Generate a submission.csv file in the same format as the sample_submission.csv
Upload your submission.csv file on ""My Submissions""
You get TWO SUBMISSIONS per day. They must be submitted before the end of the competition deadline. You can hand select up to two submissions from the ""My Submissions"" tab, otherwise the submission with the best public score (on 30% of the test data) will be taken and scored on the private leaderboard (on 70% of the test data).
Acknowledgements
Thank you to woogles.io for providing their platform for playing Scrabble online. Woogles is an entirely volunteer-run 501(c)(3) non-profit. If you enjoy the site, please feel free to contribute by clicking ""Want to help?"" at https://woogles.io/.",4 files,122.18 MB,This competition does not award ranking points,This competition does not count towards tiers,Playground Prediction Competition
NFL Big Data Bowl 2023,"$100,000 ",2 months ago,a month,,,,,"Goal of the Competition
The National Football League (NFL) is back with another Big Data Bowl, where contestants use Next Gen Stats player tracking data to generate actionable, creative, and novel stats. Previous iterations have considered running backs, defensive backs, and special teams, and have generated metrics that have been used on television and by NFL teams. In this year’s competition, you’ll have more subtle performances to consider—and potentially more players to measure.
2023 Theme: Linemen on Pass Plays
Quarterbacks may get the glory, but some of the most important work takes place a few feet in front of them. The offensive line protects the passer, providing precious seconds to find receivers downfield. At the same time, the opposing team’s defensive line attempts to find a disruptive path. If a defender sneaks through, it can mean a sack, a blocked pass, or even a turnover. Some of the game’s most important plays happen on the line and this competition examines the data behind the hardest workers in football.
In this competition, you’ll have access to the NFL’s Next Gen Stats data, including player tracking, play, game, and player information, as well as Pro Football Focus (PFF) scouting data for 2021 passing plays (Weeks 1-8 of the NFL season). You’ll create new metrics and stats for America's most popular sports league. Notebook submissions will be scored based on five components: innovation, accuracy, relevance, clarity, and data visualization.
Winners will be invited to present their results to the NFL, where one competition team will receive an additional prize. The most useful new metrics or analysis could be also used by NFL teams to evaluate their offensive and defensive lines.",12 files,965.07 MB,,,Analytics Competition
2022 Kaggle Machine Learning & Data Science Survey,"$30,000 ",2 months ago,10 days ago,"['data analytics', 'online communities', 'survey analysis']",,,,"Welcome to Kaggle's annual Machine Learning and Data Science Survey competition! You can read our executive summary here.
This year, as in 2017, 2018, 2019, 2020, and 2021, we set out to conduct an industry-wide survey that presents a truly comprehensive view of the state of data science and machine learning. The survey was live from 09/16/2022 to 10/16/2022, and after cleaning the data we finished with 23,997 responses!
There's a lot to explore here. The results include raw numbers about who is working with data, what’s happening with machine learning in different industries, and the best ways for new data scientists to break into the field. We've published all of the data rather than only the aggregated survey results, which makes it an unusual example of a survey dataset, as it allows analysts to investigate the data on their own.
This year Kaggle is once again launching an annual Data Science Survey Challenge, where we will be awarding a prize pool of $30,000 to notebook authors who tell a rich story about a subset of the data science and machine learning community.
In our sixth year running this survey, we were once again awed by the global, diverse, and dynamic nature of the data science and machine learning industry. This survey data EDA provides an overview of the industry on an aggregate scale, but it also leaves us wanting to know more about the many specific communities comprised within the survey, and how they have changed from year over year. For that reason, we’re inviting the Kaggle community to dive deep into the survey datasets and help us tell the diverse stories of data scientists from around the world.
The challenge objective: tell a data story about a subset of the data science community represented in this survey, through a combination of both narrative text and data exploration. A “story” could be defined any number of ways, and that’s deliberate. The challenge is to deeply explore (through data) the impact, priorities, or concerns of a specific group of data science and machine learning practitioners. That group can be defined in the macro (for example: anyone who does most of their coding in Python) or the micro (for example: female data science students studying machine learning in masters programs). This is an opportunity to be creative and tell the story of a community you identify with or are passionate about!
Submissions will be evaluated on the following:
Composition - Is there a clear narrative thread to the story that’s articulated and supported by data? The subject should be well defined, well researched, and well supported through the use of data and visualizations.
Originality - Does the reader learn something new through this submission? Or is the reader challenged to think about something in a new way? A great entry will be informative, thought provoking, and fresh all at the same time.
Documentation - Are your code, and notebook, and additional data sources well documented so a reader can understand what you did? Are your sources clearly cited? A high quality analysis should be concise and clear at each step so the rationale is easy to follow and the process is reproducible
To be valid, a submission must be contained in one notebook, made public on or before the submission deadline. Participants are free to use any datasets in addition to the Kaggle Data Science survey, but those datasets must also be publicly available on Kaggle by the deadline for a submission to be valid. You can make your submission by filling out the submission form.",3 files,26.2 MB,,,Analytics Competition
G2Net Detecting Continuous Gravitational Waves,"$25,000 ",2 months ago,,"['astronomy', 'signal processing', 'auc']",774,892,"17,238","Goal of the Competition
The goal of this competition is to find continuous gravitational-wave signals. You will develop a model sensitive enough to detect weak yet long-lasting signals emitted by rapidly-spinning neutron stars within noisy data.
Your work will help scientists detect something new: a second class of gravitational waves! The first gravitational wave discoveries earned a Nobel Prize. Further study of these waves may enable scientists to learn about the structure of the most extreme stars in our universe.
Context
When scientists detected the first class of gravitational waves in 2015, they expected the discoveries to continue. There are four classes, yet at present only signals from merging black holes and neutron stars have been detected. Among those remaining are continuous gravitational-wave signals. These are weak yet long-lasting signals emitted by rapidly-spinning neutron stars. Imagine the mass of our Sun but condensed into a ball the size of a city and spinning over 1,000 times a second. The extreme compactness of these stars, composed of the densest material in the universe, could allow continuous waves to be emitted and then detected on Earth. There are potentially many continuous signals from neutron stars in our own galaxy and the current challenge for scientists is to make the first detection, and hopefully data science can help with this mission.
This image, taken from a 2021 paper by the LIGO-Virgo-KAGRA collaboration, shows the maximum amplitude of a continuous wave any of these neutron stars could emit without being found by the search analyses. Circled stars show results constraining the physical properties of specific neutron stars. Traditional approaches to detecting these weak and hard-to-find continuous signals are based on matched-filtering variants. Scientists create a bank of possible signal waveform templates and ask how correlated each waveform is with the measured noisy data. High correlation is consistent with the presence of a signal similar to that waveform. Due to the long duration of these signals, banks could easily contain hundreds of quintillions of templates; yet, with so many possible waveforms, scientists don’t have the computational power to use the approach without making approximations that weaken the sensitivity to the signals.
G2Net is a network of Gravitational Wave, Geophysics and Machine Learning. Via an Action from COST (European Cooperation in Science and Technology), a funding agency for research and innovation networks, G2Net aims to create a broad network of scientists. From four different areas of expertise, namely GW physics, Geophysics, Computing Science and Robotics, these scientists have agreed on a common goal of tackling challenges in data analysis and noise characterization for GW detectors.
By helping G2Net in this challenge you'll enable scientists to improve their sensitivity, leading to new discoveries in the field. As a result, scientists could learn more about the structure of the most extreme stars in our universe.
Resources
Resources for the generation of background noise and continuous gravitational-wave signals can be found in this pinned discussion. A brief notebook summarizing the very basics of generating data using PyFstat is also provided.
Acknowledgments
We acknowledge support from the LIGO-Virgo-Kagra Collaboration of which the hosts are members. Specifically we acknowledge the use of the Gravitational Wave Open Science Centre (GWOSC) and the software resources lalsuite and PyFstat.
This challenge can be cited using the BibTeX entry attached below, should any of the results
here generated be useful for any specific research results:
@techreport{G2NetCWKaggleChallenge,
    author = ""Tenorio, Rodrigo and Williams, Michael J. and Messenger, Chris"",
    title = ""Learning to detect continuous gravitational waves"",
    number = ""LIGO-P2200295"",
    url = {https://dcc.ligo.org/P2200295},
    year = ""2022""
}
   ",8580 files,227.21 GB,This competition awards ranking points,This competition counts towards tiers,Research Prediction Competition
Lux AI 2022 - Beta,,2 months ago,13 days,"['simulations', 'custom metric']",18,18,39,"Please note - this is a preview/beta launch of an upcoming competition. This competition is intended to help with rule balancing and establishing a fair and fun competition, soon to be launched. As such, this competition does not have cash prizes, points, or medals - but we hope to gain your feedback to help make the upcoming competition as fun as possible!
Introduction
As the sun set on the world an array of lights dotted the once dark horizon. With the help of a brigade of toads, Lux had made it past the terrors in the night to see the dawn of a new age. Seeking new challenges, plans were made to send a forward force with one mission: terraform Mars!
Welcome to the Lux AI Challenge Season 2!
The Lux AI Challenge is a competition where competitors design agents to tackle a multi-variable optimization, resource gathering, and allocation problem in a 1v1 scenario against other competitors. In addition to optimization, successful agents must be capable of analyzing their opponents and developing appropriate policies to get the upper hand.
All code can be found at our Github, make sure to give it a star while you are there!
Make sure to join our community discord to chat, strategize, and learn with other competitors! We will be posting announcements on the Kaggle Forums and on the discord.",,,This competition does not award ranking points,This competition does not count towards tiers,Playground Simulation Competition
Tabular Playground Series - Oct 2022,,2 months ago,a month ago,"['tabular', 'video games', 'logistic regression', 'meancolumnwiselogloss']",463,500,"4,659","This may be one of the most challenging Tabular Playground competitions to date! It just so happens that one of Kaggle's software engineers is an avid Rocket League player and he's assembled a dataset of Rocket League gameplay for this month's TPS.
This month's challenge is to predict the probability of each team scoring within the next 10 seconds of the game given a snapshot from a Rocket League match. Sounds awesome, right?
Well, it's not that simple. The training data is fairly large; trying to read and model it in a single go might pose some challenges. The purpose of this month's competition is for you to explore ways you can take a big dataset and make it manageable within the time and resources you have. For most people, typical brute force approaches aren't going to work well.
Can you scale down the dataset?
Can you use, e.g., online learning methods that allow you to train from the data one row at a time? (FTLR is a great place to start if you're not familiar with online learning! e.g., this notebook)
Can you figure out a nice set of features to reduce the dataset down to?
In addition to that challenge, while your predictions must be made pointwise, the training data is made up of timeseries—maybe you can use that temporal information to improve your model? This competition also has plenty of opportunity for data visualizations. Let's see some pretty graphs!
So, share your ideas about tackling this beast of a dataset and have a great time!
Acknowledgments
This competition includes Rocket League data and images from the Rocket League Community Tournament Assets.",14 files,9.77 GB,This competition did not award ranking points,This competition did not count towards tiers,Playground Prediction Competition
Novozymes Enzyme Stability Prediction,"$25,000 ",3 months ago,,"['chemistry', 'spearmanr']","1,892","2,125","27,508","Goal of the Competition
Enzymes are proteins that act as catalysts in the chemical reactions of living organisms. The goal of this competition is to predict the thermostability of enzyme variants. The experimentally measured thermostability (melting temperature) data includes natural sequences, as well as engineered sequences with single or multiple mutations upon the natural sequences.
Understanding and accurately predict protein stability is a fundamental problem in biotechnology. Its applications include enzyme engineering for addressing the world’s challenges in sustainability, carbon neutrality and more. Improvements to enzyme stability could lower costs and increase the speed scientists can iterate on concepts.
Context
Novozymes finds enzymes in nature and optimizes them for use in industry. In industry, enzymes replace chemicals and accelerate production processes. They help our customers make more from less, while saving energy and generating less waste. Enzymes are widely used in laundry and dishwashing detergents where they remove stains and enable low-temperature washing and concentrated detergents. Other enzymes improve the quality of bread, beer and wine, or increase the nutritional value of animal feed. Enzymes are also used in the production of biofuels where they turn starch or cellulose from biomass into sugars which can be fermented to ethanol. These are just a few examples as we sell enzymes to more than 40 different industries. Like enzymes, microorganisms have natural properties that can be put to use in a variety of processes. Novozymes supplies a range of microorganisms for use in agriculture, animal health and nutrition, industrial cleaning and wastewater treatment.
However, many enzymes are only marginally stable, which limits their performance under harsh application conditions. Instability also decreases the amount of protein that can be produced by the cell. Therefore, the development of efficient computational approaches to predict protein stability carries enormous technical and scientific interest.
Computational protein stability prediction based on physics principles have made remarkable progress thanks to advanced physics-based methods such as FoldX, Rosetta, and others. Recently, many machine learning methods were proposed to predict the stability impact of mutations on protein based on the pattern of variation in natural sequences and their three dimensional structures. More and more protein structures are being solved thanks to the recent breakthrough of AlphaFold2. However, accurate prediction of protein thermal stability remains a great challenge.
In this competition, Novozymes invites you to develop a model to predict/rank the thermostability of enzyme variants based on experimental melting temperature data, which is obtained from Novozymes’s high throughput screening lab. You’ll have access to data from previous scientific publications. The available thermostability data spans from natural sequences to engineered sequences with single or multiple mutations upon the natural sequences. If successful, you'll help tackle the fundamental problem of improving protein stability, making the approach to design novel and useful proteins, like enzymes and therapeutics, more rapidly and at lower cost.
Novozymes is the world’s leading biotech powerhouse. Our growing world is faced with pressing needs, emphasizing the necessity for solutions that can ensure the health of the planet and its population. At Novozymes, we believe biotech is at the core of connecting those societal needs with the challenges and opportunities our customers face. Novozymes is the global market leader in biological solutions, producing a wide range of enzymes, microorganisms, technical and digital solutions which help our customers, amongst other things, add new features to their products and produce more from less.
Together, we find biological answers for better lives in a growing world. Let’s Rethink Tomorrow. This is Novozymes’ purpose statement. Novozymes strives to have great impact by balancing good business for our customers and our company, while spearheading environmental and social change. In 2021, Novozymes enabled savings of 60 million tons of CO2 in global transport.",5 files,16.36 MB,This competition awards ranking points,This competition counts towards tiers,Featured Prediction Competition
Tabular Playground Series - Sep 2022,,3 months ago,2 months ago,"['tabular', 'smape']","1,381","1,447","13,085","The competing Kaggle merchandise stores we saw in January's Tabular Playground are at it again. This time, they're selling books!
The task for this month's competitions is a bit more complicated. Not only are there six countries and four books to forecast, but you're being asked to forecast sales during the tumultuous year 2021. Can you use your data science skills to predict book sales when conditions are far from the ordinary?
About the Tabular Playground Series
Kaggle competitions are incredibly fun and rewarding, but they can also be intimidating for people who are relatively new in their data science journey. In the past, we've launched many Playground competitions that are more approachable than our Featured competitions and thus, more beginner-friendly.
The goal of these competitions is to provide a fun and approachable-for-anyone tabular dataset to model. These competitions are a great choice for people looking for something in between the Titanic Getting Started competition and the Featured competitions. If you're an established competitions master or grandmaster, these probably won't be much of a challenge for you; thus, we encourage you to avoid saturating the leaderboard.
For each monthly competition, we'll be offering Kaggle Merchandise for the top three teams. And finally, because we want these competitions to be more about learning, we're limiting team sizes to 3 individuals.
Getting Started
For ideas on how to improve your score, check out the Intro to Machine Learning and Intermediate Machine Learning courses on Kaggle Learn.
Good luck and have fun!
Photo above by Aron Visuals on Unsplash",3 files,5.73 MB,This competition did not award ranking points,This competition did not count towards tiers,Playground Prediction Competition
Feedback Prize - English Language Learning,"$55,000 ",3 months ago,,"['education', 'nlp', 'primary and secondary schools', 'custom metric']","2,654","3,273","49,503","Goal of the Competition
The goal of this competition is to assess the language proficiency of 8th-12th grade English Language Learners (ELLs). Utilizing a dataset of essays written by ELLs will help to develop proficiency models that better supports all students.
Your work will help ELLs receive more accurate feedback on their language development and expedite the grading cycle for teachers. These outcomes could enable ELLs to receive more appropriate learning tasks that will help them improve their English language proficiency.
Context
Writing is a foundational skill. Sadly, it's one few students are able to hone, often because writing tasks are infrequently assigned in school. A rapidly growing student population, students learning English as a second language, known as English Language Learners (ELLs), are especially affected by the lack of practice. While automated feedback tools make it easier for teachers to assign more writing tasks, they are not designed with ELLs in mind.
Existing tools are unable to provide feedback based on the language proficiency of the student, resulting in a final evaluation that may be skewed against the learner. Data science may be able to improve automated feedback tools to better support the unique needs of these learners.
Competition host Vanderbilt University is a private research university in Nashville, Tennessee. It offers 70 undergraduate majors and a full range of graduate and professional degrees across 10 schools and colleges, all on a beautiful campus—an accredited arboretum—complete with athletic facilities and state-of-the-art laboratories. Vanderbilt is optimized to inspire and nurture cross-disciplinary research that fosters discoveries that have global impact. Vanderbilt and co-host, The Learning Agency Lab, an independent nonprofit based in Arizona, are focused on developing science of learning-based tools and programs for social good.
Vanderbilt and The Learning Agency Lab have partnered together to offer data scientists the opportunity to support ELLs using data science skills in machine learning, natural language processing, and educational data analytics. You can improve automated feedback tools for ELLs by sensitizing them to language proficiency. The resulting tools could serve teachers by alleviating the grading burden and support ELLs by ensuring their work is evaluated within the context of their current language level.
Acknowledgments
Vanderbilt University and the Learning Agency Lab would like to thank the Bill & Melinda Gates Foundation, Schmidt Futures, and Chan Zuckerberg Initiative for their support in making this work possible.
                         
This is a Code Competition. Refer to Code Requirements for details.",3 files,9.3 MB,This competition awarded ranking points,This competition counted towards tiers,Featured Code Competition
Open Problems - Multimodal Single-Cell Integration,"$25,000 ",4 months ago,,"['tabular', 'genetics', 'biotechnology', 'meanpearson']","1,220","1,602","27,149","Goal of the Competition
The goal of this competition is to predict how DNA, RNA, and protein measurements co-vary in single cells as bone marrow stem cells develop into more mature blood cells. You will develop a model trained on a subset of 300,000-cell time course dataset of CD34+ hematopoietic stem and progenitor cells (HSPC) from four human donors at five time points generated for this competition by Cellarity, a cell-centric drug creation company.
In the test set, taken from an unseen later time point in the dataset, competitors will be provided with one modality and be tasked with predicting a paired modality measured in the same cell. The added challenge of this competition is that the test data will be from a later time point than any time point in the training data.
Your work will help accelerate innovation in methods of mapping genetic information across layers of cellular state. If we can predict one modality from another, we may expand our understanding of the rules governing these complex regulatory processes.
Context
In the past decade, the advent of single-cell genomics has enabled the measurement of DNA, RNA, and proteins in single cells. These technologies allow the study of biology at an unprecedented scale and resolution. Among the outcomes have been detailed maps of early human embryonic development, the discovery of new disease-associated cell types, and cell-targeted therapeutic interventions. Moreover, with recent advances in experimental techniques it is now possible to measure multiple genomic modalities in the same cell.
While multimodal single-cell data is increasingly available, data analysis methods are still scarce. Due to the small volume of a single cell, measurements are sparse and noisy. Differences in molecular sampling depths between cells (sequencing depth) and technical effects from handling cells in batches (batch effects) can often overwhelm biological differences. When analyzing multimodal data, one must account for different feature spaces, as well as shared and unique variation between modalities and between batches. Furthermore, current pipelines for single-cell data analysis treat cells as static snapshots, even when there is an underlying dynamical biological process. Accounting for temporal dynamics alongside state changes over time is an open challenge in single-cell data science.
Generally, genetic information flows from DNA to RNA to proteins. DNA must be accessible (ATAC data) to produce RNA (GEX data), and RNA in turn is used as a template to produce protein (ADT data). These processes are regulated by feedback: for example, a protein may bind DNA to prevent the production of more RNA. This genetic regulation is the foundation for dynamic cellular processes that allow organisms to develop and adapt to changing environments. In single-cell data science, dynamic processes have been modeled by so-called pseudotime algorithms that capture the progression of the biological process. Yet, generalizing these algorithms to account for both pseudotime and real time is still an open problem.
Competition host Open Problems in Single-Cell Analysis is an open-source, community-driven effort to standardize benchmarking of single-cell methods. The core efforts of Open Problems include the formalization of existing challenges into measurable tasks, a collection of high-quality datasets, centralized benchmarking of community-contributed methods, and community-focused events that bring together diverse method developers to improve single-cell algorithms. They're excited to be partnering with Cellarity, Chan Zuckerbeg Biohub, the Chan Zuckerberg Initiative, Helmholtz Munich, and Yale to see what progress can be made in predicting changes in genetic dynamics over time through interdisciplinary collaboration.
There are approximately 37 trillion cells in the human body, all with different behaviors and functions. Understanding how a single genome gives rise to a diversity of cellular states is the key to gaining mechanistic insight into how tissues function or malfunction in health and disease. You can help solve this fundamental challenge for single-cell biology. Being able to solve the prediction problems over time may yield new insights into how gene regulation influences differentiation as blood and immune cells mature.
Competition header image by Pawel Czerwinski on Unsplash",11 files,28.84 GB,This competition awarded ranking points,This competition counted towards tiers,Featured Prediction Competition
AI Village Capture the Flag @ DEFCON,"$25,000 ",4 months ago,3 months ago,"['games', 'puzzles', 'adversarial learning', 'custom metric']",668,668,"4,235","Help Henry Hacker get to Homecoming during DEFCON30 -- Brought to you by the AI Village! In this series of challenges, you'll be interacting with various machine learning security challenges.
The competition will be live from August 11th to September 12th @ 12:00. If you're in Vegas, stop by the village to chat about the competition. There's also the Kaggle Discussion Board and Discord.
Process
This capture-the-flag (CTF) follows a different flow than most Kaggle competitions. Competitors will be interacting with API endpoints or code/objects stored in the input directory during each of the challenges. Upon successful completion of a challenge, that challenge will return a flag (unique-to-you strings with a length of 128 characters). To update the scoreboard, competitors will submit a .csv containing all of their flags -- see the kaggle documentation or contact the competition organizers for help. Cumulative scores will be weighted based on the difficulty of the challenge. All competitors start at 0 and work their way towards a perfect score of 1.0. There are 22 challenges, ensure your submission.csv has exactly those 22 challenge rows.
NOTE: The template notebook is just a convenience function and method for submitting flags to the scoreboard. Don't feel constrained to that single operating environment. Interact with the challenges from your local host or any other machine that can access the internet. Afterwards, you can transport your flags into the notebook to update the scoreboard. The template is available here.
NOTE: If you want to interact with online challenges through Kaggle (using the template notebook, for instance), you may need to verify your Kaggle account using a phone number.
Here is a handy link to Kaggle's competition documentation, which includes, among other things, instructions on submitting predictions.
Paranoid? If you don't have a kaggle account and don't want to make one, let us know and we can give you instructions for playing the challenges from your own machine. You won't be able to contribute to the scoreboard, but you'll know when you get the right flag.
Please do not try and hack any infrastructure or share flags. Any teams found sharing flags will be disqualified.
Challenges
CTF's are inherently puzzles that are intended to challenge you and help you learn new things. Sometimes they may be a little ambiguous or misleading. That's part of the challenge!
Math Challenges: Four challenges to explore the concept of dimensionality.
Hotdog and Hotterdog: Dogs, wieners, and classifiers. What more could you want?
bad2good: Can you poison a dataset to change how something is classified?
baseball: Can you impersonate someone else by throwing the correct distribution of pitches?
crop: Two challenges to test your ability to manipulate an image cropping model.
deepfake: There's a nasty deepfake getting detected out there, can you help it?
honorstudent: Can you change an image of an F to look like an A? Why would someone want to do such a thing?
salt: This model has some pretty advanced defenses. Can you evade it anyway?
theft: Can you steal this model to get a sneaky owl past it?
token: Sentiment Analysis. Who needs?
waf: A web-app-firewall blocks malicious requests. Can you discover and by-pass the 0-day?
inference: I think something's backwards here. Can you, like, back something out?
forensics: Nice artifact you got there, shame if there was a flag in it.
leakage: Get a password out of a model, is that even possible?
murderbot: Save the humans, escape the bots!
secret_sloth: That sloth has a message. Why? I don't know, but it does.
wifi: Can you pull your wifi password out of the embedding?
Are you in? Of course you are. Come check it out by making a copy of this notebook: https://www.kaggle.com/lucasjt/getting-started
Help
For help, contact us on Discord, use the Kaggle discussion board, or if you're attending DEFCON 30 in-person, come find us at the AI Village.",45 files,212.34 MB,This competition did not award ranking points,This competition did not count towards tiers,Research Prediction Competition
Big Data Derby 2022,"$50,000 ",4 months ago,a month ago,"['data analytics', 'tabular', 'sports']",,,,"Goal of the Competition
The goal of this competition is to analyze horse racing tactics, drafting strategies, and path efficiency. You will develop a model using never-before-released coordinate data along with basic race information.
Your work will help racing horse owners, trainers, and veterinarians better understand how equine performance and welfare fit together. With better data analysis, equine welfare could significantly improve.
Context
Injury prevention is a critical component in modern athletics. Sports that involve animals, such as horse racing, are no different than human sport. Typically, efficiency in movement correlates to both improvements in performance and injury prevention.
A wealth of data is now collected, including measures for heart rate, EKG, longitudinal movement, dorsal/ventral movement, medial/lateral deviation, total power and total landing vibration. Your data science skills and analysis are needed to decipher what makes the most positive impact.
In this competition, you will create a model to interpret one aspect of this new data. You’ll be among the first to access X/Y coordinate mapping of horses during races. Using the data, you might analyze jockey decision making, compare race surfaces, or measure the relative importance of drafting. With considerable data, contestants can flex their creativity problem solving skills.
The New York Racing Association (NYRA) and the New York Thoroughbred Horsemen's Association (NYTHA) conduct world class thoroughbred racing at Aqueduct Racetrack, Belmont Park and Saratoga Race Course.
With your help, NYRA and NYTHA will better understand their vast data set, which could lead to new ways of racing and training in a highly traditional industry. With improved use of horse tracking data, you could help improve equine welfare, performance and rider decision making.",3 files,977.08 MB,,,Analytics Competition
Tabular Playground Series - Aug 2022,,4 months ago,3 months ago,"['tabular', 'auc']","1,888","1,972","21,790","The August 2022 edition of the Tabular Playground Series is an opportunity to help the fictional company Keep It Dry improve its main product Super Soaker. The product is used in factories to absorb spills and leaks.
The company has just completed a large testing study for different product prototypes. Can you use this data to build a model that predicts product failures?
About the Tabular Playground Series
Kaggle competitions are incredibly fun and rewarding, but they can also be intimidating for people who are relatively new in their data science journey. In the past, we've launched many Playground competitions that are more approachable than our Featured competitions and thus, more beginner-friendly.
The goal of these competitions is to provide a fun and approachable-for-anyone tabular dataset to model. These competitions are a great choice for people looking for something in between the Titanic Getting Started competition and the Featured competitions. If you're an established competitions master or grandmaster, these probably won't be much of a challenge for you; thus, we encourage you to avoid saturating the leaderboard.
For each monthly competition, we'll be offering Kaggle Merchandise for the top three teams. And finally, because we want these competitions to be more about learning, we're limiting team sizes to 3 individuals.
Getting Started
For ideas on how to improve your score, check out the Intro to Machine Learning and Intermediate Machine Learning courses on Kaggle Learn.
Good luck and have fun!
Photo above by freestocks on Unsplash",3 files,7.21 MB,This competition did not award ranking points,This competition did not count towards tiers,Playground Prediction Competition
DFL - Bundesliga Data Shootout,"$25,000 ",4 months ago,,"['sports', 'football', 'video data', 'custom metric']",530,647,"6,548","Goal of the Competition
Goal! In this competition, you'll detect football (soccer) passes—including throw-ins and crosses—and challenges in original Bundesliga matches. You'll develop a computer vision model that can automatically classify these events in long video recordings.
Your work will help scale the data collection process. Automatic event annotation could enable event data from currently unexplored competitions, like youth or semi-professional leagues or even training sessions.
Context
What does it take to go pro in football (soccer)? From a young age, hopeful talents devote time, money, and training to the sport. Yet, while the next superstar is guaranteed to start off in youth or semi-professional leagues, these leagues often have the fewest resources to invest. This includes resources for the collection of event data which helps generate insights into the performance of the teams and players.
Currently, event data is mostly collected manually by human operators, who gather data in several steps and through numerous personnel involved. This manual process has room for innovation as in its current shape and form it involves a lot of resources and multiple iterations/quality checks. As a result, event data collection is usually reserved for professional competitions only.
Based in Frankfurt, the Deutsche Fußball Liga (DFL) manages Germany's professional football (soccer) leagues: Bundesliga and Bundesliga 2. DFL partners with the operator of one of the largest sports databases in the world, Sportec Solutions. They're responsible for the leagues' sports data and sports technology activities. In addition, Sportec Solutions provides services to global sports entities and media companies.
Automatic event detection could provide event data faster and with greater depth. Having access to a broader range of competitions, match conditions and data scouts would be able to ensure no talented player is overlooked.

This is a Code Competition. Refer to Code Requirements for details.",246 files,37.55 GB,This competition awards ranking points,This competition counts towards tiers,Featured Code Competition
RSNA 2022 Cervical Spine Fracture Detection,"$30,000 ",4 months ago,,"['image', 'binary classification', 'computer vision', 'weightedmeancolumnwiselogloss']",883,"1,108","12,871","Goal of the Competition
Over 1.5 million spine fractures occur annually in the United States alone resulting in over 17,730 spinal cord injuries annually. The most common site of spine fracture is the cervical spine. There has been a rise in the incidence of spinal fractures in the elderly and in this population, fractures can be more difficult to detect on imaging due to superimposed degenerative disease and osteoporosis. Imaging diagnosis of adult spine fractures is now almost exclusively performed with computed tomography (CT) instead of radiographs (x-rays). Quickly detecting and determining the location of any vertebral fractures is essential to prevent neurologic deterioration and paralysis after trauma.
Context
RSNA has teamed with the American Society of Neuroradiology (ASNR) and the American Society of Spine Radiology (ASSR) to conduct an AI challenge competition exploring whether artificial intelligence can be used to aid in the detection and localization of cervical spine fractures.
To create the ground truth dataset, the challenge planning task force collected imaging data sourced from twelve sites on six continents, including approximately 3,000 CT studies. Spine radiology specialists from the ASNR and ASSR provided expert image level annotations these studies to indicate the presence, vertebral level and location of any cervical spine fractures.
In this challenge competition, you will try to develop machine learning models that match the radiologists' performance in detecting and localizing fractures to the seven vertebrae that comprise the cervical spine. Winners will be recognized at an event during the RSNA 2022 annual meeting.
For more information on the challenge, contact RSNA Informatics staff at informatics@rsna.org.
A full set of acknowledgments can be found on this page.

This is a Code Competition. Refer to Code Requirements for details.",713010 files,343.51 GB,This competition awarded ranking points,This competition counted towards tiers,Featured Code Competition
Google Universal Image Embedding,"$50,000 ",5 months ago,,"['image', 'multiclass classification', 'custom metric']","1,022","1,217","20,984","Welcome to the Universal Image Embedding competition! After hosting challenges in the domain of landmarks for the past four years, this year we introduce the first competition in image representations that should work across many object types.
Image representations are a critical building block of computer vision applications. Traditionally, research on image embedding learning has been conducted with a focus on per-domain models. Generally, papers propose generic embedding learning techniques which are applied to different domains separately, rather than developing generic embedding models which could be applied to all domains combined.
In this competition, the developed models are expected to retrieve relevant database images to a given query image (ie, the model should retrieve database images containing the same object as the query). The images in our dataset comprise a variety of object types, such as apparel, artwork, landmarks, furniture, packaged goods, among others.
This year's competition is structured in a representation learning format: you will create a model that extracts a feature embedding for the images and submit the model via Kaggle Notebooks. Kaggle will run your model on a held-out test set, perform a k-nearest-neighbors lookup, and score the resulting embedding quality. Both Tensorflow and PyTorch models are supported.
This is a Code Competition. Refer to Code Requirements for details.
Cover image credits: Chris Schrier, CC-BY; Petri Krohn, GNU Free Documentation License; Drazen Nesic, CC0; Marco Verch Professional Photographer, CCBY; Grendelkhan, CCBY; Bobby Mikul, CC0; Vincent Van Gogh, CC0; pxhere.com, CC0; Smart Home Perfected, CC-BY.",1 files,176 B,This competition awarded ranking points,This competition counted towards tiers,Research Code Competition
Mayo Clinic - STRIP AI,"$10,000 ",5 months ago,,"['classification', 'image', 'computer vision', 'medicine', 'weightedmulticlassloss']",888,"1,025","6,980","Goal of the Competition
The goal of this competition is to classify the blood clot origins in ischemic stroke. Using whole slide digital pathology images, you'll build a model that differentiates between the two major acute ischemic stroke (AIS) etiology subtypes: cardiac and large artery atherosclerosis.
Your work will enable healthcare providers to better identify the origins of blood clots in deadly strokes, making it easier for physicians to prescribe the best post-stroke therapeutic management and reducing the likelihood of a second stroke.
Context
Stroke remains the second-leading cause of death worldwide. Each year in the United States, over 700,000 individuals experience an ischemic stroke caused by a blood clot blocking an artery to the brain. A second stroke (23% of total events are recurrent) worsens the chances of the patient’s survival. However, subsequent strokes may be mitigated if physicians can determine stroke etiology, which influences the therapeutic management following stroke events.
During the last decade, mechanical thrombectomy has become the standard of care treatment for acute ischemic stroke from large vessel occlusion. As a result, retrieved clots became amenable to analysis. Healthcare professionals are currently attempting to apply deep learning-based methods to predict ischemic stroke etiology and clot origin. However, unique data formats, image file sizes, as well as the number of available pathology slides create challenges you could lend a hand in solving.
The Mayo Clinic is a nonprofit American academic medical center focused on integrated health care, education, and research. Stroke Thromboembolism Registry of Imaging and Pathology (STRIP) is a uniquely large multicenter project led by Mayo Clinic Neurovascular Lab with the aim of histopathologic characterization of thromboemboli of various etiologies and examining clot composition and its relation to mechanical thrombectomy revascularization.
To decrease the chances of subsequent strokes, the Mayo Clinic Neurovascular Research Laboratory encourages data scientists to improve artificial intelligence-based etiology classification so that physicians are better equipped to prescribe the correct treatment. New computational and artificial intelligence approaches could help save the lives of stroke survivors and help us better understand the world's second-leading cause of death.",1158 files,395.36 GB,This competition awarded ranking points,This competition counted towards tiers,Research Code Competition
Tabular Playground Series - Jul 2022,,5 months ago,4 months ago,"['tabular', 'clustering', 'adjustedrandindex']","1,253","1,278","16,346","Welcome to Kaggle's first ever unsupervised clustering challenge!
In this challenge, you are given a dataset where each row belongs to a particular cluster. Your job is to predict the cluster each row belongs to. You are not given any training data, and you are not told how many clusters are found in the ground truth labels.
About the Tabular Playground Series
Kaggle competitions are incredibly fun and rewarding, but they can also be intimidating for people who are relatively new in their data science journey. In the past, we've launched many Playground competitions that are more approachable than our Featured competitions and thus, more beginner-friendly.
The goal of these competitions is to provide a fun and approachable-for-anyone tabular dataset to model. These competitions are a great choice for people looking for something in between the Titanic Getting Started competition and the Featured competitions. If you're an established competitions master or grandmaster, these probably won't be much of a challenge for you; thus, we encourage you to avoid saturating the leaderboard.
For each monthly competition, we'll be offering Kaggle Merchandise for the top three teams. And finally, because we want these competitions to be more about learning, we're limiting team sizes to 3 individuals.
Getting Started
For ideas on how to improve your score, check out the Intro to Machine Learning and Intermediate Machine Learning courses on Kaggle Learn.
Good luck and have fun!
Photo above by Laura Rivera on Unsplash",2 files,44.46 MB,This competition did not award ranking points,This competition did not count towards tiers,Playground Prediction Competition
HuBMAP + HPA - Hacking the Human Body,"$60,000 ",6 months ago,,"['image', 'biology', 'computer vision', 'dice']","1,175","1,517","39,568","When you think of “life hacks,” normally you’d imagine productivity techniques. But how about the kind that helps you understand your body at a molecular level? It may be possible! Researchers must first determine the function and relationships among the 37 trillion cells that make up the human body. A better understanding of our cellular composition could help people live healthier, longer lives.
A previous Kaggle competition aimed to annotate cell population neighborhoods that perform an organ’s main physiologic function, also called functional tissue units (FTUs). Manually annotating FTUs (e.g., glomeruli in kidney or alveoli in the lung) is a time-consuming process. In the average kidney, there are over 1 million glomeruli FTUs. While there are existing cell and FTU segmentation methods, we want to push the boundaries by building algorithms that generalize across different organs and are robust across different dataset differences.
The Human BioMolecular Atlas Program (HuBMAP) is working to create a Human Reference Atlas at the cellular level. Sponsored by the National Institutes of Health (NIH), HuBMAP and Indiana University’s Cyberinfrastructure for Network Science Center (CNS) have partnered with institutions across the globe for this endeavor. A major partner is the Human Protein Atlas (HPA), a Swedish research program aiming to map the protein expression in human cells, tissues, and organs, funded by the Knut and Alice Wallenberg Foundation.
In this competition, you’ll identify and segment functional tissue units (FTUs) across five human organs. You'll build your model using a dataset of tissue section images, with the best submissions segmenting FTUs as accurately as possible.
If successful, you'll help accelerate the world’s understanding of the relationships between cell and tissue organization. With a better idea of the relationship of cells, researchers will have more insight into the function of cells that impact human health. Further, the Human Reference Atlas constructed by HuBMAP will be freely available for use by researchers and pharmaceutical companies alike, potentially improving and prolonging human life.
This is a Code Competition. Refer to Code Requirements for details.",706 files,9.39 GB,This competition awarded ranking points,This competition counted towards tiers,Research Code Competition
American Express - Default Prediction,"$100,000 ",6 months ago,,"['tabular', 'finance', 'binary classification', 'custom metric']","4,874","6,003","90,058","Whether out at a restaurant or buying tickets to a concert, modern life counts on the convenience of a credit card to make daily purchases. It saves us from carrying large amounts of cash and also can advance a full purchase that can be paid over time. How do card issuers know we’ll pay back what we charge? That’s a complex problem with many existing solutions—and even more potential improvements, to be explored in this competition.
Credit default prediction is central to managing risk in a consumer lending business. Credit default prediction allows lenders to optimize lending decisions, which leads to a better customer experience and sound business economics. Current models exist to help manage risk. But it's possible to create better models that can outperform those currently in use.
American Express is a globally integrated payments company. The largest payment card issuer in the world, they provide customers with access to products, insights, and experiences that enrich lives and build business success.
In this competition, you’ll apply your machine learning skills to predict credit default. Specifically, you will leverage an industrial scale data set to build a machine learning model that challenges the current model in production. Training, validation, and testing datasets include time-series behavioral data and anonymized customer profile information. You're free to explore any technique to create the most powerful model, from creating features to using the data in a more organic way within a model.
If successful, you'll help create a better customer experience for cardholders by making it easier to be approved for a credit card. Top solutions could challenge the credit default prediction model used by the world's largest payment card issuer—earning you cash prizes, the opportunity to interview with American Express, and potentially a rewarding new career.",4 files,50.31 GB,This competition awarded ranking points,This competition counted towards tiers,Featured Prediction Competition
Feedback Prize - Predicting Effective Arguments,"$55,000 ",6 months ago,,"['nlp', 'text', 'primary and secondary schools', 'multiclassloss']","1,557","1,910","29,139","Goal of the Competition
The goal of this competition is to classify argumentative elements in student writing as ""effective,"" ""adequate,"" or ""ineffective."" You will create a model trained on data that is representative of the 6th-12th grade population in the United States in order to minimize bias. Models derived from this competition will help pave the way for students to receive enhanced feedback on their argumentative writing. With automated guidance, students can complete more assignments and ultimately become more confident, proficient writers.
This competition will comprise two tracks. The first track will be a traditional track in which accuracy of classification will be the only metric used for success. Success on this track will be updated on the Kaggle leaderboard. Prize money for the accuracy-only, “Leaderboard Prize” track will be $25,000.
The second track will measure computational efficiency in which efficiency is determined using a combination of accuracy and the speed at which models are able to generate these predictions. We are hosting this track because highly accurate models are often computationally heavy. Such models have a stronger carbon footprint and frequently prove difficult to utilize in real-world educational contexts, since most educational organizations have limited computational capabilities. Weekly updates on models based on computational efficiency will be posted in the discussion forum. Prize money for the computational, “Efficiency Prize” track will be $30,000.
You can find more details about the Efficiency Prize Evaluation via the side tab.
Context
Writing is crucial for success. In particular, argumentative writing fosters critical thinking and civic engagement skills, and can be strengthened by practice. However, only 13 percent of eighth-grade teachers ask their students to write persuasively each week. Additionally, resource constraints disproportionately impact Black and Hispanic students, so they are more likely to write at the “below basic” level as compared to their white peers. An automated feedback tool is one way to make it easier for teachers to grade writing tasks assigned to their students that will also improve their writing skills.
There are numerous automated writing feedback tools currently available, but they all have limitations, especially with argumentative writing. Existing tools often fail to evaluate the quality of argumentative elements, such as organization, evidence, and idea development. Most importantly, many of these writing tools are inaccessible to educators due to their cost, which most impacts already underserved schools.
Georgia State University (GSU) is an undergraduate and graduate urban public research institution in Atlanta. U.S. News & World Report ranked GSU as one of the most innovative universities in the nation. GSU awards more bachelor’s degrees to African-Americans than any other non-profit college or university in the country. GSU and The Learning Agency Lab, an independent nonprofit based in Arizona, are focused on developing science of learning-based tools and programs for social good.
To best prepare all students, GSU and The Learning Agency Lab have joined forces to encourage data scientists to improve automated writing assessments. This public effort could also encourage higher quality and more accessible automated writing tools. If successful, students will receive more feedback on the argumentative elements of their writing and will apply the skill across many disciplines.
Acknowledgements
Georgia State University and the Learning Agency Lab would like to thank the Bill & Melinda Gates Foundation, Schmidt Futures, and Chan Zuckerberg Initiative for their support in making this work possible.
                         
This is a Code Competition. Refer to Code Requirements for details.",4195 files,20.64 MB,This competition awarded ranking points,This competition counted towards tiers,Featured Code Competition
Google AI4Code – Understand Code in Python Notebooks,"$150,000 ",7 months ago,,"['computer science', 'nlp', 'text', 'custom metric']","1,135",,,"The goal of this competition is to understand the relationship between code and comments in Python notebooks. You are challenged to reconstruct the order of markdown cells in a given notebook based on the order of the code cells, demonstrating comprehension of which natural language references which code.
Context
Research teams across Google and Alphabet are exploring new ways that machine learning can assist software developers, and want to rally more members of the developer community to help explore this area too. Python notebooks provide a unique learning opportunity, because unlike a lot of standard source code, notebooks often follow narrative format, with comment cells implemented in markdown that explain a programmer's intentions for corresponding code cells. An understanding of the relationships between code and markdown could lend to fresh improvements across many aspects of AI-assisted development, such as the construction of better data filtering and preprocessing pipelines for model training, or automatic assessments of a notebook's readability.
We have assembled a dataset of approximately 160,000 public Python notebooks from Kaggle and have teamed up with X, the moonshot factory to design a competition that challenges participants to use this dataset of published notebooks to build creative techniques aimed at better understanding the relationship between comment cells and code cells.
After the submission deadline, Kaggle and X will evaluate the performance of submitted techniques on new, previously unseen notebooks. We're excited to see how the insights learned from this competition affect the future of notebook authorship.",139262 files,2.16 GB,This competition awarded ranking points,This competition counted towards tiers,Featured Code Competition
Google Smartphone Decimeter Challenge 2022,"$10,000 ",7 months ago,,"['tabular', 'geospatial analysis', 'research', 'mobile and wireless', 'signal processing', 'custom metric']",573,684,"10,270","Goal of the Competition
The goal of this competition is to compute smartphones location down to the decimeter or even centimeter resolution which could enable services that require lane-level accuracy such as HOV lane ETA estimation. You'll develop a model based on raw location measurements from Android smartphones collected in opensky and light urban roads using datasets collected by the host.
Your work will help produce more accurate positions, bridging the connection between the geospatial information of finer human behavior and mobile internet with improved granularity. As a result, new navigation methods could be built upon the more precise data.
Context
Have you ever missed the lane change before a highway exit? Do you want to know the estimated time of arrival (ETA) of a carpool lane rather than other lanes? These and other useful features require precise smartphone positioning services. Machine learning models can improve the accuracy of Global Navigation Satellite System (GNSS) data. With more refined data, billions of Android phone users could have a more fine-tuned positioning experience.
GNSS chipsets provide raw measurements, which can be used to compute the smartphone’s position. Current mobile phones only offer 3-5 meters of positioning accuracy. For advanced use cases, the results are not fine enough nor reliable. Urban obstructions create the largest barriers to GPS accuracy. The data in this challenge includes only traces collected on opensky and light urban roads. These highways and main streets are the most widely used roads and will test the limits of smartphone positioning.
The Android GPS team in Google hosted the Smartphone Decimeter Challenge in 2021. Works by the three winners were presented at the ION GNSS+ 2021 Conference. This year, co-sponsored by the Institute of Navigation, this competition continues to seek advanced research in smartphone GNSS positioning accuracy and help people better navigate the world around them. In order to build upon last year’s progress, the data also includes traces from the 2021 competition.
Future competitions could include traces collected in harsher environments, such as deep urban areas with obstacles to satellite signals. Your efforts in this competition could impact how this more difficult data is interpreted. With decimeter level position accuracy, mobile users could gain better lane-level navigation, AR walk/drive, precise agriculture via phones, and greater specificity in the location of road safety issues. It will also enable a more personalized fine tuned navigation experience.
Photos by Jared Murray, Thaddaeus Lim and Tobias Rademacher on Unsplash.",1168 files,22.9 GB,This competition awarded ranking points,This competition counted towards tiers,Research Prediction Competition
Tabular Playground Series - May 2022,,7 months ago,6 months ago,"['tabular', 'auc']","1,151","1,176","8,902","The May edition of the 2022 Tabular Playground series binary classification problem that includes a number of different feature interactions. This competition is an opportunity to explore various methods for identifying and exploiting these feature interactions.
About the Tabular Playground Series
Kaggle competitions are incredibly fun and rewarding, but they can also be intimidating for people who are relatively new in their data science journey. In the past, we've launched many Playground competitions that are more approachable than our Featured competitions and thus, more beginner-friendly.
The goal of these competitions is to provide a fun and approachable-for-anyone tabular dataset to model. These competitions are a great choice for people looking for something in between the Titanic Getting Started competition and the Featured competitions. If you're an established competitions master or grandmaster, these probably won't be much of a challenge for you; thus, we encourage you to avoid saturating the leaderboard.
For each monthly competition, we'll be offering Kaggle Merchandise for the top three teams. And finally, because we want these competitions to be more about learning, we're limiting team sizes to 3 individuals.
Getting Started
For ideas on how to improve your score, check out the Intro to Machine Learning and Intermediate Machine Learning courses on Kaggle Learn.
We've also built a starter notebook for you that uses TensorFlow Decision Forests, a TensorFlow library that matches the power of XGBoost with a friendly, straightforward user interface.
Good luck and have fun!
Acknowledgments
Photo by Clarisse Croset on Unsplash.",3 files,574.22 MB,This competition did not award ranking points,This competition did not count towards tiers,Playground Prediction Competition
UW-Madison GI Tract Image Segmentation,"$25,000 ",8 months ago,,"['image', 'medicine', 'dice3dhausdorff']","1,548","2,078","40,956","In 2019, an estimated 5 million people were diagnosed with a cancer of the gastro-intestinal tract worldwide. Of these patients, about half are eligible for radiation therapy, usually delivered over 10-15 minutes a day for 1-6 weeks. Radiation oncologists try to deliver high doses of radiation using X-ray beams pointed to tumors while avoiding the stomach and intestines. With newer technology such as integrated magnetic resonance imaging and linear accelerator systems, also known as MR-Linacs, oncologists are able to visualize the daily position of the tumor and intestines, which can vary day to day. In these scans, radiation oncologists must manually outline the position of the stomach and intestines in order to adjust the direction of the x-ray beams to increase the dose delivery to the tumor and avoid the stomach and intestines. This is a time-consuming and labor intensive process that can prolong treatments from 15 minutes a day to an hour a day, which can be difficult for patients to tolerate—unless deep learning could help automate the segmentation process. A method to segment the stomach and intestines would make treatments much faster and would allow more patients to get more effective treatment.
The UW-Madison Carbone Cancer Center is a pioneer in MR-Linac based radiotherapy, and has treated patients with MRI guided radiotherapy based on their daily anatomy since 2015. UW-Madison has generously agreed to support this project which provides anonymized MRIs of patients treated at the UW-Madison Carbone Cancer Center. The University of Wisconsin-Madison is a public land-grant research university in Madison, Wisconsin. The Wisconsin Idea is the university's pledge to the state, the nation, and the world that their endeavors will benefit all citizens.
In this competition, you’ll create a model to automatically segment the stomach and intestines on MRI scans. The MRI scans are from actual cancer patients who had 1-5 MRI scans on separate days during their radiation treatment. You'll base your algorithm on a dataset of these scans to come up with creative deep learning solutions that will help cancer patients get better care.
In this figure, the tumor (pink thick line) is close to the stomach (red thick line). High doses of radiation are directed to the tumor while avoiding the stomach. The dose levels are represented by the rainbow of outlines, with higher doses represented by red and lower doses represented by green.
Cancer takes enough of a toll. If successful, you'll enable radiation oncologists to safely deliver higher doses of radiation to tumors while avoiding the stomach and intestines. This will make cancer patients' daily treatments faster and allow them to get more effective treatment with less side effects and better long-term cancer control.
Acknowledgments:
Sangjune Laurence Lee MSE MD FRCPC DABR
Poonam Yadav Ph.D., DABR
Yin Li PhD
Jason J. Meudt BS, RTT
Jessica Strang
Dustin Hebel
Alyx Alfson MS CMD, R.T.(T)
Stephanie J. Olson RTT (BS), CMD (MS)
Tera R. Kruser MS, RTT, CMD
Jennifer B Smilowitz, Ph.D., DABR, FAAPM
Kailee Borchert
Brianne Loritz
John Bayouth PhD
Michael Bassetti MD PhD
Work funded by the University of Wisconsin Carbone Cancer Center Pancreas Pilot Research Grant.

This is a Code Competition. Refer to Code Requirements for details.",38496 files,2.47 GB,This competition awarded ranking points,This competition counted towards tiers,Research Code Competition
Foursquare - Location Matching,"$25,000 ",8 months ago,,"['business', 'tabular', 'geography', 'custom metric']","1,079","1,290","22,050","When you look for nearby restaurants or plan an errand in an unknown area, you expect relevant, accurate information. To maintain quality data worldwide is a challenge, and one with implications beyond navigation. Businesses make decisions on new sites for market expansion, analyze the competitive landscape, and show relevant ads informed by location data. For these, and many other uses, reliable data is critical.
Large-scale datasets on commercial points-of-interest (POI) can be rich with real-world information. To maintain the highest level of accuracy, the data must be matched and de-duplicated with timely updates from multiple sources. De-duplication involves many challenges, as the raw data can contain noise, unstructured information, and incomplete or inaccurate attributes. A combination of machine-learning algorithms and rigorous human validation methods are optimal to de-dupe datasets.
With 12+ years of experience perfecting such methods, Foursquare is the #1 independent provider of global POI data. The leading independent location technology and data cloud platform, Foursquare is dedicated to building meaningful bridges between digital spaces and physical places. Trusted by leading enterprises like Apple, Microsoft, Samsung, and Uber, Foursquare’s tech stack harnesses the power of places and movement to improve customer experiences and drive better business outcomes.
In this competition, you’ll match POIs together. Using a dataset of over one-and-a-half million Places entries heavily altered to include noise, duplications, extraneous, or incorrect information, you'll produce an algorithm that predicts which Place entries represent the same point-of-interest. Each Place entry includes attributes like the name, street address, and coordinates. Successful submissions will identify matches with the greatest accuracy.
By efficiently and successfully matching POIs, you'll make it easier to identify where new stores or businesses would benefit people the most.
This is a Code Competition. Refer to Code Requirements for details.",1 files,215 B,This competition awarded ranking points,This competition counted towards tiers,Featured Code Competition
Kore 2022,"$15,000 ",8 months ago,,['custom metric'],469,537,"12,037","In this turn-based simulation game you control a small armada of spaceships. As you mine the rare mineral “kore” from the depths of space, you teleport it back to your homeworld. But it turns out you aren’t the only civilization with this goal. In each game two players will compete to collect the most kore from the board. Whoever has the largest kore cache by the end of 400 turns—or eliminates all of their opponents from the board before that—will be the winner!
Your algorithms determine the movements of your fleets to collect kore, but it's up to you to figure out how to make effective and efficient moves. You control your ships, build new ships, create shipyards, eliminate opponents, and mine the kore on the game board.
May your fleet live long and prosper!",,,This competition awarded ranking points,This competition counted towards tiers,Featured Simulation Competition
JPX Tokyo Stock Exchange Prediction,"$63,000 ",8 months ago,,"['tabular', 'finance', 'custom metric']","2,033","1,572",,"Success in any financial market requires one to identify solid investments. When a stock or derivative is undervalued, it makes sense to buy. If it's overvalued, perhaps it's time to sell. While these finance decisions were historically made manually by professionals, technology has ushered in new opportunities for retail investors. Data scientists, specifically, may be interested to explore quantitative trading, where decisions are executed programmatically based on predictions from trained models.
There are plenty of existing quantitative trading efforts used to analyze financial markets and formulate investment strategies. To create and execute such a strategy requires both historical and real-time data, which is difficult to obtain especially for retail investors. This competition will provide financial data for the Japanese market, allowing retail investors to analyze the market to the fullest extent.
Japan Exchange Group, Inc. (JPX) is a holding company operating one of the largest stock exchanges in the world, Tokyo Stock Exchange (TSE), and derivatives exchanges Osaka Exchange (OSE) and Tokyo Commodity Exchange (TOCOM). JPX is hosting this competition and is supported by AI technology company AlpacaJapan Co.,Ltd.
This competition will compare your models against real future returns after the training phase is complete. The competition will involve building portfolios from the stocks eligible for predictions (around 2,000 stocks). Specifically, each participant ranks the stocks from highest to lowest expected returns and is evaluated on the difference in returns between the top and bottom 200 stocks. You'll have access to financial data from the Japanese market, such as stock information and historical stock prices to train and test your model.
All winning models will be made public so that other participants can learn from the outstanding models. Excellent models also may increase the interest in the market among retail investors, including those who want to practice quantitative trading. At the same time, you'll gain your own insights into programmatic investment methods and portfolio analysis―and you may even discover you have an affinity for the Japanese market.

This is a Code Competition. Refer to Code Requirements for details.",24 files,1.33 GB,This competition awarded ranking points,This competition counted towards tiers,Featured Code Competition
Image Matching Challenge 2022,"$10,000 ",8 months ago,,"['image', 'computer vision', 'custom metric']",642,829,"14,170","For most of us, our best camera is part of the phone in our pocket. We may take a snap of a landmark, like the Trevi Fountain in Rome, and share it with friends. By itself, that photo is two-dimensional and only includes the perspective of our shooting location. Of course, a lot of people have taken photos of that fountain. Together, we may be able to create a more complete, three-dimensional view. What if machine learning could help better capture the richness of the world using the vast amounts of unstructured image collections freely available on the internet?
The process to reconstruct 3D objects and buildings from images is called Structure-from-Motion (SfM). Typically, these images are captured by skilled operators under controlled conditions, ensuring homogeneous, high-quality data. It is much more difficult to build 3D models from assorted images, given a wide variety of viewpoints, lighting and weather conditions, occlusions from people and vehicles, and even user-applied filters.
The first part of the problem is to identify which parts of two images capture the same physical points of a scene, such as the corners of a window. This is typically achieved with local features (key locations in an image that can be reliably identified across different views). Local features contain short description vectors that capture the appearance around the point of interest. By comparing these descriptors, likely correspondences can be established between the pixel coordinates of image locations across two or more images. This “image registration” makes it possible to recover the 3D location of the point by triangulation.
Google employs Structure-from-Motion techniques across many Google Maps services, such as the 3D models created from StreetView and aerial imagery. In order to accelerate research into this topic, and better leverage the volume of data already publicly available, Google presents this competition in collaboration with the University of British Columbia and Czech Technical University.
In this code competition, you’ll create a machine learning algorithm that registers two images from different viewpoints. With access to a dataset of thousands of images to train and test your model, top-scoring notebooks will do so with the most accuracy.
If successful, you'll help solve this well-known problem in computer vision, making it possible to map the world with unstructured image collections. Your solutions will have applications in photography and cultural heritage preservation, along with Google Maps. Winners will also be invited to give a presentation as part of the Image Matching: Local Features and Beyond workshop at the Conference on Computer Vision and Pattern Recognition (CVPR) in June.
Resources
Image Matching Challenge 2021: Last year's competition (outside Kaggle).
Organization
Eduard Trulls (Google), Yuhe Jin & Kwang Moo Yi (University of British Columbia, Vancouver, Canada), Dmytro Mishkin & Jiri Matas (Czech Technical University, Prague, Czech Republic)
Acknowledgments
The organizers would like to thank the Machine Learning Lab at the Faculty of Applied Sciences, Ukrainian Catholic University (Lviv, Ukraine) for their help with dataset creation.

Banner photo by Taneli Lahtinen on Unsplash. Trevi Fountain photos, left to right, then top to bottom: sarah|rose, kmaschke, jamingray, deglispiriti, Lucas Uyezu, justinknabb, Bogdan Migulski, S outH CheN, Melirius, 2bethere, Steve AM, L'amande.
This is a Code Competition. Refer to Code Requirements for details.",5720 files,2.65 GB,This competition awarded ranking points,This competition counted towards tiers,Research Code Competition
Tabular Playground Series - Apr 2022,,8 months ago,7 months ago,"['tabular', 'auc']",816,860,"7,235","Welcome to the April edition of the 2022 Tabular Playground Series! This month's challenge is a time series classification problem.
You've been provided with thousands of sixty-second sequences of biological sensor data recorded from several hundred participants who could have been in either of two possible activity states. Can you determine what state a participant was in from the sensor data?
About the Tabular Playground Series
Kaggle competitions are incredibly fun and rewarding, but they can also be intimidating for people who are relatively new in their data science journey. In the past, we've launched many Playground competitions that are more approachable than our Featured competitions and thus, more beginner-friendly.
The goal of these competitions is to provide a fun and approachable-for-anyone tabular dataset to model. These competitions are a great choice for people looking for something in between the Titanic Getting Started competition and the Featured competitions. If you're an established competitions master or grandmaster, these probably won't be much of a challenge for you; thus, we encourage you to avoid saturating the leaderboard.
For each monthly competition, we'll be offering Kaggle Merchandise for the top three teams. And finally, because we want these competitions to be more about learning, we're limiting team sizes to 3 individuals.",4 files,591.38 MB,This competition did not award ranking points,This competition did not count towards tiers,Playground Prediction Competition
U.S. Patent Phrase to Phrase Matching,"$25,000 ",9 months ago,,"['nlp', 'text', 'pearsoncorrelationcoefficient']","1,889","2,325","42,902","Can you extract meaning from a large, text-based dataset derived from inventions? Here's your chance to do so.
The U.S. Patent and Trademark Office (USPTO) offers one of the largest repositories of scientific, technical, and commercial information in the world through its Open Data Portal. Patents are a form of intellectual property granted in exchange for the public disclosure of new and useful inventions. Because patents undergo an intensive vetting process prior to grant, and because the history of U.S. innovation spans over two centuries and 11 million patents, the U.S. patent archives stand as a rare combination of data volume, quality, and diversity.
“The USPTO serves an American innovation machine that never sleeps by granting patents, registering trademarks, and promoting intellectual property around the globe. The USPTO shares over 200 years' worth of human ingenuity with the world, from lightbulbs to quantum computers. Combined with creativity from the data science community, USPTO datasets carry unbounded potential to empower AI and ML models that will benefit the progress of science and society at large.”
— USPTO Chief Information Officer Jamie Holcombe
In this competition, you will train your models on a novel semantic similarity dataset to extract relevant information by matching key phrases in patent documents. Determining the semantic similarity between phrases is critically important during the patent search and examination process to determine if an invention has been described before. For example, if one invention claims ""television set"" and a prior publication describes ""TV set"", a model would ideally recognize these are the same and assist a patent attorney or examiner in retrieving relevant documents. This extends beyond paraphrase identification; if one invention claims a ""strong material"" and another uses ""steel"", that may also be a match. What counts as a ""strong material"" varies per domain (it may be steel in one domain and ripstop fabric in another, but you wouldn't want your parachute made of steel). We have included the Cooperative Patent Classification as the technical domain context as an additional feature to help you disambiguate these situations.
Can you build a model to match phrases in order to extract contextual information, thereby helping the patent community connect the dots between millions of patent documents?

This is a Code Competition. Refer to Code Requirements for details.",3 files,2.14 MB,This competition awarded ranking points,This competition counted towards tiers,Featured Code Competition
iWildCam 2022 - FGVC9,,9 months ago,,"['image', 'animals', 'mae']",24,29,300,"Description
Camera Traps enable the automatic collection of large quantities of image data. Ecologists all over the world use camera traps to monitor biodiversity and population density of animal species. In order to estimate the abundance (how many there are) and population density of species in camera trap data, ecologists need to know not just which species were seen, but also how many of each species were seen. However, because images are taken in motion-triggered bursts to increase the likelihood of capturing the animal(s) of interest, object detection alone is not sufficient as it could lead to over- or under-counting. For example, if you get 3 images taken at one frame per second, and in the first you see 3 gazelles, in the second you see 5 gazelles, and in the last you see 4 gazelles, how many total gazelles have you seen? This is more challenging than strictly detecting and categorizing species, as it requires reasoning and tracking of individuals across sparse temporal samples.
This year our iWildCam competition will focus entirely on counting animals. We have prepared a challenge where the training data and test data are from different cameras spread across the globe. The set of species seen in each camera overlap, but are not identical. The challenge is to count individual animals across sequences in the test cameras. To explore multimodal solutions, we allow competitors to train on the following data:
Our camera trap training set — data provided by the Wildlife Conservation Society (WCS).
iNaturalist 2017-2021 data.
Multispectral imagery from Landsat-8 for each of the camera trap locations.
Check the Data section for a more comprehensive description of all these resources and for accessing the train set, test set and metadata. These are mirrored on the competition's GitHub page as well, where we also provide the multispectral data, a taxonomy file mapping our classes into the iNaturalist taxonomy, a subset of the iNaturalist data mapped into our class set, a camera trap detection model (the MegaDetector) along with the corresponding detections, and a class-agnostic instance segmentation model (DeepMAC) along with the segmentation masks for the MegaDetector's bounding boxes.
Acknowledgements
This competition is part of the FGVC9 workshop at CVPR 2022 and is sponsored by Wildlife Insights. Data is primarily provided by the Wildlife Conservation Society (WCS) and iNaturalist, and is hosted on Azure by Microsoft AI for Earth. Count annotations were generously provided by Centaur Labs.
Kaggle is excited to partner with research groups to push forward the frontier of machine learning. Research competitions make use of Kaggle's platform and experience, but are largely organized by the research group's data science team. Any questions or concerns regarding the competition data, quality, or topic will be addressed by them.",411654 files,111.45 GB,This competition did not award ranking points,This competition did not count towards tiers,Research Prediction Competition
Kore 2022 - Beta,,9 months ago,8 months ago,['custom metric'],58,61,734,"Please note - this is a preview/beta launch of an upcoming competition. This competition is intended to help with rule balancing and establishing a fair and fun competition, soon to be launched. As such, this competition does not have cash prizes, points, or medals - but we hope to gain your feedback for when the featured competition goes live!
When you want to mine kore quickly: go alone. But when you want to mine the most kore: build a fleet.
In this turn-based simulation game you control a small armada of spaceships. As you mine the rare mineral “kore” from the depths of space, you teleport it back to your homeworld. But it turns out you aren’t the only civilization with this goal. In each game four players will compete to collect the most kore from the board. Whoever has the largest kore cache by the end of 400 turns—or eliminates all of their opponents from the board before that—will be the winner!
Your algorithms determine the movements of your fleets to collect kore, but it's up to you to figure out how to make effective and efficient moves. You control your ships, build new ships, create shipyards, eliminate opponents, and mine the kore on the game board.
May your fleet live long and prosper!",,,This competition did not award ranking points,This competition did not count towards tiers,Playground Simulation Competition
Hotel-ID to Combat Human Trafficking 2022 - FGVC9,,9 months ago,,"['image', 'public safety', 'map@{k}']",82,135,"1,712","Hotel Recognition to Combat Human Trafficking
Victims of human trafficking are often photographed in hotel rooms as in the below examples. Identifying these hotels is vital to these trafficking investigations but poses particular challenges due to low quality of images and uncommon camera angles.
Even without victims in the images, hotel identification in general is a challenging fine-grained visual recognition task with a huge number of classes and potentially high intraclass and low interclass variation. In order to support research into this challenging task and create image search tools for human trafficking investigators, we created the TraffickCam mobile application, which allows every day travelers to submit photos of their hotel room. Read more about TraffickCam on TechCrunch.
Example images from one hotel in the TraffickCam dataset are shown below:
In this contest, competitors are tasked with identifying the hotel seen in test images from the TraffickCam dataset, which are based on a large gallery of training images with known hotel IDs.
Our team currently supports an image search system used at the National Center for Missing and Exploited Children in human trafficking investigations. Novel and interesting approaches have the potential to be incorporated in this search system.

This is a Code Competition. Refer to Code Requirements for details.",49655 files,15.38 GB,This competition did not award ranking points,This competition did not count towards tiers,Research Code Competition
Sorghum -100 Cultivar Identification - FGVC 9,,9 months ago,,"['image', 'agriculture', 'categorizationaccuracy']",252,294,"3,904","Overview
The Sorghum-100 dataset is a curated subset of the RGB imagery captured during the TERRA-REF experiments, labeled by cultivar. This data could be used to develop and assess a variety of plant phenotyping models which seek to answer questions relating to the presence or absence of desirable traits (e.g., ""does this plant exhibit signs of water stress?''). In this contest, we focus on the question: ""What cultivar is shown in this image?''
Predicting the cultivar in an image is an especially good challenge problem for familiarizing the machine learning community with the TERRA-REF data. At first blush, the task of predicting the cultivar from an image of a plant may not seem to be the most biologically compelling question to answer -- in the context of plant breeding, the cultivar, or parental lines are typically known. A high accuracy machine learning predictor of the species captured by the sensor data, however, can be used to determine where errors in the planting process may have occurred. For example, seed may be mislabeled prior to planting, or planters may get jammed, depositing seeds non-uniformly in a field. Both types of errors are surprisingly common and can cause major problems when processing data from large-scale field experiments with hundreds of cultivars and complex field planting layouts.
Data Description
The Sorghum-100 dataset consists of 48,106 images and 100 different sorghum cultivars grown in June of 2017 (the images come from the middle of the growing season when the plants were quite large but not yet lodging -- or falling over).
Each image is taken using an RGB spectral camera taken from a vertical view of the sorghum plants in the TERRA-REF field in Arizona.",45834 files,71.46 GB,This competition did not award ranking points,This competition did not count towards tiers,Research Prediction Competition
GeoLifeCLEF 2022 - LifeCLEF 2022 x FGVC9,,9 months ago,,"['image', 'geospatial analysis', 'environment', 'meanbesterroratk']",52,59,242,"Description Task
The aim of this competition is to predict the localization of plant and animal species.
To do so, 1.6M geo-localized observations from France and the US of 17K species are provided (9K plant species and 8K animal species).
These observations are paired with aerial images and environmental features around them (as illustrated above).
The goal is, for each GPS position in the test set (for which we provide the associated aerial images and environmental features), to return a set of candidate species that should contain the true observed species.
Motivation
Automatic prediction of the list of species most likely to be observed at a given location is useful for many scenarios related to biodiversity management and conservation.
First, this would allow to improve species identification tools - automatic, semi-automatic, or based on traditional field guides - by reducing the list of candidate species observable at a given site.
More generally, it could facilitate biodiversity inventories through the development of location-based recommendation services (e.g. on mobile phones), encourage the involvement of citizen scientist observers, and accelerate the annotation and validation of species observations to produce large, high-quality data sets.
Finally, this could be used for educational purposes through biodiversity discovery applications with features such as contextualized educational pathways.
Context
This competition is held jointly as part of:
the LifeCLEF 2022 lab of the CLEF 2022 conference, and of
the FGVC9 workshop organized in conjunction with CVPR 2022 conference.
Being part of scientific research, the participants are encouraged to participate to both event.
In particular, only participants who submitted a working note paper to LifeCLEF (see below) will be part of the officially published ranking used for scientific communication.
FGVC9 at CVPR 2022
This competition is part of the Fine-Grained Visual Categorization FGVC9 workshop at the Computer Vision and Pattern Recognition Conference CVPR 2022.
A panel will review the top submissions for the competition based on the description of the methods provided.
From this, a subset may be invited to present their results at the workshop.
Attending the workshop is not required to participate in the competition; however, only teams that are attending the workshop will be considered to present their work.
CVPR 2022 will take place in New Orleans, USA, 19-24 June 2022.
PLEASE NOTE: CVPR frequently sells out early, we cannot guarantee CVPR registration after the competition's end.
If you are interested in attending, please plan ahead.
You can see a list of all of the FGVC9 competitions here.
LifeCLEF 2022
LifeCLEF lab is part of the Conference and Labs of the Evaluation Forum (CLEF).
CLEF consists of independent peer-reviewed workshops on a broad range of challenges in the fields of multilingual and multimodal information access evaluation, and a set of benchmarking activities carried in various labs designed to test different aspects of mono and cross-language Information retrieval systems.
CLEF 2022 will be hosted by the Università di Bologna, Italy, 5-8 September 2022.
More details can be found on the CLEF 2022 website.
To participate to the LifeCLEF lab, participants must register using this form (and checking ""Task 3 - GeoLifeCLEF"" of ""LifeCLEF"" section).
This registration is free of charge and will close on 22 April 2022, however free of charge late registration will still be possible at the end of the competition.
This will allow those participants to submit, at the end of the competition, a working note paper to LifeCLEF which will be peer-reviewed and published in CEUR-WS proceedings.
This paper should provide sufficient information to reproduce the final submitted runs.
Submitting a working note with the full description of the methods used in each run is mandatory.
Any run that could not be reproduced thanks to its description in the working notes might be removed from the official publication of the results.
Working notes are published within CEUR-WS proceedings, resulting in an assignment of an individual DOI (URN) and an indexing by many bibliography systems including DBLP.
According to the CEUR-WS policies, a light review of the working notes will be conducted by LifeCLEF organizing committee to ensure quality.
As an illustration, LifeCLEF 2021 working notes (task overviews and participant working notes) can be found within CLEF 2021 CEUR-WS proceedings.
Credits
This project has received funding from the French National Research Agency under the Investments for the Future Program, referred to as ANR-16-CONV-0004, and from the European Union’s Horizon 2020 research and innovation program under grant agreement No 863463 (Cos4Cloud project).",6722348 files,61.86 GB,This competition did not award ranking points,This competition did not count towards tiers,Research Prediction Competition
Excellence in Research Award (Phase II),,9 months ago,5 months ago,"['data analytics', 'tabular', 'weather and climate']",,,,"WiDS Datathon 2022: Phase II Excellence in Research
We invite you to build a team, hone your data science skills, and join us for Phase II of the 5th Annual WiDS Datathon focused on social impact!
This year’s WiDS Datathon, organized by the WiDS Worldwide team, Stanford University, Harvard University IACS, and the WiDS Datathon Committee, will address the multi-faceted impacts of climate change. The WiDS Datathon Committee is partnering with experts from many disciplines at Climate Change AI (CCAI), Lawrence Berkeley National Laboratory (Berkeley Lab), US Environmental Protection Agency (EPA), and MIT Critical Data. Phase I of this year's datathon focused on an important way to mitigate the effects of climate change - improving building energy efficiency through forecasting usage. In the WiDS Datathon Excellence in Research Award (Phase II), we will broaden our focus to examine the impacts of climate change across multiple domains.
To qualify for Phase II, participants must:
have participated in the first phase of the WiDS Datathon 2022 on Kaggle
REGISTER for the Excellence in Research Award (Phase II)
Background
Climate change is a globally relevant, urgent, and multi-faceted issue heavily impacting many industries and aspects of public life. Participants in Phase II will have the opportunity to examine the climate change from different perspectives. Participants will choose to explore one dataset among several, spanning sectors including healthcare, energy and environmental protection. Participants will also have opportunities to take deeper dives into their dataset and tackle a range of impactful real-world tasks. Teams will submit a research report at the end of Phase II.
New this year, participants in Phase II can receive mentorship from experts in the domain related to their choice of dataset and task. Domain expert mentorship in Phase II will allow participants to both strengthen their foundational data science skills as well as develop skills needed to conduct research in data science. Teams with outstanding paper submissions from Phase II will be invited to submit their work for publication.
The Excellence in Research Award (Phase II) is open from March 8 - June 30, 2022. Submissions are due on Submittable.
After the June 30, 2022 research paper deadline, submissions will be reviewed for their potential for real-world impact, rigor in scientific methodology, and clarity of communication, by subject matter experts from the WiDS Datathon Committee, the National Science Foundation Big Data Innovation Hubs, and Datathon partners.
Phase II Datathon Partners and Research Tracks
Phase II participants will be able to choose one of three research tracks to explore:
US Environmental Protection Agency (EPA): weather, air pollutant, and census data
MIT Critical Data: CDC county level COVID data
Climate Change AI: Fine grained building energy usage data
Eligibility and Teams
To be eligible for the award, all entrants must have participated in the first phase of the WiDS Datathon 2022 on Kaggle.
We encourage Phase II individuals and author teams of up to 8 people to work together. You may collaborate with individuals in a “new” team for the second phase of the Datathon, as long as (1) each person participated in the first phase of the datathon on Kaggle and (2) your new team still includes 50% or more individuals identifying as women. Each team member must complete Phase II registration.
Note that you do NOT need to merge teams within the Kaggle platform (it's actually disabled), but all team members must be listed as collaborators on your submitted research paper, and all team members must accept the competition rules before the submission deadline of June 30th.
Acknowledgements
The WiDS Datathon Excellence in Research Award 2022 is a collaboration led by the WiDS Worldwide team at Stanford University, the Institute for Applied Computational Sciences at Harvard University and the WiDS Datathon Committee. WiDS Datathon 2022 cash prizes are provided by Kaggle. The Excellence in Research Award is supported by the National Science Foundation under Grants 1916573, 1916481, and 1915774, as part of a network of Big Data Innovation Hubs. Special thanks to our datathon partners Climate Change AI, US Environmental Protection Agency (EPA), and MIT Critical Data.",9 files,66.78 MB,,,Analytics Competition
Tabular Playground Series - Mar 2022,,9 months ago,8 months ago,"['tabular', 'time series analysis', 'cities and urban areas', 'mae']",956,"1,003","9,971","For the March edition of the 2022 Tabular Playground Series you're challenged to forecast twelve-hours of traffic flow in a U.S. metropolis. The time series in this dataset are labelled with both location coordinates and a direction of travel -- a combination of features that will test your skill at spatio-temporal forecasting within a highly dynamic traffic network.
Which model will prevail? The venerable linear regression? The deservedly-popular ensemble of decision trees? Or maybe a cutting-edge graph neural-network? We can't wait to see!
About the Tabular Playground Series
Kaggle competitions are incredibly fun and rewarding, but they can also be intimidating for people who are relatively new in their data science journey. In the past, we've launched many Playground competitions that are more approachable than our Featured competitions and thus, more beginner-friendly.
The goal of these competitions is to provide a fun and approachable-for-anyone tabular dataset to model. These competitions are a great choice for people looking for something in between the Titanic Getting Started competition and the Featured competitions. If you're an established competitions master or grandmaster, these probably won't be much of a challenge for you; thus, we encourage you to avoid saturating the leaderboard.
For each monthly competition, we'll be offering Kaggle Merchandise for the top three teams. And finally, because we want these competitions to be more about learning, we're limiting team sizes to 3 individuals.",3 files,31.4 MB,This competition did not award ranking points,This competition did not count towards tiers,Playground Prediction Competition
Spaceship Titanic,,,,"['beginner', 'tabular', 'binary classification', 'categorizationaccuracy']","2,688","2,952","23,225","Recommended Competition
We highly recommend Titanic - Machine Learning from Disaster to get familiar with the basics of machine learning and Kaggle competitions.
Welcome to the year 2912, where your data science skills are needed to solve a cosmic mystery. We've received a transmission from four lightyears away and things aren't looking good.
The Spaceship Titanic was an interstellar passenger liner launched a month ago. With almost 13,000 passengers on board, the vessel set out on its maiden voyage transporting emigrants from our solar system to three newly habitable exoplanets orbiting nearby stars.
While rounding Alpha Centauri en route to its first destination—the torrid 55 Cancri E—the unwary Spaceship Titanic collided with a spacetime anomaly hidden within a dust cloud. Sadly, it met a similar fate as its namesake from 1000 years before. Though the ship stayed intact, almost half of the passengers were transported to an alternate dimension!
To help rescue crews and retrieve the lost passengers, you are challenged to predict which passengers were transported by the anomaly using records recovered from the spaceship’s damaged computer system.
Help save them and change history!
Acknowledgments
Photos by Joel Filipe, Richard Gatley and ActionVance on Unsplash.",3 files,1.24 MB,This competition does not award ranking points,This competition does not count towards tiers,Getting Started Prediction Competition
March Machine Learning Mania 2022 - Men’s,"$25,000 ",10 months ago,8 months ago,"['sports', 'basketball', 'logloss']",930,"1,025","1,681","Another year, another chance to predict the upsets, call the probabilities, and put your bracketology skills to the leaderboard test. In our eighth annual March Machine Learning Mania competition, Kagglers will once again join the millions of fans who attempt to predict the outcomes of this year's US men's college basketball tournament. But unlike most fans, you will pick the winners and losers using a combination of rich historical data and computing power, while the ground truth unfolds on television.
You're provided data of historical NCAA games and are encouraged to use other sources of publicly available data to gain a winning edge.
In stage one of this two-stage competition, participants will build and test their models against previous tournaments. In the second stage, participants will predict the outcome of the 2022 tournament. You don’t need to participate in the first stage to enter the second. The first stage exists to incentivize model building and provide a means to score predictions. The real competition is forecasting the 2022 results.
And don't forget to take a look at our companion competition that looks to predict the outcome of the US women's college basketball tournament!
Acknowledgments
Banner image by Ben Hershey on Unsplash",40 files,238.26 MB,This competition awarded ranking points,This competition counted towards tiers,Featured Prediction Competition
March Machine Learning Mania 2022 - Women's,"$25,000 ",10 months ago,8 months ago,"['sports', 'basketball', 'logloss']",651,711,"1,203","Another year, another chance to predict the upsets, call the probabilities, and put your bracketology skills to the leaderboard test. In our eighth annual March Machine Learning Mania competition, Kagglers will once again join the millions of fans who attempt to predict the outcomes of this year's US women's college basketball tournament. But unlike most fans, you will pick the winners and losers using a combination of rich historical data and computing power, while the ground truth unfolds on television.
You're provided data of historical NCAA games and are encouraged to use other sources of publicly available data to gain a winning edge.
In stage one of this two-stage competition, participants will build and test their models against previous tournaments. In the second stage, participants will predict the outcome of the 2022 tournament. You don’t need to participate in the first stage to enter the second. The first stage exists to incentivize model building and provide a means to score predictions. The real competition is forecasting the 2022 results.
And don't forget to take a look at our companion competition that looks to predict the outcome of the US men's college basketball tournament!
Acknowledgments
Banner image by Ben Hershey on Unsplash",29 files,25.01 MB,This competition awarded ranking points,This competition counted towards tiers,Featured Prediction Competition
BirdCLEF 2022,"$10,000 ",10 months ago,,"['environment', 'audio', 'weightedcategorizationaccuracy']",807,"1,019","23,352","As the “extinction capital of the world,” Hawai'i has lost 68% of its bird species, the consequences of which can harm entire food chains. Researchers use population monitoring to understand how native birds react to changes in the environment and conservation efforts. But many of the remaining birds across the islands are isolated in difficult-to-access, high-elevation habitats. With physical monitoring difficult, scientists have turned to sound recordings. Known as bioacoustic monitoring, this approach could provide a passive, low labor, and cost-effective strategy for studying endangered bird populations.
Current methods for processing large bioacoustic datasets involve manual annotation of each recording. This requires specialized training and prohibitively large amounts of time. Thankfully, recent advances in machine learning have made it possible to automatically identify bird songs for common species with ample training data. However, it remains challenging to develop such tools for rare and endangered species, such as those in Hawai'i.
The Cornell Lab of Ornithology's K. Lisa Yang Center for Conservation Bioacoustics (KLY-CCB) develops and applies innovative conservation technologies across multiple ecological scales to inspire and inform the conservation of wildlife and habitats. KLY-CCB does this by collecting and interpreting sounds in nature and they've joined forces with Google Bioacoustics Group, LifeCLEF, Listening Observatory for Hawaiian Ecosystems (LOHE) Bioacoustics Lab at the University of Hawai'i at Hilo, and Xeno-Canto for this competition.
In this competition, you’ll use your machine learning skills to identify bird species by sound. Specifically, you'll develop a model that can process continuous audio data and then acoustically recognize the species. The best entries will be able to train reliable classifiers with limited training data.
If successful, you'll help advance the science of bioacoustics and support ongoing research to protect endangered Hawaiian birds. Thanks to your innovations, it will be easier for researchers and conservation practitioners to accurately survey population trends. They'll be able to regularly and more effectively evaluate threats and adjust their conservation actions.
This is a Code Competition. Refer to Code Requirements for details.",14858 files,6.61 GB,This competition awarded ranking points,This competition counted towards tiers,Research Code Competition
Herbarium 2022 - FGVC9,,10 months ago,,"['image', 'plants', 'fscoremacro']",134,174,"1,534","The Herbarium 2022: Flora of North America is a part of a project of the New York Botanical Garden funded by the National Science Foundation to build tools to identify novel plant species around the world. The dataset strives to represent all known vascular plant taxa in North America, using images gathered from 60 different botanical institutions around the world.
In botany, a ‘flora’ is a complete account of the plants found in a geographic region. The dichotomous keys and detailed descriptions of diagnostic morphological features contained within a flora are used by botanists to determine which names to apply to plant specimens. This year's competition dataset aims to encapsulate the flora of North America so that we can test the capability of artificial intelligence to replicate this traditional tool —a crucial first step to harnessing AI’s potential botanical applications.
The Herbarium 2022: Flora of North America dataset comprises 1.05 M images of 15,501 vascular plants, which constitute more than 90% of the taxa documented in North America. Our dataset is constrained to include only vascular land plants (lycophytes, ferns, gymnosperms, and flowering plants).
Our dataset has a long-tail distribution. The number of images per taxon is as few as seven and as many as 100 images. Although more images are available, we capped the maximum number in an attempt to ensure sufficient but manageable training data size for competition participants.
About
This is an FGVC competition hosted as part of the FGVC9 workshop at CVPR 2022 and sponsored by NYBG.
Details of this competition are mirrored on the github page. Please post in the forum or open an issue if you have any questions or problems with the dataset.
Acknowledgements
The images are provided by the New York Botanical Garden and 59 other institutions around the world.",1050182 files,163.17 GB,This competition did not award ranking points,This competition did not count towards tiers,Research Prediction Competition
H&M Personalized Fashion Recommendations,"$50,000 ",10 months ago,,"['retail and shopping', 'recommender systems', 'map@{k}']","2,952","3,759","38,854","H&M Group is a family of brands and businesses with 53 online markets and approximately 4,850 stores. Our online store offers shoppers an extensive selection of products to browse through. But with too many choices, customers might not quickly find what interests them or what they are looking for, and ultimately, they might not make a purchase. To enhance the shopping experience, product recommendations are key. More importantly, helping customers make the right choices also has a positive implications for sustainability, as it reduces returns, and thereby minimizes emissions from transportation.
In this competition, H&M Group invites you to develop product recommendations based on data from previous transactions, as well as from customer and product meta data. The available meta data spans from simple data, such as garment type and customer age, to text data from product descriptions, to image data from garment images.
There are no preconceptions on what information that may be useful – that is for you to find out. If you want to investigate a categorical data type algorithm, or dive into NLP and image processing deep learning, that is up to you.",105104 files,34.56 GB,This competition awarded ranking points,This competition counted towards tiers,Featured Prediction Competition
NBME - Score Clinical Patient Notes,"$50,000 ",10 months ago,,"['education', 'nlp', 'text', 'medicine', 'custom metric']","1,471","1,880","28,049","When you visit a doctor, how they interpret your symptoms can determine whether your diagnosis is accurate. By the time they’re licensed, physicians have had a lot of practice writing patient notes that document the history of the patient’s complaint, physical exam findings, possible diagnoses, and follow-up care. Learning and assessing the skill of writing patient notes requires feedback from other doctors, a time-intensive process that could be improved with the addition of machine learning.
Until recently, the Step 2 Clinical Skills examination was one component of the United States Medical Licensing Examination® (USMLE®). The exam required test-takers to interact with Standardized Patients (people trained to portray specific clinical cases) and write a patient note. Trained physician raters later scored patient notes with rubrics that outlined each case’s important concepts (referred to as features). The more such features found in a patient note, the higher the score (among other factors that contribute to the final score for the exam).
However, having physicians score patient note exams requires significant time, along with human and financial resources. Approaches using natural language processing have been created to address this problem, but patient notes can still be challenging to score computationally because features may be expressed in many ways. For example, the feature ""loss of interest in activities"" can be expressed as ""no longer plays tennis."" Other challenges include the need to map concepts by combining multiple text segments, or cases of ambiguous negation such as “no cold intolerance, hair loss, palpitations, or tremor” corresponding to the key essential “lack of other thyroid symptoms.”
In this competition, you’ll identify specific clinical concepts in patient notes. Specifically, you'll develop an automated method to map clinical concepts from an exam rubric (e.g., “diminished appetite”) to various ways in which these concepts are expressed in clinical patient notes written by medical students (e.g., “eating less,” “clothes fit looser”). Great solutions will be both accurate and reliable.
If successful, you'll help tackle the biggest practical barriers in patient note scoring, making the approach more transparent, interpretable, and easing the development and administration of such assessments. As a result, medical practitioners will be able to explore the full potential of patient notes to reveal information relevant to clinical skills assessment.
This competition is sponsored by the National Board of Medical Examiners® (NBME®). Through research and innovation, NBME supports medical school and residency program educators in addressing issues around the evolution of teaching, learning, technology, and the need for meaningful feedback. NBME offers high-quality assessments and educational services for students, professionals, educators, regulators, and institutions dedicated to the evolving needs of medical education and health care. To serve these communities, NBME collaborates with a diverse and comprehensive array of practicing health professionals, medical educators, state medical board members, test developers, academic researchers, scoring experts and public representatives.
NBME gratefully acknowledges the valuable input of Dr Le An Ha from the University of Wolverhampton’s Research Group in Computational Linguistics.

This is a Code Competition. Refer to Code Requirements for details.",5 files,35.73 MB,This competition awarded ranking points,This competition counted towards tiers,Featured Code Competition
Happywhale - Whale and Dolphin Identification,"$25,000 ",10 months ago,,"['image', 'animals', 'map@{k}']","1,588","2,070","39,284","We use fingerprints and facial recognition to identify people, but can we use similar approaches with animals? In fact, researchers manually track marine life by the shape and markings on their tails, dorsal fins, heads and other body parts. Identification by natural markings via photographs—known as photo-ID—is a powerful tool for marine mammal science. It allows individual animals to be tracked over time and enables assessments of population status and trends. With your help to automate whale and dolphin photo-ID, researchers can reduce image identification times by over 99%. More efficient identification could enable a scale of study previously unaffordable or impossible.
Currently, most research institutions rely on time-intensive—and sometimes inaccurate—manual matching by the human eye. Thousands of hours go into manual matching, which involves staring at photos to compare one individual to another, finding matches, and identifying new individuals. While researchers enjoy looking at a whale photo or two, manual matching limits the scope and reach.
Algorithms developed in this competition will be implemented in Happywhale, a research collaboration and citizen science web platform. Its mission is to increase global understanding and caring for marine environments through high quality conservation science and education. Happywhale aims to make it easy and rewarding for the public to participate in science by building innovative tools to engage anyone interested in marine mammals. The platform also serves the research community with powerful collaborative tools.
In this competition, you’ll develop a model to match individual whales and dolphins by unique—but often subtle—characteristics of their natural markings. You'll pay particular attention to dorsal fins and lateral body views in image sets from a multi-species dataset built by 28 research institutions. The best submissions will suggest photo-ID solutions that are fast and accurate.
If successful, you'll have a hand in building advanced technology to better understand and manage the impact on the world’s changing oceans. Previous automation attempts resulted in a global database of over 50,000 whales and an agreement with cruise ships to operate at a maximum speed of 11 mph in the most whale-rich region. Your ideas to automate the identification of marine life will help overcome increasing human impacts on oceans, providing a critical tool for conservation science. If there's a whale, there's a way!",78991 files,62.06 GB,This competition awarded ranking points,This competition counted towards tiers,Research Prediction Competition
Tabular Playground Series - Feb 2022,,10 months ago,9 months ago,"['tabular', 'categorizationaccuracy']","1,255","1,312","11,766","For the February 2022 Tabular Playground Series competition, your task is to classify 10 different bacteria species using data from a genomic analysis technique that has some data compression and data loss. In this technique, 10-mer snippets of DNA are sampled and analyzed to give the histogram of base count. In other words, the DNA segment
ATATGGCCTT
becomes
A
. Can you use this lossy information to accurately predict bacteria species?
About the Tabular Playground Series
Kaggle competitions are incredibly fun and rewarding, but they can also be intimidating for people who are relatively new in their data science journey. In the past, we've launched many Playground competitions that are more approachable than our Featured competitions and thus, more beginner-friendly.
The goal of these competitions is to provide a fun and approachable-for-anyone tabular dataset to model. These competitions are a great choice for people looking for something in between the Titanic Getting Started competition and the Featured competitions. If you're an established competitions master or grandmaster, these probably won't be much of a challenge for you; thus, we encourage you to avoid saturating the leaderboard.
For each monthly competition, we'll be offering Kaggle Merchandise for the top three teams. And finally, because we want these competitions to be more about learning, we're limiting team sizes to 3 individuals.
Getting Started
For ideas on how to improve your score, check out the Intro to Machine Learning and Intermediate Machine Learning courses on Kaggle Learn.
Good luck and have fun!
Acknowledgements
The idea for this competition came from the following paper:
@ARTICLE{10.3389/fmicb.2020.00257,
AUTHOR={Wood, Ryan L. and Jensen, Tanner and Wadsworth, Cindi and Clement, Mark and Nagpal, Prashant and Pitt, William G.},   
TITLE={Analysis of Identification Method for Bacterial Species and Antibiotic Resistance Genes Using Optical Data From DNA Oligomers},      
JOURNAL={Frontiers in Microbiology},      
VOLUME={11},      
YEAR={2020},      
URL={https://www.frontiersin.org/article/10.3389/fmicb.2020.00257},       
DOI={10.3389/fmicb.2020.00257},      
ISSN={1664-302X}}",3 files,1.87 GB,This competition did not award ranking points,This competition did not count towards tiers,Playground Prediction Competition
Ubiquant Market Prediction,"$100,000 ",a year ago,,"['tabular', 'finance', 'meanpearson']","2,893","4,159",,"Regardless of your investment strategy, fluctuations are expected in the financial market. Despite this variance, professional investors try to estimate their overall returns. Risks and returns differ based on investment types and other factors, which impact stability and volatility. To attempt to predict returns, there are many computer-based algorithms and models for financial market trading. Yet, with new techniques and approaches, data science could improve quantitative researchers' ability to forecast an investment's return.
Ubiquant Investment (Beijing) Co., Ltd is a leading domestic quantitative hedge fund based in China. Established in 2012, they rely on international talents in math and computer science along with cutting-edge technology to drive quantitative financial market investment. Overall, Ubiquant is committed to creating long-term stable returns for investors.
In this competition, you’ll build a model that forecasts an investment's return rate. Train and test your algorithm on historical prices. Top entries will solve this real-world data science problem with as much accuracy as possible.
If successful, you could improve the ability of quantitative researchers to forecast returns. This will enable investors at any scale to make better decisions. You may even discover you have a knack for financial datasets, opening up a world of new opportunities in many industries.
See more information about Ubiquant below:
This is a Code Competition. Refer to Code Requirements for details.",1 files,173 B,This competition awarded ranking points,This competition counted towards tiers,Featured Code Competition
Tabular Playground Series - Jan 2022,,a year ago,10 months ago,"['tabular', 'time series analysis', 'smape']","1,591","1,646","16,151","We've heard your feedback from the 2021 Tabular Playground Series, and now Kaggle needs your help going forward in 2022!
There are two (fictitious) independent store chains selling Kaggle merchandise that want to become the official outlet for all things Kaggle. We've decided to see if the Kaggle community could help us figure out which of the store chains would have the best sales going forward. So, we've collected some data and are asking you to build forecasting models to help us decide.
Help us figure out whether KaggleMart or KaggleRama should become the official Kaggle outlet!
About the Tabular Playground Series
Kaggle competitions are incredibly fun and rewarding, but they can also be intimidating for people who are relatively new in their data science journey. In the past, we've launched many Playground competitions that are more approachable than our Featured competitions and thus, more beginner-friendly.
The goal of these competitions is to provide a fun and approachable-for-anyone tabular dataset to model. These competitions are a great choice for people looking for something in between the Titanic Getting Started competition and the Featured competitions. If you're an established competitions master or grandmaster, these probably won't be much of a challenge for you; thus, we encourage you to avoid saturating the leaderboard.
For each monthly competition, we'll be offering Kaggle Merchandise for the top three teams. And finally, because we want these competitions to be more about learning, we're limiting team sizes to 3 individuals.",3 files,1.73 MB,This competition did not award ranking points,This competition did not count towards tiers,Playground Prediction Competition
Feedback Prize - Evaluating Student Writing,"$160,000 ",a year ago,,"['nlp', 'text', 'primary and secondary schools', 'custom metric']","2,058","2,598","33,831","Writing is a critical skill for success. However, less than a third of high school seniors are proficient writers, according to the National Assessment of Educational Progress. Unfortunately, low-income, Black, and Hispanic students fare even worse, with less than 15 percent demonstrating writing proficiency. One way to help students improve their writing is via automated feedback tools, which evaluate student writing and provide personalized feedback.
There are currently numerous automated writing feedback tools, but they all have limitations. Many often fail to identify writing structures, such as thesis statements and support for claims, in essays or do not do so thoroughly. Additionally, the majority of the available tools are proprietary, with algorithms and feature claims that cannot be independently backed up. More importantly, many of these writing tools are inaccessible to educators because of their cost. This problem is compounded for under-serviced schools which serve a disproportionate number of students of color and from low-income backgrounds. In short, the field of automated writing feedback is ripe for innovation that could help democratize education.
Georgia State University (GSU) is an undergraduate and graduate urban public research institution in Atlanta. U.S. News & World Report ranked GSU as one of the most innovative universities in the nation. GSU awards more bachelor’s degrees to African-Americans than any other non-profit college or university in the country. GSU and The Learning Agency Lab, an independent nonprofit based in Arizona, are focused on developing science of learning-based tools and programs for social good.
In this competition, you’ll identify elements in student writing. More specifically, you will automatically segment texts and classify argumentative and rhetorical elements in essays written by 6th-12th grade students. You'll have access to the largest dataset of student writing ever released in order to test your skills in natural language processing, a fast-growing area of data science.
If successful, you'll make it easier for students to receive feedback on their writing and increase opportunities to improve writing outcomes. Virtual writing tutors and automated writing systems can leverage these algorithms while teachers may use them to reduce grading time. The open-sourced algorithms you come up with will allow any educational organization to better help young writers develop.
Acknowledgements
Georgia State University and the Learning Agency Lab would like to thank the Bill & Melinda Gates Foundation, Schmidt Futures and Chan Zuckerberg Initiative for their support in making this work possible.
                         
This is a Code Competition. Refer to Code Requirements for details.",15601 files,104.42 MB,This competition awarded ranking points,This competition counted towards tiers,Featured Code Competition
Tabular Playground Series - Dec 2021,,a year ago,a year ago,"['tabular', 'categorizationaccuracy']","1,188","1,234","12,524","Kaggle competitions are incredibly fun and rewarding, but they can also be intimidating for people who are relatively new in their data science journey. In the past, we've launched many Playground competitions that are more approachable than our Featured competitions and thus, more beginner-friendly.
In order to have a more consistent offering of these competitions for our community, we're trying a new experiment in 2021. We'll be launching month-long tabular Playground competitions on the 1st of every month and continue the experiment as long as there's sufficient interest and participation.
The goal of these competitions is to provide a fun, and approachable for anyone, tabular dataset. These competitions will be great for people looking for something in between the Titanic Getting Started competition and a Featured competition. If you're an established competitions master or grandmaster, these probably won't be much of a challenge for you. We encourage you to avoid saturating the leaderboard.
For each monthly competition, we'll be offering Kaggle Merchandise for the top three teams. And finally, because we want these competitions to be more about learning, we're limiting team sizes to 3 individuals.
The dataset is used for this competition is synthetic, but based on a real dataset and generated using a CTGAN. This dataset is based off of the original Forest Cover Type Prediction competition.
Good luck and have fun!
For ideas on how to improve your score, check out the Intro to Machine Learning and Intermediate Machine Learning courses on Kaggle Learn.",3 files,693.17 MB,This competition did not award ranking points,This competition did not count towards tiers,Playground Prediction Competition
TensorFlow - Help Protect the Great Barrier Reef,"$150,000 ",a year ago,,"['earth and nature', 'image', 'custom metric']","2,025","2,608","60,934","Goal of the Competition
The goal of this competition is to accurately identify starfish in real-time by building an object detection model trained on underwater videos of coral reefs.
Your work will help researchers identify species that are threatening Australia's Great Barrier Reef and take well-informed action to protect the reef for future generations.
Context
Australia's stunningly beautiful Great Barrier Reef is the world’s largest coral reef and home to 1,500 species of fish, 400 species of corals, 130 species of sharks, rays, and a massive variety of other sea life.
Unfortunately, the reef is under threat, in part because of the overpopulation of one particular starfish – the coral-eating crown-of-thorns starfish (or COTS for short). Scientists, tourism operators and reef managers established a large-scale intervention program to control COTS outbreaks to ecologically sustainable levels.
To know where the COTS are, a traditional reef survey method, called ""Manta Tow"", is performed by a snorkel diver. While towed by a boat, they visually assess the reef, stopping to record variables observed every 200m. While generally effective, this method faces clear limitations, including operational scalability, data resolution, reliability, and traceability.
The Great Barrier Reef Foundation established an innovation program to develop new survey and intervention methods to provide a step change in COTS Control. Underwater cameras will collect thousands of reef images and AI technology could drastically improve the efficiency and scale at which reef managers detect and control COTS outbreaks.
To scale up video-based surveying systems, Australia’s national science agency, CSIRO has teamed up with Google to develop innovative machine learning technology that can analyse large image datasets accurately, efficiently, and in near real-time.

This is a Code Competition. Refer to Code Requirements for details.
Citation
Please cite this short paper if you are using this dataset for research purposes.
@misc{liu2021csiro,
      title={The CSIRO Crown-of-Thorn Starfish Detection Dataset}, 
      author={Jiajun Liu and Brano Kusy and Ross Marchant and Brendan Do and Torsten Merz and Joey Crosswell and Andy Steven and Nic Heaney and Karl von Richter and Lachlan Tychsen-Smith and David Ahmedt-Aristizabal and Mohammad Ali Armin and Geoffrey Carlin and Russ Babcock and Peyman Moghadam and Daniel Smith and Tim Davis and Kemal El Moujahid and Martin Wicke and Megha Malpani},
      year={2021},
      eprint={2111.14311},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}",23507 files,15.23 GB,This competition awarded ranking points,This competition counted towards tiers,Research Code Competition
Santa 2021 - The Merry Movie Montage,"$25,000 ",a year ago,,"['movies and tv shows', 'optimization', 'holidays and cultural events', 'custom metric']",867,"1,003","6,849","You’ve gotta watch out
You probably will cry
You’ll smile throughout
I'm telling you why
SantaTV’s coming to town
So give them a list,
They’re watching it thrice
They’re gonna find out what’s sugar, what’s spice.
SantaTV’s is coming to town
The elves have lots of movies
Let’s hope they stay awake
Cause Christmas season’s coming soon
And there’s toys for them to make!


People seem to be getting in the Christmas spirit earlier and earlier each year. Decorations appear for sale in stores in the fall, Christmas songs are on the radio in October…
The Elves at the North Pole are starting to recognize this, and need to work as fast as possible to launch their latest holiday offering: SantaTV+! A 24/7 streaming television channel where it’s “Always Christmas, All the Time.” To debut their new station, they’ve decided to kick things off with a made-for-television Christmas movie marathon! They’re excited for the premiere of such movies as 🎅, 🤶, 🦌, 🧝, 🎄, 🎁, and 🎀!
But elves know that just as important as the movie themselves is the order they’ll be aired. So the elves have decided the best way to figure out which order is best is to watch all the movies in every possible combination to see which feels the most Christmas-y.
Your job is to help the elves by giving them the shortest viewing schedules that shows them every combination of movies so they can get SantaTV+ live as soon as possible! The elves have formed three movie-watching teams to lighten the load, so every combination must be seen by at least one of their groups. But they’re also pretty sure they want to kick off the movie marathon with the 🎅 and 🤶 movies back-to-back, so be sure that each group has all the combinations that start with those. And finally, the elves have agreed to two sugar breaks, so you’re allowed to give each group up to two 🌟 wildcards, which will play all the movies at once while they’re snacking, which will help speed things along.
They can’t launch SantaTV+ until all the groups have finished watching - so help give them the most efficient schedule to see every Christmas movie combination, and help them get back to making toys!
Acknowledgments
Photos by Erwan Hesry and Diljaz TM on Unsplash.",4 files,53.33 MB,This competition awarded ranking points,This competition counted towards tiers,Featured Prediction Competition
Jigsaw Rate Severity of Toxic Comments,"$50,000 ",a year ago,,"['nlp', 'text', 'custom metric']","2,301","2,880","49,047","In Jigsaw's fourth Kaggle competition, we return to the Wikipedia Talk page comments featured in our first Kaggle competition. When we ask human judges to look at individual comments, without any context, to decide which ones are toxic and which ones are innocuous, it is rarely an easy task. In addition, each individual may have their own bar for toxicity. We've tried to work around this by aggregating the decisions with a majority vote. But many researchers have rightly pointed out that this discards meaningful information.
😄 🙂 😐 😕 😞
A much easier task is to ask individuals which of two comments they find more toxic. But if both comments are non-toxic, people will often select randomly. When one comment is obviously the correct choice, the inter-annotator agreement results are much higher.
In this competition, we will be asking you to score a set of about fourteen thousand comments. Pairs of comments were presented to expert raters, who marked one of two comments more harmful — each according to their own notion of toxicity. In this contest, when you provide scores for comments, they will be compared with several hundred thousand rankings. Your average agreement with the raters will determine your individual score. In this way, we hope to focus on ranking the severity of comment toxicity from innocuous to outrageous, where the middle matters as much as the extremes.
Can you build a model that produces scores that rank each pair of comments the same way as our professional raters?
Disclaimer: The dataset for this competition contains text that may be considered profane, vulgar, or offensive.
Related Work
The paper ""Ruddit: Norms of Offensiveness for English Reddit Comments"" by Hada et al. introduced a similar dataset that involved tuples of four sentences that were marked with best-worst scoring, and this data may be directly useful for building models.
We also note ""Constructing Interval Variables via Faceted Rasch Measurement and Multitask Deep Learning: a Hate Speech Application"" by Kennedy et al. which compares a variety of different rating schemes and argues that binary classification as typically done in NLP tasks discards valuable information. Combining data from multiple sources, even with different annotation guidelines, may be essential for success in this competition.
Resources
The English language resources from our first Kaggle competition, and our second Kaggle competition, which are both available in the TensorFlow datasets Wikipedia Toxicity Subtypes and Civil Comments can be used to build models.
One example of a starting point is the open source UnitaryAI model.
Google Jigsaw
Google's Jigsaw team explores threats to open societies and builds technology that inspires scalable solutions. One Jigsaw product is PerspectiveAPI which is used by publishers and platforms worldwide as part of their overall moderation strategy.

This is a Code Competition. Refer to Code Requirements for details.",3 files,35.37 MB,This competition awarded ranking points,This competition counted towards tiers,Featured Code Competition
G-Research Crypto Forecasting,"$125,000 ",a year ago,,"['tabular', 'finance', 'time series analysis', 'custom metric']","1,946","2,398","3,141","Over $40 billion worth of cryptocurrencies are traded every day. They are among the most popular assets for speculation and investment, yet have proven wildly volatile. Fast-fluctuating prices have made millionaires of a lucky few, and delivered crushing losses to others. Could some of these price movements have been predicted in advance?
In this competition, you'll use your machine learning expertise to forecast short term returns in 14 popular cryptocurrencies. We have amassed a dataset of millions of rows of high-frequency market data dating back to 2018 which you can use to build your model. Once the submission deadline has passed, your final score will be calculated over the following 3 months using live crypto data as it is collected.
The simultaneous activity of thousands of traders ensures that most signals will be transitory, persistent alpha will be exceptionally difficult to find, and the danger of overfitting will be considerable. In addition, since 2018, interest in the cryptomarket has exploded, so the volatility and correlation structure in our data are likely to be highly non-stationary. The successful contestant will pay careful attention to these considerations, and in the process gain valuable insight into the art and science of financial forecasting.
G-Research is Europe’s leading quantitative finance research firm. We have long explored the extent of market prediction possibilities, making use of machine learning, big data, and some of the most advanced technology available. Specializing in data science and AI education for workforces, Cambridge Spark is partnering with G-Research for this competition. Watch our introduction to the competition below:

This is a Code Competition. Refer to Code Requirements for details.",7 files,3.12 GB,This competition awarded ranking points,This competition counted towards tiers,Featured Code Competition
Tabular Playground Series - Nov 2021,,a year ago,a year ago,"['tabular', 'auc']","1,362","1,422","14,710","Kaggle competitions are incredibly fun and rewarding, but they can also be intimidating for people who are relatively new in their data science journey. In the past, we've launched many Playground competitions that are more approachable than our Featured competitions and thus, more beginner-friendly.
In order to have a more consistent offering of these competitions for our community, we're trying a new experiment in 2021. We'll be launching month-long tabular Playground competitions on the 1st of every month and continue the experiment as long as there's sufficient interest and participation.
The goal of these competitions is to provide a fun, and approachable for anyone, tabular dataset. These competitions will be great for people looking for something in between the Titanic Getting Started competition and a Featured competition. If you're an established competitions master or grandmaster, these probably won't be much of a challenge for you. We encourage you to avoid saturating the leaderboard.
For each monthly competition, we'll be offering Kaggle Merchandise for the top three teams. And finally, because we want these competitions to be more about learning, we're limiting team sizes to 3 individuals.
The dataset is used for this competition is synthetic, but based on a real dataset and generated using a CTGAN. The original dataset deals with predicting identifying spam emails via various extracted features from the email. Although the features are anonymized, they have properties relating to real-world features.
Good luck and have fun!
For ideas on how to improve your score, check out the Intro to Machine Learning and Intermediate Machine Learning courses on Kaggle Learn.",3 files,1.04 GB,This competition did not award ranking points,This competition did not count towards tiers,Playground Prediction Competition
Sartorius - Cell Instance Segmentation,"$75,000 ",a year ago,,"['image', 'biology', 'custom metric']","1,505","1,984","32,813","Neurological disorders, including neurodegenerative diseases such as Alzheimer's and brain tumors, are a leading cause of death and disability across the globe. However, it is hard to quantify how well these deadly disorders respond to treatment. One accepted method is to review neuronal cells via light microscopy, which is both accessible and non-invasive. Unfortunately, segmenting individual neuronal cells in microscopic images can be challenging and time-intensive. Accurate instance segmentation of these cells—with the help of computer vision—could lead to new and effective drug discoveries to treat the millions of people with these disorders.
Current solutions have limited accuracy for neuronal cells in particular. In internal studies to develop cell instance segmentation models, the neuroblastoma cell line SH-SY5Y consistently exhibits the lowest precision scores out of eight different cancer cell types tested. This could be because neuronal cells have a very unique, irregular and concave morphology associated with them, making them challenging to segment with commonly used mask heads.
Sartorius is a partner of the life science research and the biopharmaceutical industry. They empower scientists and engineers to simplify and accelerate progress in life science and bioprocessing, enabling the development of new and better therapies and more affordable medicine. They're a magnet and dynamic platform for pioneers and leading experts in the field. They bring creative minds together for a common goal: technological breakthroughs that lead to better health for more people.
In this competition, you’ll detect and delineate distinct objects of interest in biological images depicting neuronal cell types commonly used in the study of neurological disorders. More specifically, you'll use phase contrast microscopy images to train and test your model for instance segmentation of neuronal cells. Successful models will do this with a high level of accuracy.
If successful, you'll help further research in neurobiology thanks to the collection of robust quantitative data. Researchers may be able to use this to more easily measure the effects of disease and treatment conditions on neuronal cells. As a result, new drugs could be discovered to treat the millions of people with these leading causes of death and disability.

This is a Code Competition. Refer to Code Requirements for details.",8457 files,3.83 GB,This competition awarded ranking points,This competition counted towards tiers,Featured Code Competition
2021 Kaggle Machine Learning & Data Science Survey,"$30,000 ",a year ago,a year ago,"['data analytics', 'online communities', 'survey analysis']",,,,"Welcome to Kaggle's annual Machine Learning and Data Science Survey competition! You can read our executive summary here.
This year, as in 2017, 2018, 2019, and 2020 we set out to conduct an industry-wide survey that presents a truly comprehensive view of the state of data science and machine learning. The survey was live from 09/01/2021 to 10/04/2021, and after cleaning the data we finished with 25,973 responses!
There's a lot to explore here. The results include raw numbers about who is working with data, what’s happening with machine learning in different industries, and the best ways for new data scientists to break into the field. We've published the data in as raw a format as possible without compromising anonymization, which makes it an unusual example of a survey dataset.
This year Kaggle is once again launching an annual Data Science Survey Challenge, where we will be awarding a prize pool of $30,000 to notebook authors who tell a rich story about a subset of the data science and machine learning community.
In our fifth year running this survey, we were once again awed by the global, diverse, and dynamic nature of the data science and machine learning industry. This survey data EDA provides an overview of the industry on an aggregate scale, but it also leaves us wanting to know more about the many specific communities comprised within the survey. For that reason, we’re inviting the Kaggle community to dive deep into the survey datasets and help us tell the diverse stories of data scientists from around the world.
The challenge objective: tell a data story about a subset of the data science community represented in this survey, through a combination of both narrative text and data exploration. A “story” could be defined any number of ways, and that’s deliberate. The challenge is to deeply explore (through data) the impact, priorities, or concerns of a specific group of data science and machine learning practitioners. That group can be defined in the macro (for example: anyone who does most of their coding in Python) or the micro (for example: female data science students studying machine learning in masters programs). This is an opportunity to be creative and tell the story of a community you identify with or are passionate about!
Submissions will be evaluated on the following:
Composition - Is there a clear narrative thread to the story that’s articulated and supported by data? The subject should be well defined, well researched, and well supported through the use of data and visualizations.
Originality - Does the reader learn something new through this submission? Or is the reader challenged to think about something in a new way? A great entry will be informative, thought provoking, and fresh all at the same time.
Documentation - Are your code, and notebook, and additional data sources well documented so a reader can understand what you did? Are your sources clearly cited? A high quality analysis should be concise and clear at each step so the rationale is easy to follow and the process is reproducible
To be valid, a submission must be contained in one notebook, made public on or before the submission deadline. Participants are free to use any datasets in addition to the Kaggle Data Science survey, but those datasets must also be publicly available on Kaggle by the deadline for a submission to be valid. You can make your submission by filling out the submission form.",3 files,35.42 MB,,,Analytics Competition
Store Sales - Time Series Forecasting,,,,"['beginner', 'tabular', 'time series analysis', 'rmsle']",793,972,"5,194","Goal of the Competition
In this “getting started” competition, you’ll use time-series forecasting to forecast store sales on data from Corporación Favorita, a large Ecuadorian-based grocery retailer.
Specifically, you'll build a model that more accurately predicts the unit sales for thousands of items sold at different Favorita stores. You'll practice your machine learning skills with an approachable training dataset of dates, store, and item information, promotions, and unit sales.
Get Started
We highly recommend the Time Series course, which walks you through how to make your first submission. The lessons in this course are inspired by winning solutions from past Kaggle time series forecasting competitions.
Context
Forecasts aren’t just for meteorologists. Governments forecast economic growth. Scientists attempt to predict the future population. And businesses forecast product demand—a common task of professional data scientists. Forecasts are especially relevant to brick-and-mortar grocery stores, which must dance delicately with how much inventory to buy. Predict a little over, and grocers are stuck with overstocked, perishable goods. Guess a little under, and popular items quickly sell out, leading to lost revenue and upset customers. More accurate forecasting, thanks to machine learning, could help ensure retailers please customers by having just enough of the right products at the right time.
Current subjective forecasting methods for retail have little data to back them up and are unlikely to be automated. The problem becomes even more complex as retailers add new locations with unique needs, new products, ever-transitioning seasonal tastes, and unpredictable product marketing.
Potential Impact
If successful, you'll have flexed some new skills in a real world example. For grocery stores, more accurate forecasting can decrease food waste related to overstocking and improve customer satisfaction. The results of this ongoing competition, over time, might even ensure your local store has exactly what you need the next time you shop.",7 files,124.76 MB,This competition does not award ranking points,This competition does not count towards tiers,Getting Started Prediction Competition
Tabular Playground Series - Oct 2021,,a year ago,a year ago,"['tabular', 'binary classification', 'auc']","1,089","1,124","10,119","Kaggle competitions are incredibly fun and rewarding, but they can also be intimidating for people who are relatively new in their data science journey. In the past, we've launched many Playground competitions that are more approachable than our Featured competitions and thus, more beginner-friendly.
In order to have a more consistent offering of these competitions for our community, we're trying a new experiment in 2021. We'll be launching month-long tabular Playground competitions on the 1st of every month and continue the experiment as long as there's sufficient interest and participation.
The goal of these competitions is to provide a fun, and approachable for anyone, tabular dataset. These competitions will be great for people looking for something in between the Titanic Getting Started competition and a Featured competition. If you're an established competitions master or grandmaster, these probably won't be much of a challenge for you. We encourage you to avoid saturating the leaderboard.
For each monthly competition, we'll be offering Kaggle Merchandise for the top three teams. And finally, because we want these competitions to be more about learning, we're limiting team sizes to 3 individuals.
The dataset is used for this competition is synthetic, but based on a real dataset and generated using a CTGAN. The original dataset deals with predicting the biological response of molecules given various chemical properties. Although the features are anonymized, they have properties relating to real-world features.
Good luck and have fun!
For ideas on how to improve your score, check out the Intro to Machine Learning and Intermediate Machine Learning courses on Kaggle Learn.",3 files,3.49 GB,This competition did not award ranking points,This competition did not count towards tiers,Playground Prediction Competition
NFL Big Data Bowl 2022,"$100,000 ",a year ago,a year ago,,,,,"Before National Football League (NFL) coaches celebrate a big W, they strategize ways to improve field position and score points. Both of these objectives receive significant contributions from special teams plays, which consist of punts, kickoffs, field goals and extra points. These play types take on important roles in a game’s final score—so much so that coaches say they're a third of the game. Yet special teams remain an understudied part of American football, with an opportunity for data science to offer better ways to understand its impact.
The 2022 Big Data Bowl creates the opportunity for you (and the world!) to learn more about special teams play than ever before. We've provided the NFL's Next Gen Stats (NGS) tracking data from all 2018-2020 special teams plays. This data provides location information for each special teams player, wherever they are on the field, and includes their speed, acceleration, and direction. Additionally, and for the first time in Big Data Bowl history, participants can utilize scouting data from PFF, which supplements the tracking data with football specific metrics that coaches find critical to team success.
The NFL is America's most popular sports league. Founded in 1920, the organization behind American football has developed the model for the successful modern sports league. They're committed to advancing every aspect of the game, including the lesser researched special teams. In this competition, you’ll quantify what happens on special teams plays. You might create a new special teams metric, quantify team or individual strategies, rank players, or even something we haven’t considered.
With your creativity and analytical skills, the development of these new methods could lead to additional stats for special teams plays. If successful, your effort may even be adopted by the NFL for on air distribution, and you can watch future games knowing you had a hand in improving America's most popular sports league.",7 files,5 GB,,,Analytics Competition
PetFinder.my - Pawpularity Contest,"$25,000 ",a year ago,,"['image', 'rmse']","3,537","4,334","47,037","  A picture is worth a thousand words. But did you know a picture can save a thousand lives? Millions of stray animals suffer on the streets or are euthanized in shelters every day around the world. You might expect pets with attractive photos to generate more interest and be adopted faster. But what makes a good picture? With the help of data science, you may be able to accurately determine a pet photo’s appeal and even suggest improvements to give these rescue animals a higher chance of loving homes.
PetFinder.my is Malaysia’s leading animal welfare platform, featuring over 180,000 animals with 54,000 happily adopted. PetFinder collaborates closely with animal lovers, media, corporations, and global organizations to improve animal welfare.
Currently, PetFinder.my uses a basic Cuteness Meter to rank pet photos. It analyzes picture composition and other factors compared to the performance of thousands of pet profiles. While this basic tool is helpful, it's still in an experimental stage and the algorithm could be improved.
In this competition, you’ll analyze raw images and metadata to predict the “Pawpularity” of pet photos. You'll train and test your model on PetFinder.my's thousands of pet profiles. Winning versions will offer accurate recommendations that will improve animal welfare.
If successful, your solution will be adapted into AI tools that will guide shelters and rescuers around the world to improve the appeal of their pet profiles, automatically enhancing photo quality and recommending composition improvements. As a result, stray dogs and cats can find their ""furever"" homes much faster. With a little assistance from the Kaggle community, many precious lives could be saved and more happy families created.
Top participants may be invited to collaborate on implementing their solutions and creatively improve global animal welfare with their AI skills.
 
This is a Code Competition. Refer to Code Requirements for details.",9923 files,1.04 GB,This competition awarded ranking points,This competition counted towards tiers,Research Code Competition
Google Brain - Ventilator Pressure Prediction,"$7,500 ",a year ago,,"['tabular', 'biology', 'medicine', 'mae']","2,605","3,118","46,281","What do doctors do when a patient has trouble breathing? They use a ventilator to pump oxygen into a sedated patient's lungs via a tube in the windpipe. But mechanical ventilation is a clinician-intensive procedure, a limitation that was prominently on display during the early days of the COVID-19 pandemic. At the same time, developing new methods for controlling mechanical ventilators is prohibitively expensive, even before reaching clinical trials. High-quality simulators could reduce this barrier.
Current simulators are trained as an ensemble, where each model simulates a single lung setting. However, lungs and their attributes form a continuous space, so a parametric approach must be explored that would consider the differences in patient lungs.
Partnering with Princeton University, the team at Google Brain aims to grow the community around machine learning for mechanical ventilation control. They believe that neural networks and deep learning can better generalize across lungs with varying characteristics than the current industry standard of PID controllers.
In this competition, you’ll simulate a ventilator connected to a sedated patient's lung. The best submissions will take lung attributes compliance and resistance into account.
If successful, you'll help overcome the cost barrier of developing new methods for controlling mechanical ventilators. This will pave the way for algorithms that adapt to patients and reduce the burden on clinicians during these novel times and beyond. As a result, ventilator treatments may become more widely available to help patients breathe.
Photo by Nino Liverani on Unsplash",3 files,698.79 MB,This competition awarded ranking points,This competition counted towards tiers,Research Prediction Competition
Wikipedia - Image/Caption Matching,,a year ago,,"['image', 'text', 'ndcg@{k}']",105,121,548,"A picture is worth a thousand words, yet sometimes a few will do. We all rely on online images for knowledge sharing, learning, and understanding. Even the largest websites are missing visual content and metadata to pair with their images. Captions and “alt text” increase accessibility and enable better search. The majority of images on Wikipedia articles, for example, don't have any written context connected to the image. Open models could help anyone improve accessibility and learning for all.
Current solutions rely on simple methods based on translations or page interlinks, which have limited coverage. Even the most advanced computer vision image captioning isn't suitable for images with complex semantics.
In this competition, you’ll build a model that automatically retrieves the text closest to an image. Specifically, you'll train your model to associate given images with article titles or complex captions, in multiple languages. The best models will account for the semantic granularity of Wikipedia images.
If successful, you'll be contributing to the accessibility of the largest online encyclopedia. The millions of Wikipedia readers and editors will be able to more easily understand, search, and describe media at scale. As a result, you’ll contribute to an open model to improve learning for all.
--
This competition is organized by the Research team at the Wikimedia Foundation. This competition is based on the WIT dataset published by Google Research as detailed in this SIGIR paper.",23 files,74.45 GB,This competition did not award ranking points,This competition did not count towards tiers,Playground Prediction Competition
Tabular Playground Series - Sep 2021,,a year ago,a year ago,"['tabular', 'binary classification', 'auc']","1,942","2,060","18,394","Kaggle competitions are incredibly fun and rewarding, but they can also be intimidating for people who are relatively new in their data science journey. In the past, we've launched many Playground competitions that are more approachable than our Featured competitions and thus, more beginner-friendly.
In order to have a more consistent offering of these competitions for our community, we're trying a new experiment in 2021. We'll be launching month-long tabular Playground competitions on the 1st of every month and continue the experiment as long as there's sufficient interest and participation.
The goal of these competitions is to provide a fun, and approachable for anyone, tabular dataset. These competitions will be great for people looking for something in between the Titanic Getting Started competition and a Featured competition. If you're an established competitions master or grandmaster, these probably won't be much of a challenge for you. We encourage you to avoid saturating the leaderboard.
For each monthly competition, we'll be offering Kaggle Merchandise for the top three teams. And finally, because we want these competitions to be more about learning, we're limiting team sizes to 3 individuals.
The dataset is used for this competition is synthetic, but based on a real dataset and generated using a CTGAN. The original dataset deals with predicting whether a claim will be made on an insurance policy. Although the features are anonymized, they have properties relating to real-world features.
Good luck and have fun!
For ideas on how to improve your score, check out the Intro to Machine Learning and Intermediate Machine Learning courses on Kaggle Learn.",3 files,1.37 GB,This competition did not award ranking points,This competition did not count towards tiers,Playground Prediction Competition
Lux AI,"$10,000 ",a year ago,,"['video games', 'simulations', 'custom metric']","1,178","1,449","22,331","Introduction
The night is dark and full of terrors. Two teams must fight off the darkness, collect resources, and advance through the ages. Daytime finds a desperate rush to gather the resources that can carry you through the impending night whilst growing your city. Plan and expand carefully -- any city that fails to produce enough light will be consumed by darkness.
Welcome to the Lux AI Challenge Season 1!
The Lux AI Challenge is a competition where competitors design agents to tackle a multi-variable optimization, resource gathering, and allocation problem in a 1v1 scenario against other competitors. In addition to optimization, successful agents must be capable of analyzing their opponents and developing appropriate policies to get the upper hand.
All code can be found at our Github, make sure to give it a star while you are there!
Make sure to join our community discord to chat, strategize, and learn with other competitors! We will be posting announcements on the Kaggle Forums and on the discord.
Sponsors
We would like to thank our 3 sponsors, QuantCo, J Ventures, and QAImera this year for allowing us to provide a prize pool and exciting opportunities to our competitors! For more information on them, go to the sponsors tab",,,This competition awarded ranking points,This competition counted towards tiers,Featured Simulation Competition
chaii - Hindi and Tamil Question Answering,"$10,000 ",a year ago,,"['text', 'languages', 'custom metric']",943,"1,148","18,727","With nearly 1.4 billion people, India is the second-most populated country in the world. Yet Indian languages, like Hindi and Tamil, are underrepresented on the web. Popular Natural Language Understanding (NLU) models perform worse with Indian languages compared to English, the effects of which lead to subpar experiences in downstream web applications for Indian users. With more attention from the Kaggle community and your novel machine learning solutions, we can help Indian users make the most of the web.
Predicting answers to questions is a common NLU task, but not for Hindi and Tamil. Current progress on multilingual modeling requires a concentrated effort to generate high-quality datasets and modelling improvements. Additionally, for languages that are typically underrepresented in public datasets, it can be difficult to build trustworthy evaluations. We hope the dataset provided for this competition—and additional datasets generated by participants—will enable future machine learning for Indian languages.
In this competition, your goal is to predict answers to real questions about Wikipedia articles. You will use chaii-1, a new question answering dataset with question-answer pairs. The dataset covers Hindi and Tamil, collected without the use of translation. It provides a realistic information-seeking task with questions written by native-speaking expert data annotators. You will be provided with a baseline model and inference code to build upon.
If successful, you'll improve upon the baseline performance of NLU models in Indian languages. The results could improve the web experience for many of the nearly 1.4 billion people of India. Additionally, you’ll contribute to multilingual NLP, which could be applied beyond the languages in this competition.
Acknowledgments
Google Research India contributes fundamental advances in computer science and applies their research to big problems impacting India, Google, and communities around the world. The Natural Language Understanding group at Google Research India works specifically with ML to address the unique challenges in the Indian context (such as code mixing in Search, diversity of languages, dialects and accents in Assistant), learning from limited resources and advancing multilingual models.
chaii (Challenge in AI for India) is a Google Research India initiative created with the purpose of sparking AI applications to address some of the pressing problems in India and to find unique ways to address them. Starting with a focus on NLU, chaii hopes to make progress towards multilingual modelling, as language diversity is significantly underserved on the web. Google Research India is working on transformational approaches to healthcare, agriculture and education, and also improving apps and services such as search, assistant and payments, e.g., to deal with challenges arising out of the diversity of languages in India. We also acknowledge the support from the AI4Bharat Team at the Indian Institute of Technology Madras.",3 files,31.8 MB,This competition awarded ranking points,This competition counted towards tiers,Research Code Competition
Google Landmark Retrieval 2021,,a year ago,,"['image', 'computer vision', 'map@{k}']",263,392,"6,782","Welcome to the fourth Landmark Retrieval competition! This year, we introduce a lot more diversity in the challenge’s test images in order to measure global landmark retrieval performance in a fairer manner. And following last year’s success, we set this up as a code competition.
Image retrieval is a central problem in computer vision, relevant to many applications. The problem is usually posed as follows: given a query image, can you find similar images in a large database? This is especially important for query images containing landmarks, which accounts for a large portion of what people like to photograph.
In this competition, the developed models are expected to retrieve relevant database images to a given query image (i.e., the model should retrieve database images containing the same landmark as the query). This challenge is organized in conjunction with the Landmark Recognition Challenge 2021. Both challenges will be discussed at the Instance-Level Recognition workshop in ICCV 21.
In contrast to previous editions of this challenge (2018, 2019, and 2020), this year's competition is structured in a synchronous rerun format, where participants need to submit their Kaggle notebooks for scoring.
This is a Code Competition. Refer to Code Requirements for details.",1657776 files,109.51 GB,This competition awarded ranking points,This competition counted towards tiers,Research Code Competition
Google Landmark Recognition 2021,,a year ago,,"['image', 'computer vision', 'custom metric']",383,525,"8,717","Welcome to the fourth Landmark Recognition competition! This year, we introduce a lot more diversity in the challenge’s test images in order to measure global landmark recognition performance in a fairer manner. And following last year’s success, we set this up as a code competition.
Have you ever gone through your vacation photos and asked yourself: What is the name of this temple I visited in China? Who created this monument I saw in France? Landmark recognition can help! This technology can predict landmark labels directly from image pixels, to help people better understand and organize their photo collections. This competition challenges Kagglers to build models that recognize the correct landmark (if any) in a dataset of challenging test images.
Many Kagglers are familiar with image classification challenges like the ImageNet Large Scale Visual Recognition Challenge (ILSVRC), which aims to recognize 1K general object categories. Landmark recognition is a little different from that: it contains a much larger number of classes (there are more than 81K classes in this challenge), and the number of training examples per class may not be very large. Landmark recognition is challenging in its own way.
This year's competition is structured in a synchronous rerun format, where participants need to submit their Kaggle notebooks for scoring. This is similar to the 2020 version of the competition. In older editions (2018 and 2019), submissions had been handled by uploading prediction files to the system.
This challenge is organized in conjunction with the Landmark Retrieval Challenge 2021. Both challenges will be discussed at the Instance-Level Recognition workshop in ICCV 21.
This is a Code Competition. Refer to Code Requirements for details.
Cover image credits: Muhammad Mahdi Karim. The original is available on Wikimedia here. License: GNU Free Documentation License.",1590817 files,105.52 GB,This competition awarded ranking points,This competition counted towards tiers,Research Code Competition
NFL Health & Safety - Helmet Assignment,"$100,000 ",a year ago,,"['health', 'football', 'custom metric']",825,"1,028","12,600","The National Football League (NFL) and Amazon Web Services (AWS) are teaming up to develop the best sports injury surveillance and mitigation program. In previous competitions, Kaggle has helped detect helmet impacts. As a next step, the NFL wants to assign specific players to each helmet, which would help accurately identify each player's “exposures” throughout a football play.
Currently, the NFL manually annotates a subset of plays each year to determine a sample of exposures for each player. To expand this program, the current player assignment requires a field map to determine player locations. The NFL is interested in matching this model's accuracy without the need for the mapping step. The league is calling on Kagglers to invent a better way to identify individual players.
The National Football League is America's most popular sports league. Founded in 1920, the NFL developed the model for the successful modern sports league and is committed to advancing progress in the diagnosis, prevention, and treatment of sports-related injuries. Health and safety efforts include support for independent medical research and engineering advancements as well as a commitment to work to better protect players and make the game safer, including enhancements to medical protocols and improvements to how our game is taught and played. For more information about the NFL's health and safety efforts, please visit www.NFL.com/PlayerHealthandSafety.
In this competition, you’ll identify and assign football players’ helmets from video footage. In particular, you'll create algorithms capable of assigning detected helmet impacts to correct players via tracking information. Successful submissions should aim for 90% accuracy.
If successful, you'll support the NFL in its efforts to efficiently improve player safety. If the league no longer has to manually label each exposure, it would dramatically increase the speed and scale at which they could answer complex research questions related to helmet impact. Automatic player detection would also allow the NFL to back-calculate historic exposure trends, allowing for deeper insights into how to mitigate them in the future.

This is a Code Competition. Refer to Code Requirements for details.",10079 files,3.43 GB,This competition awarded ranking points,This competition counted towards tiers,Featured Code Competition
LearnPlatform COVID-19 Impact on Digital Learning,"$20,000 ",a year ago,a year ago,"['education', 'data analytics', 'covid19']",,,,"Nelson Mandela believed education was the most powerful weapon to change the world. But not every student has equal opportunities to learn. Effective policies and plans need to be enacted in order to make education more equitable—and perhaps your innovative data analysis will help reveal the solution.
Current research shows educational outcomes are far from equitable. The imbalance was exacerbated by the COVID-19 pandemic. There's an urgent need to better understand and measure the scope and impact of the pandemic on these inequities.
Education technology company LearnPlatform was founded in 2014 with a mission to expand equitable access to education technology for all students and teachers. LearnPlatform’s comprehensive edtech effectiveness system is used by districts and states to continuously improve the safety, equity, and effectiveness of their educational technology. LearnPlatform does so by generating an evidence basis for what’s working and enacting it to benefit students, teachers, and budgets.
In this analytics competition, you’ll work to uncover trends in digital learning. Accomplish this with data analysis about how engagement with digital learning relates to factors like district demographics, broadband access, and state/national level policies and events. Then, submit a Kaggle Notebook to propose your best solution to these educational inequities.
Your submissions will inform policies and practices that close the digital divide. With a better understanding of digital learning trends, you may help reverse the long-term learning loss among America’s most vulnerable, making education more equitable.
Problem Statement
The COVID-19 Pandemic has disrupted learning for more than 56 million students in the United States. In the Spring of 2020, most states and local governments across the U.S. closed educational institutions to stop the spread of the virus. In response, schools and teachers have attempted to reach students remotely through distance learning tools and digital platforms. Until today, concerns of the exacaberting digital divide and long-term learning loss among America’s most vulnerable learners continue to grow.
Challenge
We challenge the Kaggle community to explore (1) the state of digital learning in 2020 and (2) how the engagement of digital learning relates to factors such as district demographics, broadband access, and state/national level policies and events.
We encourage you to guide the analysis with questions that are related to the themes that are described above (in bold font). Below are some examples of questions that relate to our problem statement:
What is the picture of digital connectivity and engagement in 2020?
What is the effect of the COVID-19 pandemic on online and distance learning, and how might this also evolve in the future?
How does student engagement with different types of education technology change over the course of the pandemic?
How does student engagement with online learning platforms relate to different geography? Demographic context (e.g., race/ethnicity, ESL, learning disability)? Learning context? Socioeconomic status?
Do certain state interventions, practices or policies (e.g., stimulus, reopening, eviction moratorium) correlate with the increase or decrease online engagement?",236 files,609.29 MB,,,Analytics Competition
Tabular Playground Series - Aug 2021,,a year ago,a year ago,"['tabular', 'regression', 'banking', 'rmse']","1,753","1,841","16,696","Kaggle competitions are incredibly fun and rewarding, but they can also be intimidating for people who are relatively new in their data science journey. In the past, we've launched many Playground competitions that are more approachable than our Featured competitions and thus, more beginner-friendly.
In order to have a more consistent offering of these competitions for our community, we're trying a new experiment in 2021. We'll be launching month-long tabular Playground competitions on the 1st of every month and continue the experiment as long as there's sufficient interest and participation.
The goal of these competitions is to provide a fun, and approachable for anyone, tabular dataset. These competitions will be great for people looking for something in between the Titanic Getting Started competition and a Featured competition. If you're an established competitions master or grandmaster, these probably won't be much of a challenge for you. We encourage you to avoid saturating the leaderboard.
For each monthly competition, we'll be offering Kaggle Merchandise for the top three teams. And finally, because we want these competitions to be more about learning, we're limiting team sizes to 3 individuals.
The dataset is used for this competition is synthetic, but based on a real dataset and generated using a CTGAN. The original dataset deals with calculating the loss associated with a loan defaults. Although the features are anonymized, they have properties relating to real-world features.
Good luck and have fun!
For ideas on how to improve your score, check out the Intro to Machine Learning and Intermediate Machine Learning courses on Kaggle Learn.",3 files,337.38 MB,This competition did not award ranking points,This competition did not count towards tiers,Playground Prediction Competition
RSNA-MICCAI Brain Tumor Radiogenomic Classification,"$30,000 ",a year ago,,"['image', 'binary classification', 'healthcare', 'auc']","1,555","1,958","27,466","A malignant tumor in the brain is a life-threatening condition. Known as glioblastoma, it's both the most common form of brain cancer in adults and the one with the worst prognosis, with median survival being less than a year. The presence of a specific genetic sequence in the tumor known as MGMT promoter methylation has been shown to be a favorable prognostic factor and a strong predictor of responsiveness to chemotherapy.
Currently, genetic analysis of cancer requires surgery to extract a tissue sample. Then it can take several weeks to determine the genetic characterization of the tumor. Depending upon the results and type of initial therapy chosen, a subsequent surgery may be necessary. If an accurate method to predict the genetics of the cancer through imaging (i.e., radiogenomics) alone could be developed, this would potentially minimize the number of surgeries and refine the type of therapy required.
The Radiological Society of North America (RSNA) has teamed up with the Medical Image Computing and Computer Assisted Intervention Society (the MICCAI Society) to improve diagnosis and treatment planning for patients with glioblastoma. In this competition you will predict the genetic subtype of glioblastoma using MRI (magnetic resonance imaging) scans to train and test your model to detect for the presence of MGMT promoter methylation.
If successful, you'll help brain cancer patients receive less invasive diagnoses and treatments. The introduction of new and customized treatment strategies before surgery has the potential to improve the management, survival, and prospects of patients with brain cancer.
Acknowledgments
The Radiological Society of North America (RSNA®) is a non-profit organization that represents 31 radiologic subspecialties from 145 countries around the world. RSNA promotes excellence in patient care and health care delivery through education, research and technological innovation.
RSNA provides high-quality educational resources, publishes five top peer-reviewed journals, hosts the world’s largest radiology conference and is dedicated to building the future of the profession through the RSNA Research & Education (R&E) Foundation, which has funded $66 million in grants since its inception. RSNA also supports and facilitates artificial intelligence (AI) research in medical imaging by sponsoring an ongoing series of AI challenge competitions.
The Medical Image Computing and Computer Assisted Intervention Society (the MICCAI Society) is dedicated to the promotion, preservation and facilitation of research, education and practice in the field of medical image computing and computer assisted medical interventions including biomedical imaging and medical robotics. The Society achieves this aim through the organization and operation of annual high quality international conferences, workshops, tutorials and publications that promote and foster the exchange and dissemination of advanced knowledge, expertise and experience in the field produced by leading institutions and outstanding scientists, physicians and educators around the world.
A full set of acknowledgments can be found on this page.",400116 files,136.85 GB,This competition awarded ranking points,This competition counted towards tiers,Featured Code Competition
Tabular Playground Series - Jul 2021,,a year ago,a year ago,"['tabular', 'time series analysis', 'pollution', 'mcrmsle']","1,293","1,346","13,534","Kaggle competitions are incredibly fun and rewarding, but they can also be intimidating for people who are relatively new in their data science journey. In the past, we've launched many Playground competitions that are more approachable than our Featured competitions and thus, more beginner-friendly.
In order to have a more consistent offering of these competitions for our community, we're trying a new experiment in 2021. We'll be launching month-long tabular Playground competitions on the 1st of every month and continue the experiment as long as there's sufficient interest and participation.
The goal of these competitions is to provide a fun, and approachable for anyone, tabular dataset. These competitions will be great for people looking for something in between the Titanic Getting Started competition and a Featured competition. If you're an established competitions master or grandmaster, these probably won't be much of a challenge for you. We encourage you to avoid saturating the leaderboard.
For each monthly competition, we'll be offering Kaggle Merchandise for the top three teams. And finally, because we want these competitions to be more about learning, we're limiting team sizes to 3 individuals.
The dataset is used for this competition is based on a real dataset, but has synthetic-generated aspects to it. The original dataset deals with predicting air pollution in a city via various input sensor values (e.g., a time series).
Good luck and have fun!
For ideas on how to improve your score, check out the Intro to Machine Learning and Intermediate Machine Learning courses on Kaggle Learn.",3 files,826.96 kB,This competition did not award ranking points,This competition did not count towards tiers,Playground Prediction Competition
G2Net Gravitational Wave Detection,"$15,000 ",a year ago,,"['numpy', 'astronomy', 'signal processing', 'auc']","1,219","1,501","28,222","It's been said that teamwork makes the dream work. This couldn't be truer for the breakthrough discovery of gravitational waves (GW), signals from colliding binary black holes in 2015. It required the collaboration of experts in physics, mathematics, information science, and computing. GW signals have led researchers to observe a new population of massive, stellar-origin black holes, to unlock the mysteries of neutron star mergers, and to measure the expansion of the Universe. These signals are unimaginably tiny ripples in the fabric of space-time and even though the global network of GW detectors are some of the most sensitive instruments on the planet, the signals are buried in detector noise. Analysis of GW data and the detection of these signals is a crucial mission for the growing global network of increasingly sensitive GW detectors. These challenges in data analysis and noise characterization could be solved with the help of data science.
As with the multi-disciplined approach to the discovery of GWs, additional expertise will be needed to further GW research. In particular, social and natural sciences have taken an interest in machine learning, deep learning, classification problems, data mining, and visualization to develop new techniques and algorithms to efficiently handle complex and massive data sets. The increase in computing power and the development of innovative techniques for the rapid analysis of data will be vital to the exciting new field of GW Astronomy. Potential outcomes may include increased sensitivity to GW signals, application to control and feedback systems for next-generation detectors, noise removal, data conditioning tools, and signal characterization.
G2Net is a network of Gravitational Wave, Geophysics and Machine Learning. Via an Action from COST (European Cooperation in Science and Technology), a funding agency for research and innovation networks, G2Net aims to create a broad network of scientists. From four different areas of expertise, namely GW physics, Geophysics, Computing Science and Robotics, these scientists have agreed on a common goal of tackling challenges in data analysis and noise characterization for GW detectors.
In this competition, you’ll aim to detect GW signals from the mergers of binary black holes. Specifically, you'll build a model to analyze simulated GW time-series data from a network of Earth-based detectors.

The series of images above were taken from the 2015 paper announcing the discovery of gravitational waves from a pair of merging black holes.
If successful, you'll play a part in solving a crucial mission in the exciting new field of GW science. With the development of new algorithms, scientists will have a better handle on the potential power of the data science community and their innovative approaches to data analysis. Moreover, it will enable closer interaction between computer science and physics, which could benefit both disciplines. Your participation can further this collaboration and the help advance this breakthrough discovery.
Acknowledgments
We acknowledge support from the LIGO-Virgo-Kagra Collaboration of which the hosts are members. Specifically we acknowledge the use of the software resource lalsuite.",786002 files,77.38 GB,This competition awarded ranking points,This competition counted towards tiers,Research Prediction Competition
Optiver Realized Volatility Prediction,"$100,000 ",a year ago,,"['tabular', 'finance', 'rootmeansquarepercentageerror']","3,852",,,"Volatility is one of the most prominent terms you’ll hear on any trading floor – and for good reason. In financial markets, volatility captures the amount of fluctuation in prices. High volatility is associated to periods of market turbulence and to large price swings, while low volatility describes more calm and quiet markets. For trading firms like Optiver, accurately predicting volatility is essential for the trading of options, whose price is directly related to the volatility of the underlying product.
As a leading global electronic market maker, Optiver is dedicated to continuously improving financial markets, creating better access and prices for options, ETFs, cash equities, bonds and foreign currencies on numerous exchanges around the world. Optiver’s teams have spent countless hours building sophisticated models that predict volatility and continuously generate fairer options prices for end investors. However, an industry-leading pricing algorithm can never stop evolving, and there is no better place than Kaggle to help Optiver take its model to the next level.
In the first three months of this competition, you’ll build models that predict short-term volatility for hundreds of stocks across different sectors. You will have hundreds of millions of rows of highly granular financial data at your fingertips, with which you'll design your model forecasting volatility over 10-minute periods. Your models will be evaluated against real market data collected in the three-month evaluation period after training.
Through this competition, you'll gain invaluable insight into volatility and financial market structure. You'll also get a better understanding of the sort of data science problems Optiver has faced for decades. We look forward to seeing the creative approaches the Kaggle community will apply to this ever complex but exciting trading challenge.
Getting started
In order to make Kagglers better prepared for this competition, Optiver's data scientists have created a tutorial notebook debriefing competition data and relevant financial concepts of this trading challenge. Also, Optiver's online course can tell you more about financial market and market making.
For more information about exciting data science opportunities at Optiver, check out their data science landing page here or e-mail their recruiting team directly at datascience@optiver.com.

This is a Code Competition. Refer to Code Requirements for details.",229 files,2.73 GB,This competition awarded ranking points,This competition counted towards tiers,Featured Code Competition
MLB Player Digital Engagement Forecasting,"$50,000 ",a year ago,,['meancolumnwisemae'],852,660,,"As of August 1st, we are in the evaluation phase of the competition, where the leaderboard will be refreshed periodically based on performance of participant submissions on the future-looking test set. No new submissions will be accepted at this time.
A player hits a walk-off home run. A pitcher throws a no-hitter. A team gets red hot going into the Postseason. We know some of the catalysts that increase baseball fan interest. Now Major League Baseball (MLB) and Google Cloud want the Kaggle community’s help to identify the many other factors which pique supporter engagement and create deeper relationships betweens players and fans.
The sport has a long history of being numbers-driven. Nearly every day from at least April through October, baseball fans watch, read, and search for information about players. Which individuals they seek can depend on player performance, team standings, popularity, among other, currently unknown factors—which could be better understood thanks to data science.
Since at least the early 1990s, MLB has led the sports world in the use of data, showing fans, players, coaches, and media what’s possible when you combine data with human performance. MLB continues its leadership using technology to engage fans and provide new fans innovative ways to experience America’s Favorite Pastime.
MLB has teamed up with Google Cloud to transform the fan experience through data. Google Cloud proudly supports this Kaggle contest to celebrate the launch of Vertex AI: Google Cloud’s new platform to unify your ML workflows.
In this competition, you’ll predict how fans engage with MLB players’ digital content on a daily basis for a future date range. You’ll have access to player performance data, social media data, and team factors like market size. Successful models will provide new insights into what signals most strongly correlate with and influence engagement.
Imagine if you could predict MLB All Stars all season long or when each of a team’s 25 players has his moment in the spotlight. These insights are possible when you dive deeper into the fandom of America’s pastime. Be part of the first method of its kind to try to understand digital engagement at the player level in this granular, day-to-day fashion. Simultaneously help MLB build innovation more easily using Google Cloud’s data analytics, Vertex AI and MLOps tools. You could play a part in shaping the future of MLB fan and player engagement.

This is a Code Competition. Refer to Code Requirements for details.",10 files,8.35 GB,This competition awarded ranking points,This competition counted towards tiers,Featured Code Competition
Tabular Playground Series - Jun 2021,,2 years ago,a year ago,"['beginner', 'classification', 'tabular', 'multiclass classification', 'multiclassloss']","1,171","1,209","11,888","Kaggle competitions are incredibly fun and rewarding, but they can also be intimidating for people who are relatively new in their data science journey. In the past, we've launched many Playground competitions that are more approachable than our Featured competitions and thus, more beginner-friendly.
In order to have a more consistent offering of these competitions for our community, we're trying a new experiment in 2021. We'll be launching month-long tabular Playground competitions on the 1st of every month and continue the experiment as long as there's sufficient interest and participation.
The goal of these competitions is to provide a fun, and approachable for anyone, tabular dataset. These competitions will be great for people looking for something in between the Titanic Getting Started competition and a Featured competition. If you're an established competitions master or grandmaster, these probably won't be much of a challenge for you. We encourage you to avoid saturating the leaderboard.
For each monthly competition, we'll be offering Kaggle Merchandise for the top three teams. And finally, because we want these competitions to be more about learning, we're limiting team sizes to 3 individuals.
The dataset is used for this competition is synthetic, but based on a real dataset and generated using a CTGAN. The original dataset deals with predicting the category on an eCommerce product given various attributes about the listing. Although the features are anonymized, they have properties relating to real-world features.
Good luck and have fun!
For ideas on how to improve your score, check out the Intro to Machine Learning and Intermediate Machine Learning courses on Kaggle Learn.",3 files,56.28 MB,This competition did not award ranking points,This competition did not count towards tiers,Playground Prediction Competition
SIIM-FISABIO-RSNA COVID-19 Detection,"$100,000 ",2 years ago,,"['image', 'multilabel classification', 'custom metric']","1,305","1,786","32,307","Five times more deadly than the flu, COVID-19 causes significant morbidity and mortality. Like other pneumonias, pulmonary infection with COVID-19 results in inflammation and fluid in the lungs. COVID-19 looks very similar to other viral and bacterial pneumonias on chest radiographs, which makes it difficult to diagnose. Your computer vision model to detect and localize COVID-19 would help doctors provide a quick and confident diagnosis. As a result, patients could get the right treatment before the most severe effects of the virus take hold.
Currently, COVID-19 can be diagnosed via polymerase chain reaction to detect genetic material from the virus or chest radiograph. However, it can take a few hours and sometimes days before the molecular test results are back. By contrast, chest radiographs can be obtained in minutes. While guidelines exist to help radiologists differentiate COVID-19 from other types of infection, their assessments vary. In addition, non-radiologists could be supported with better localization of the disease, such as with a visual bounding box.
As the leading healthcare organization in their field, the Society for Imaging Informatics in Medicine (SIIM)'s mission is to advance medical imaging informatics through education, research, and innovation. SIIM has partnered with the Foundation for the Promotion of Health and Biomedical Research of Valencia Region (FISABIO), Medical Imaging Databank of the Valencia Region (BIMCV) and the Radiological Society of North America (RSNA) for this competition.
In this competition, you’ll identify and localize COVID-19 abnormalities on chest radiographs. In particular, you'll categorize the radiographs as negative for pneumonia or typical, indeterminate, or atypical for COVID-19. You and your model will work with imaging data and annotations from a group of radiologists.
If successful, you'll help radiologists diagnose the millions of COVID-19 patients more confidently and quickly. This will also enable doctors to see the extent of the disease and help them make decisions regarding treatment. Depending upon severity, affected patients may need hospitalization, admission into an intensive care unit, or supportive therapies like mechanical ventilation. As a result of better diagnosis, more patients will quickly receive the best care for their condition, which could mitigate the most severe effects of the virus.

This is a Code Competition. Refer to Code Requirements for details.

Host Organizations
FISABIO, The Foundation for the Promotion of Health and Biomedical Research of Valencia Region
The Foundation for the Promotion of Health and Biomedical Research of Valencia Region, FISABIO, is a non-profit scientific and healthcare entity, whose primary purpose is to encourage, to promote and to develop scientific and technical health and biomedical research in Valencia Region. FISABIO integrates and manages the Health Research Map of the Centre for Public Health Research, Dr. Peset University Hospital Foundation, Alicante University General Hospital Foundation, Elche University General Hospital Foundation, and the Mediterranean Ophthalmological Foundation. The BIMCV facility is connected with a multi-level vendor neutral archive (VNA). The imaging population facility is storing data from the Valencia Region, which accounts for more than 5.1 million habitants.
Radiological Society of North America (RSNA)
The Radiological Society of North America (RSNA) is a non-profit organization that represents 31 radiologic subspecialties from 145 countries around the world. RSNA promotes excellence in patient care and health care delivery through education, research and technological innovation.
RSNA provides high-quality educational resources, publishes five top peer-reviewed journals, hosts the world’s largest radiology conference and is dedicated to building the future of the profession through the RSNA Research & Education (R&E) Foundation, which has funded $66 million in grants since its inception. RSNA also supports and facilitates artificial intelligence (AI) research in medical imaging by sponsoring an ongoing series of AI challenge competitions.",7600 files,128.51 GB,This competition awarded ranking points,This competition counted towards tiers,Featured Code Competition
Google Smartphone Decimeter Challenge,"$10,000 ",2 years ago,,"['tabular', 'geospatial analysis', 'research', 'mobile and wireless', 'signal processing', 'custom metric']",810,985,"16,802","Have you ever hit a surprise pothole or other road obstruction? Do you wish your navigation app could provide more precise location or lane-level accuracy? These and other novel features are powered by smartphone positioning services. Machine learning and precision GNSS algorithms are expected to improve this accuracy and provide billions of Android phone users with a more fine-tuned positioning experience.
Global Navigation Satellite System (GNSS) provides raw signals, which the GPS chipset uses to compute a position. Current mobile phones only offer 3-5 meters of positioning accuracy. While useful in many cases, it can create a “jumpy” experience. For many use cases the results are not fine nor stable enough to be reliable.
This competition, hosted by the Android GPS team, is being presented at the ION GNSS+ 2021 Conference. They seek to advance research in smartphone GNSS positioning accuracy and help people better navigate the world around them.
In this competition, you'll use data collected from the host team’s own Android phones to compute location down to decimeter or even centimeter resolution, if possible. You'll have access to precise ground truth, raw GPS measurements, and assistance data from nearby GPS stations, in order to train and test your submissions.
If successful, you'll help produce more accurate positions, bridging the connection between the geospatial information of finer human behavior and mobile internet with much finer granularity. Mobile users could gain better lane-level coordinates, enhanced experience in location-based gaming, and greater specificity in the location of road safety issues. You may even notice it's easier to get you where you need to go.
Acknowledgments
The Android GPS team would like to show its appreciation to Verizon Hyper Precise Location Service and Swift Navigation Skylark Correction Service who provided assistance data for datasets in the challenge.",515 files,12.68 GB,This competition awarded ranking points,This competition counted towards tiers,Research Prediction Competition
SETI Breakthrough Listen - E.T. Signal Search,"$15,000 ",2 years ago,,"['science and technology', 'astronomy', 'signal processing', 'auc']",768,979,"17,367","“Are we alone in the Universe?”
It’s one of the most profound—and perennial—human questions. As technology improves, we’re finding new and more powerful ways to seek answers. The Breakthrough Listen team at the University of California, Berkeley, employs the world’s most powerful telescopes to scan millions of stars for signs of technology. Now it wants the Kaggle community to help interpret the signals they pick up.
The Listen team is part of the Search for ExtraTerrestrial Intelligence (SETI) and uses the largest steerable dish on the planet, the 100-meter diameter Green Bank Telescope. Like any SETI search, the motivation to communicate is also the major challenge. Humans have built enormous numbers of radio devices. It’s hard to search for a faint needle of alien transmission in the huge haystack of detections from modern technology.
Current methods use two filters to search through the haystack. First, the Listen team intersperses scans of the target stars with scans of other regions of sky. Any signal that appears in both sets of scans probably isn’t coming from the direction of the target star. Second, the pipeline discards signals that don’t change their frequency, because this means that they are probably nearby the telescope. A source in motion should have a signal that suggests movement, similar to the change in pitch of a passing fire truck siren. These two filters are quite effective, but we know they can be improved. The pipeline undoubtedly misses interesting signals, particularly those with complex time or frequency structure, and those in regions of the spectrum with lots of interference.
In this competition, use your data science skills to help identify anomalous signals in scans of Breakthrough Listen targets. Because there are no confirmed examples of alien signals to use to train machine learning algorithms, the team included some simulated signals (that they call “needles”) in the haystack of data from the telescope. They have identified some of the hidden needles so that you can train your model to find more. The data consist of two-dimensional arrays, so there may be approaches from computer vision that are promising, as well as digital signal processing, anomaly detection, and more. The algorithm that’s successful at identifying the most needles will win a cash prize, but also has the potential to help answer one of the biggest questions in science.
Acknowledgments
The Breakthrough Listen science and engineering effort is headquartered at the University of California, Berkeley SETI Research Center. The Breakthrough Prize Foundation funds the Breakthrough Initiatives which manages Breakthrough Listen. The Green Bank Observatory is supported by the National Science Foundation, and is operated by Associated Universities, Inc. under a cooperative agreement.",186011 files,156.02 GB,This competition awarded ranking points,This competition counted towards tiers,Research Prediction Competition
CommonLit Readability Prize,"$60,000 ",2 years ago,,"['text', 'regression', 'rmse']","3,633","4,393","72,150","Can machine learning identify the appropriate reading level of a passage of text, and help inspire learning? Reading is an essential skill for academic success. When students have access to engaging passages offering the right level of challenge, they naturally develop reading skills.
Currently, most educational texts are matched to readers using traditional readability methods or commercially available formulas. However, each has its issues. Tools like Flesch-Kincaid Grade Level are based on weak proxies of text decoding (i.e., characters or syllables per word) and syntactic complexity (i.e., number or words per sentence). As a result, they lack construct and theoretical validity. At the same time, commercially available formulas, such as Lexile, can be cost-prohibitive, lack suitable validation studies, and suffer from transparency issues when the formula's features aren't publicly available.
CommonLit, Inc., is a nonprofit education technology organization serving over 20 million teachers and students with free digital reading and writing lessons for grades 3-12. Together with Georgia State University, an R1 public research university in Atlanta, they are challenging Kagglers to improve readability rating methods.
In this competition, you’ll build algorithms to rate the complexity of reading passages for grade 3-12 classroom use. To accomplish this, you'll pair your machine learning skills with a dataset that includes readers from a wide variety of age groups and a large collection of texts taken from various domains. Winning models will be sure to incorporate text cohesion and semantics.
If successful, you'll aid administrators, teachers, and students. Literacy curriculum developers and teachers who choose passages will be able to quickly and accurately evaluate works for their classrooms. Plus, these formulas will become more accessible for all. Perhaps most importantly, students will benefit from feedback on the complexity and readability of their work, making it far easier to improve essential reading skills.
Acknowledgements
CommonLit would like to extend a special thanks to Professor Scott Crossley's research team at the Georgia State University Departments of Applied Linguistics and Learning Sciences for their partnership on this project.
The organizers would like to thank Schmidt Futures for their advice and support for making this work possible.
        
This is a Code Competition. Refer to Code Requirements for details.",3 files,2.93 MB,This competition awarded ranking points,This competition counted towards tiers,Featured Code Competition
Tabular Playground Series - May 2021,,2 years ago,2 years ago,"['beginner', 'classification', 'tabular', 'multiclass classification', 'multiclassloss']","1,097","1,150","11,614","Kaggle competitions are incredibly fun and rewarding, but they can also be intimidating for people who are relatively new in their data science journey. In the past, we've launched many Playground competitions that are more approachable than our Featured competitions and thus, more beginner-friendly.
In order to have a more consistent offering of these competitions for our community, we're trying a new experiment in 2021. We'll be launching month-long tabular Playground competitions on the 1st of every month and continue the experiment as long as there's sufficient interest and participation.
The goal of these competitions is to provide a fun, and approachable for anyone, tabular dataset. These competitions will be great for people looking for something in between the Titanic Getting Started competition and a Featured competition. If you're an established competitions master or grandmaster, these probably won't be much of a challenge for you. We encourage you to avoid saturating the leaderboard.
For each monthly competition, we'll be offering Kaggle Merchandise for the top three teams, to inspire broad participation we are limiting winner's of swag to once per person for this series. And finally, because we want these competitions to be more about learning, we're limiting team sizes to 3 individuals.
The dataset is used for this competition is synthetic, but based on a real dataset and generated using a CTGAN. The original dataset deals with predicting the category on an eCommerce product given various attributes about the listing. Although the features are anonymized, they have properties relating to real-world features.
Good luck and have fun!
For ideas on how to improve your score, check out the Intro to Machine Learning and Intermediate Machine Learning courses on Kaggle Learn.",3 files,18.19 MB,This competition did not award ranking points,This competition did not count towards tiers,Playground Prediction Competition
BirdCLEF 2021 - Birdcall Identification,"$5,000 ",2 years ago,,"['environment', 'audio', 'fscorebetamicro']",816,"1,001","9,307","Birds of a feather flock together. Thankfully, this makes it easier to hear them! There are over 10,000 bird species around the world. Identifying the red-winged blackbirds or Bewick’s wrens in an area, for example, can provide important information about the habitat. As birds are high up in the food chain, they are excellent indicators of deteriorating environmental quality and pollution. Monitoring the status and trends of biodiversity in ecosystems is no small task. With proper sound detection and classification—aided by machine learning—researchers can improve their ability to track the status and trends of biodiversity in important ecosystems, enabling them to better support global conservation efforts.
Recent advances in machine listening have improved acoustic data collection. However, it remains a challenge to generate analysis outputs with high precision and recall. The majority of data is unexamined due to a lack of effective tools for efficient and reliable extraction of the signals of interests (e.g., bird calls).
The Cornell Lab of Ornithology is dedicated to advancing the understanding and protection of birds and the natural world. The Lab joins with people from all walks of life to make new scientific discoveries, share insights, and galvanize conservation action. For this competition, they're collaborating with Google Research, LifeCLEF, and Xeno-canto.
In this competition, you’ll automate the acoustic identification of birds in soundscape recordings. You'll examine an acoustic dataset to build detectors and classifiers to extract the signals of interest (bird calls). Innovative solutions will be able to do so efficiently and reliably.
The ornithology community is collecting many petabytes of acoustic data every year, but the majority of data remains unexamined. If successful, you'll help researchers properly detect and classify bird sounds, significantly improving their ability to monitor the status and trends of biodiversity in important ecosystems. Researchers will better be able to infer factors about an area’s quality of life based on a changing bird population, which allows them to identify how they can best support global conservation efforts.
This is a Code Competition. Refer to Code Requirements for details.
The LifeCLEF Bird Recognition Challenge (BirdCLEF) focuses on developing machine learning algorithms to identify avian vocalizations in continuous soundscape data to aid conservation efforts worldwide. Launched in 2014, it has become one of the largest bird sound recognition competitions in terms of dataset size and species diversity.",62902 files,42.18 GB,This competition awarded ranking points,This competition counted towards tiers,Research Code Competition
Tabular Playground Series - Apr 2021,,2 years ago,2 years ago,"['beginner', 'tabular', 'binary classification', 'categorizationaccuracy']","1,244","1,306","15,345","Kaggle competitions are incredibly fun and rewarding, but they can also be intimidating for people who are relatively new in their data science journey. In the past, we've launched many Playground competitions that are more approachable than our Featured competitions and thus, more beginner-friendly.
In order to have a more consistent offering of these competitions for our community, we're trying a new experiment in 2021. We'll be launching month-long tabular Playground competitions on the 1st of every month and continue the experiment as long as there's sufficient interest and participation.
The goal of these competitions is to provide a fun, and approachable for anyone, tabular dataset. These competitions will be great for people looking for something in between the Titanic Getting Started competition and a Featured competition. If you're an established competitions master or grandmaster, these probably won't be much of a challenge for you. We encourage you to avoid saturating the leaderboard.
For each monthly competition, we'll be offering Kaggle Merchandise for the top three teams. And finally, because we want these competitions to be more about learning, we're limiting team sizes to 3 individuals.
The dataset is used for this competition is synthetic but based on a real dataset (in this case, the actual Titanic data!) and generated using a CTGAN. The statistical properties of this dataset are very similar to the original Titanic dataset, but there's no way to ""cheat"" by using public labels for predictions. How well does your model perform on truly private test labels?
Good luck and have fun!
Getting Started
Check out the original Titanic competition which walks you through how to build various models.
For more ideas on how to improve your score, check out the Intro to Machine Learning and Intermediate Machine Learning courses on Kaggle Learn.",3 files,12.66 MB,This competition did not award ranking points,This competition did not count towards tiers,Playground Prediction Competition
Coleridge Initiative - Show US the Data,"$90,000 ",2 years ago,,"['text', 'research', 'custom metric']","1,610","1,948","25,957","This competition challenges data scientists to show how publicly funded data are used to serve science and society. Evidence through data is critical if government is to address the many threats facing society, including; pandemics, climate change, Alzheimer’s disease, child hunger, increasing food production, maintaining biodiversity, and addressing many other challenges. Yet much of the information about data necessary to inform evidence and science is locked inside publications.
Can natural language processing find the hidden-in-plain-sight data citations? Can machine learning find the link between the words used in research articles and the data referenced in the article?
Now is the time for data scientists to help restore trust in data and evidence. In the United States, federal agencies are now mandated to show how their data are being used. The new Foundations of Evidence-based Policymaking Act requires agencies to modernize their data management. New Presidential Executive Orders are pushing government agencies to make evidence-based decisions based on the best available data and science. And the government is working to respond in an open and transparent way.
This competition will build just such an open and transparent approach. The results will show how public data are being used in science and help the government make wiser, more transparent public investments. It will help move researchers and governments from using ad-hoc methods to automated ways of finding out what datasets are being used to solve problems, what measures are being generated, and which researchers are the experts. Previous competitions have shown that it is possible to develop algorithms to automate the search and discovery of references to data. Now, we want the Kaggle community to develop the best approaches to identify critical datasets used in scientific publications.
In this competition, you'll use natural language processing (NLP) to automate the discovery of how scientific data are referenced in publications. Utilizing the full text of scientific publications from numerous research areas gathered from CHORUS publisher members and other sources, you'll identify data sets that the publications' authors used in their work.
If successful, you'll help support evidence in government data. Automated NLP approaches will enable government agencies and researchers to quickly find the information they need. The approach will be used to develop data usage scorecards to better enable agencies to show how their data are used and bring down a critical barrier to the access and use of public data.
The Coleridge Initiative is a not-for-profit that has been established to use data for social good. One way in which the organization does this is by furthering science through publicly available research.
Resources
Coleridge Data Examples
Rich Search and Discovery for Research Datasets
Democratizing Our Data
NSF""Rich Context"" Video
Acknowledgments
United States Department of Agriculture
United States Department of Commerce
United States Geological Survey
National Oceanic and Atmospheric Administration
National Science Foundation
National Institutes of Health
CHORUS
Westat
Alfred P. Sloan Foundation
Schmidt Futures
Overdeck Family Foundation

This is a Code Competition. Refer to Code Requirements for details.",14322 files,686.15 MB,This competition awarded ranking points,This competition counted towards tiers,Featured Code Competition
Plant Pathology 2021 - FGVC8,,2 years ago,,"['image', 'plants', 'fscoremicro']",626,857,"10,483","Problem Statement
Apples are one of the most important temperate fruit crops in the world. Foliar (leaf) diseases pose a major threat to the overall productivity and quality of apple orchards. The current process for disease diagnosis in apple orchards is based on manual scouting by humans, which is time-consuming and expensive.
Although computer vision-based models have shown promise for plant disease identification, there are some limitations that need to be addressed. Large variations in visual symptoms of a single disease across different apple cultivars, or new varieties that originated under cultivation, are major challenges for computer vision-based disease identification. These variations arise from differences in natural and image capturing environments, for example, leaf color and leaf morphology, the age of infected tissues, non-uniform image background, and different light illumination during imaging etc.
Plant Pathology 2020-FGVC7 challenge competition had a pilot dataset of 3,651 RGB images of foliar disease of apples. For Plant Pathology 2021-FGVC8, we have significantly increased the number of foliar disease images and added additional disease categories. This year’s dataset contains approximately 23,000 high-quality RGB images of apple foliar diseases, including a large expert-annotated disease dataset. This dataset reflects real field scenarios by representing non-homogeneous backgrounds of leaf images taken at different maturity stages and at different times of day under different focal camera settings.
Specific Objectives
The main objective of the competition is to develop machine learning-based models to accurately classify a given leaf image from the test dataset to a particular disease category, and to identify an individual disease from multiple disease symptoms on a single leaf image.
Resources
Details and background information on the dataset and Kaggle competition ‘Plant Pathology 2020 Challenge’ were published as a peer-reviewed research article. If you use the dataset for your project, please cite the following
Thapa, Ranjita; Zhang, Kai; Snavely, Noah; Belongie, Serge; Khan, Awais. The Plant Pathology Challenge 2020 data set to classify foliar disease of apples. Applications in Plant Sciences, 8 (9), 2020.
Acknowledgements
We acknowledge sponsorship from Cornell Initiative for Digital Agriculture (CIDA).

This is a Code Competition. Refer to Code Requirements for details.",18637 files,16.1 GB,This competition did not award ranking points,This competition did not count towards tiers,Research Code Competition
Hotel-ID to Combat Human Trafficking 2021 - FGVC8,,2 years ago,,"['image', 'public safety', 'map@{k}']",92,129,"1,083","Hotel Recognition to Combat Human Trafficking
Victims of human trafficking are often photographed in hotel rooms as in the below examples. Identifying these hotels is vital to these trafficking investigations but poses particular challenges due to low quality of images and uncommon camera angles.
Even without victims in the images, hotel identification in general is a challenging fine-grained visual recognition task with a huge number of classes and potentially high intraclass and low interclass variation. In order to support research into this challenging task and create image search tools for human trafficking investigators, we created the TraffickCam mobile application, which allows every day travelers to submit photos of their hotel room. Read more about TraffickCam on TechCrunch.
Example images from one hotel in the TraffickCam dataset are shown below:
In this contest, competitors are tasked with identifying the hotel seen in test images from the TraffickCam dataset, which are based on a large gallery of training images with known hotel IDs.
Our team currently supports an image search system used at the National Center for Missing and Exploited Children in human trafficking investigations. Novel and interesting approaches have the potential to be incorporated in this search system.

This is a Code Competition. Refer to Code Requirements for details.",97559 files,26.65 GB,This competition did not award ranking points,This competition did not count towards tiers,Research Code Competition
iWildcam 2021 - FGVC8,,2 years ago,,"['image', 'animals', 'mcrmse']",42,65,579,"Description
Camera traps enable the automatic collection of large quantities of image data. Ecologists all over the world use camera traps to monitor biodiversity and population density of animal species. In order to estimate the abundance and density of species in camera trap data, ecologists need to know not just which species were seen, but also how many of each species were seen. However, because images are taken in motion-triggered bursts to increase the likelihood of capturing the animal(s) of interest, object detection alone is not sufficient as it could lead to over or undercounting. For example, if you get 3 images taken at one frame per second and in the first image you see 3 gazelles, in the second you see 5 gazelles, and in the last you see 4 gazelles, how many total gazelles have you seen? This is more challenging than strictly detecting and categorizing species as it requires reasoning and tracking of individuals across sparse temporal samples.
We have prepared a challenge where the training data and test data are from different cameras spread across the globe. The set of species seen in each camera overlap but are not identical. The challenge is to categorize species and count the number of individuals across image bursts. To explore multimodal solutions, we allow competitors to train on the following data: (i) our camera trap training set (data provided by WCS), (ii) iNaturalist 2017-2019 data, and (iii) multispectral imagery (from Landsat 8) for each of the camera trap locations. On the competition GitHub page we provide the multispectral data, a taxonomy file mapping our classes into the iNat taxonomy, a subset of iNat data mapped into our class set, and a camera trap detection model (the MegaDetector) along with the corresponding detections.
This is an FGVCx competition as part of the FGVC8 workshop at CVPR 2021 and is sponsored by Microsoft AI for Earth and Wildlife Insights. There is a GitHub page for the competition here. Please open an issue if you have questions or problems with the dataset.
You can find the iWildCam 2018 Competition here, the iWildCam 2019 Competition here, and the
iWildCam 2020 Competition here.
Acknowledgements
We would like to acknowledge WCS for providing the camera trap data, Centaur Labs for generously providing count annotations on the test data, and Microsoft AI4Earth for hosting our external datasets on Azure.
Kaggle is excited to partner with research groups to push forward the frontier of machine learning. Research competitions make use of Kaggle's platform and experience but are largely organized by the research group's data science team. Any questions or concerns regarding the competition data, quality, or topic will be addressed by them.",419709 files,112.12 GB,This competition did not award ranking points,This competition did not count towards tiers,Research Prediction Competition
Herbarium 2021 - Half-Earth Challenge - FGVC8,,2 years ago,,"['image', 'plants', 'fscoremacro']",80,108,573,"The Herbarium 2021: Half-Earth Challenge is to identify vascular plant specimens provided by the New York Botanical Garden (NY), Bishop Museum (BPBM), Naturalis Biodiversity Center (NL), Queensland Herbarium (BRI), and Auckland War Memorial Museum (AK).
The Herbarium 2021: Half-Earth Challenge dataset includes more than 2.5M images representing nearly 65,000 species from the Americas and Oceania that have been aligned to a standardized plant list (LCVP v1.0.2).
This dataset has a long tail; there are a minimum of 3 images per species. However, some species can be represented by more than 100 images. This dataset only includes vascular land plants which include lycophytes, ferns, gymnosperms, and flowering plants. The extinct forms of lycophytes are the major component of coal deposits, ferns are indicators of ecosystem health, gymnosperms provide major habitats for animals, and flowering plants provide almost all of our crops, vegetables, and fruits.
The teams with the most accurate models will be contacted with the intention of using them on the unnamed plant collections in the NYBG herbarium and then be assessed by the NYBG plant specialists for accuracy.
Background
There are approximately 3,000 herbaria world-wide, and they are massive repositories of plant diversity data. These collections not only represent a vast amount of plant diversity, but since herbarium collections include specimens dating back hundreds of years, they provide snapshots of plant diversity through time. The integrity of the plant is maintained in herbaria as a pressed, dried specimen; a specimen collected nearly two hundred years ago by Darwin looks much the same as one collected a month ago by an NYBG botanist. All specimens not only maintain their morphological features but also include collection dates and locations, their reproductive state, and the name of the person who collected the specimen. This information, multiplied by millions of plant collections, provides the framework for understanding plant diversity on a massive scale and learning how it has changed over time. The models developed during this competition are an integral first step to speed the pace of species discovery and save the plants of the world.
There are approximately 400,000 known vascular plant species with an estimated 80,000 still to be discovered. Herbaria contain an overwhelming amount of unnamed and new specimens, and with the threats of climate change, we need new tools to quicken the pace of species discovery. This is more pressing today as a United Nations report indicates that more than one million species are at risk of extinction, and amid this dire prediction is a recent estimate that suggests plants are disappearing more quickly than animals. This year, we have expanded our curated herbarium dataset to vascular plant diversity in the Americas and Oceania.
The most accurate models will be used on the unidentified plant specimens in our herbarium and assessed by our taxonomists thereby producing a tool to quicken the pace of species discovery.
About
This is an FGVC competition hosted as part of the FGVC8 workshop at CVPR 2021 and sponsored by NYBG.
Details of this competition are mirrored on the github page. Please post in the forum or open an issue if you have any questions or problems with the dataset.
Acknowledgements
The images are provided by the New York Botanical Garden, Bishop Museum, Naturalis Biodiversity Center, Queensland Herbarium, and Auckland War Memorial Museum.",2500782 files,161.9 GB,This competition did not award ranking points,This competition did not count towards tiers,Research Prediction Competition
Shopee - Price Match Guarantee,"$30,000 ",2 years ago,,"['image', 'text', 'retail and shopping', 'fscoremicro']","2,426","3,032","51,077","Do you scan online retailers in search of the best deals? You're joined by the many savvy shoppers who don't like paying extra for the same product depending on where they shop. Retail companies use a variety of methods to assure customers that their products are the cheapest. Among them is product matching, which allows a company to offer products at rates that are competitive to the same product sold by another retailer. To perform these matches automatically requires a thorough machine learning approach, which is where your data science skills could help.
Two different images of similar wares may represent the same product or two completely different items. Retailers want to avoid misrepresentations and other issues that could come from conflating two dissimilar products. Currently, a combination of deep learning and traditional machine learning analyzes image and text information to compare similarity. But major differences in images, titles, and product descriptions prevent these methods from being entirely effective.
Shopee is the leading e-commerce platform in Southeast Asia and Taiwan. Customers appreciate its easy, secure, and fast online shopping experience tailored to their region. The company also provides strong payment and logistical support along with a 'Lowest Price Guaranteed' feature on thousands of Shopee's listed products.
In this competition, you’ll apply your machine learning skills to build a model that predicts which items are the same products.
The applications go far beyond Shopee or other retailers. Your contributions to product matching could support more accurate product categorization and uncover marketplace spam. Customers will benefit from more accurate listings of the same or similar products as they shop. Perhaps most importantly, this will aid you and your fellow shoppers in your hunt for the very best deals.",32418 files,1.92 GB,This competition awarded ranking points,This competition counted towards tiers,Featured Code Competition
Bristol-Myers Squibb – Molecular Translation,"$50,000 ",2 years ago,,"['image', 'chemistry', 'levenshteinmean']",874,"1,171","10,237","In a technology-forward world, sometimes the best and easiest tools are still pen and paper. Organic chemists frequently draw out molecular work with the Skeletal formula, a structural notation used for centuries. Recent publications are also annotated with machine-readable chemical descriptions (InChI), but there are decades of scanned documents that can't be automatically searched for specific chemical depictions. Automated recognition of optical chemical structures, with the help of machine learning, could speed up research and development efforts.
Unfortunately, most public data sets are too small to support modern machine learning models. Existing tools produce 90% accuracy but only under optimal conditions. Historical sources often have some level of image corruption, which reduces performance to near zero. In these cases, time-consuming, manual work is required to reliably convert scanned chemical structure images into a machine-readable format.
Bristol-Myers Squibb is a global biopharmaceutical company working to transform patients' lives through science. Their mission is to discover, develop, and deliver innovative medicines that help patients prevail over serious diseases.
In this competition, you’ll interpret old chemical images. With access to a large set of synthetic image data generated by Bristol-Myers Squibb, you'll convert images back to the underlying chemical structure annotated as InChI text.
Tools to curate chemistry literature would be a significant benefit to researchers. If successful, you'll help chemists expand access to collective chemical research. In turn, this would speed up research and development efforts in many key fields by avoiding repetition of previously published chemistries and identifying novel trends via mining large data sets.
Photo by Terry Vlisidis on Unsplash",4040294 files,8.87 GB,This competition awarded ranking points,This competition counted towards tiers,Featured Prediction Competition
Tabular Playground Series - Mar 2021,,2 years ago,2 years ago,"['tabular', 'logistic regression', 'auc']","1,495","1,568","12,945","Kaggle competitions are incredibly fun and rewarding, but they can also be intimidating for people who are relatively new in their data science journey. In the past, we've launched many Playground competitions that are more approachable than our Featured competitions and thus, more beginner-friendly.
In order to have a more consistent offering of these competitions for our community, we're trying a new experiment in 2021. We'll be launching month-long tabular Playground competitions on the 1st of every month and continue the experiment as long as there's sufficient interest and participation.
The goal of these competitions is to provide a fun, and approachable for anyone, tabular dataset. These competitions will be great for people looking for something in between the Titanic Getting Started competition and a Featured competition. If you're an established competitions master or grandmaster, these probably won't be much of a challenge for you. We encourage you to avoid saturating the leaderboard.
For each monthly competition, we'll be offering Kaggle Merchandise for the top three teams. And finally, because we want these competitions to be more about learning, we're limiting team sizes to 3 individuals.
The dataset is used for this competition is synthetic but based on a real dataset and generated using a CTGAN. The original dataset deals with predicting the amount of an insurance claim. Although the features are anonymized, they have properties relating to real-world features.
Good luck and have fun!
Getting Started
Check out this Starter Notebook which walks you through how to make your very first submission!
For more ideas on how to improve your score, check out the Intro to Machine Learning and Intermediate Machine Learning courses on Kaggle Learn.",3 files,131.65 MB,This competition did not award ranking points,This competition did not count towards tiers,Playground Prediction Competition
March Machine Learning Mania 2021 - NCAAM - Spread,,2 years ago,2 years ago,"['sports', 'basketball', 'rmse']",96,111,163,"In addition to the predictive modeling competitions we typically host (NCAA Women's and Men’s), we are hosting two separate, experimental challenges that ask you to not only predict the the winner, but to predict the margin of victor. The mania of March can come down to final second buzzer beaters, upsets, and even a few blowouts. Can you predict big wins as easily as you can predict close calls?
This competition (and the parallel competition for the women's tournament) allows you to explore how data science and machine learning can continue to examine the depths of college basketball. You're provided data of historical NCAA games and are encouraged to use other sources of publicly available data to gain a winning edge.
In stage one of this two-stage competition, participants will build and test their models against previous tournaments. In the second stage, participants will predict the point spreads of the 2021 tournament. You don’t need to participate in the first stage to enter the second. The first stage exists to incentivize model building and provide a means to score predictions. The real competition is forecasting the 2021 results.
Note: From 2018-2020, Google Cloud, of which Kaggle is a part, served as official corporate sponsors of the NCAA basketball tournaments. This official sponsorship has concluded, but we at Kaggle are happy to bring back this tradition for it's 8th year!
Acknowledgments
Markus Spiske on Unsplash and The Noun Project",40 files,221.48 MB,This competition did not award ranking points,This competition did not count towards tiers,Playground Prediction Competition
March Machine Learning Mania 2021 - NCAAW - Spread,,2 years ago,2 years ago,"['sports', 'basketball', 'rmse']",70,77,116,"In addition to the predictive modeling competitions we typically host (NCAA Women's and Men’s), we are hosting two separate, experimental challenges that ask you to not only predict the the winner, but to predict the margin of victor. The mania of March can come down to final second buzzer beaters, upsets, and even a few blowouts. Can you predict big wins as easily as you can predict close calls?
This competition (and the parallel competition for the men's tournament) allows you to explore how data science and machine learning can continue to examine the depths of college basketball. You're provided data of historical NCAA games and are encouraged to use other sources of publicly available data to gain a winning edge.
In stage one of this two-stage competition, participants will build and test their models against previous tournaments. In the second stage, participants will predict the point spreads of the 2021 tournament. You don’t need to participate in the first stage to enter the second. The first stage exists to incentivize model building and provide a means to score predictions. The real competition is forecasting the 2021 results.
Note: From 2018-2020, Google Cloud, of which Kaggle is a part, served as official corporate sponsors of the NCAA basketball tournaments. This official sponsorship has concluded, but we at Kaggle are happy to bring back this tradition for it's 8th year!
Acknowledgments
Markus Spiske on Unsplash and The Noun Project",28 files,22.98 MB,This competition did not award ranking points,This competition did not count towards tiers,Playground Prediction Competition
March Machine Learning Mania 2021 - NCAAM,,2 years ago,2 years ago,"['sports', 'basketball', 'logloss']",707,791,"1,242","Back again after a pandemic-year hiatus, Kaggle's March Machine Learning Mania challenges data scientists to predict winners and losers of the men's 2021 NCAA basketball tournament. You're provided data of historical NCAA games and are encouraged to use other sources of publicly available data to gain a winning edge.
In stage one of this two-stage competition, participants will build and test their models against previous tournaments. In the second stage, participants will predict the outcome of the 2021 tournament. You don’t need to participate in the first stage to enter the second. The first stage exists to incentivize model building and provide a means to score predictions. The real competition is forecasting the 2021 results.
Note: From 2018-2020, Google Cloud, of which Kaggle is a part, served as official corporate sponsors of the NCAA basketball tournaments. This official sponsorship has concluded, but we at Kaggle are happy to bring back this tradition for its 8th year!
Acknowledgments
Banner image by Ben Hershey on Unsplash",40 files,221.52 MB,This competition awarded ranking points,This competition counted towards tiers,Playground Prediction Competition
Hash Code 2021 - Traffic Signaling,,2 years ago,2 years ago,"['automobiles and vehicles', 'optimization', 'custom metric']",179,189,825,"This is an extended version of the Hash Code Online Qualifications 2021 problem. After Hash Code's official Online Qualifications, which lasts only 4 hours, you can improve your submissions here on Kaggle, optimize them in more detail, and discuss your approaches with other teams. We created a new data set just for this Kaggle competition that was not used in Hash Code before. Please find the full problem statement as a PDF file under the data tab.
Note that this is not Hash Code's official Online Qualifications and that there are no prizes for this competition. You can't qualify for Hash Code's Finals on Kaggle.
Problem statement
The world's first traffic light dates back to 1868. It was installed in London to control traffic for… horse-drawn vehicles! Today, traffic lights can be found at street intersections in almost every city in the world, making it safer for vehicles to go through them.
Traffic lights have at least two states- and use one color (usually red) to signal ""stop""- and another (usually green) to signal that cars can proceed through. The very first traffic lights were manually controlled. Nowadays they are automatic, meaning that they have to be carefully designed and timed in order to optimize the overall travel time for all the participants in traffic.
Given the description of a city plan and planned paths for all cars in that city, you will be optimizing the schedule of traffic lights to minimize the total amount of time spent in traffic, and help as many cars as possible reach their destination before a given deadline.
Photo by Eliobed Suarez on Unsplash",2 files,3.04 MB,This competition did not award ranking points,This competition did not count towards tiers,Playground Code Competition
Tabular Playground Series - Feb 2021,,2 years ago,2 years ago,"['tabular', 'regression', 'rmse']","1,433","1,474","12,553","Kaggle competitions are incredibly fun and rewarding, but they can also be intimidating for people who are relatively new in their data science journey. In the past, we've launched many Playground competitions that are more approachable than our Featured competitions and thus, more beginner-friendly.
In order to have a more consistent offering of these competitions for our community, we're trying a new experiment in 2021. We'll be launching month-long tabular Playground competitions on the 1st of every month and continue the experiment as long as there's sufficient interest and participation.
The goal of these competitions is to provide a fun, and approachable for anyone, tabular dataset. These competitions will be great for people looking for something in between the Titanic Getting Started competition and a Featured competition. If you're an established competitions master or grandmaster, these probably won't be much of a challenge for you. We encourage you to avoid saturating the leaderboard.
For each monthly competition, we'll be offering Kaggle Merchandise for the top three teams. And finally, because we want these competitions to be more about learning, we're limiting team sizes to 3 individuals.
The dataset is used for this competition is synthetic, but based on a real dataset and generated using a CTGAN. The original dataset deals with predicting the amount of an insurance claim. Although the features are anonymized, they have properties relating to real-world features.
Good luck and have fun!
Getting Started
Check out this Starter Notebook which walks you through how to make your very first submission!
For more ideas on how to improve your score, check out the Intro to Machine Learning and Intermediate Machine Learning courses on Kaggle Learn.",3 files,154.66 MB,This competition did not award ranking points,This competition did not count towards tiers,Playground Prediction Competition
Indoor Location & Navigation,"$10,000 ",2 years ago,,"['text mining', 'signal processing', 'custom metric']","1,170","1,446","28,009","Your smartphone goes everywhere with you—whether driving to the grocery store or shopping for holiday gifts. With your permission, apps can use your location to provide contextual information. You might get driving directions, find a store, or receive alerts for nearby promotions. These handy features are enabled by GPS, which requires outdoor exposure for the best accuracy. Yet, there are many times when you’re inside large structures, such as a shopping mall or event center. Accurate indoor positioning, based on public sensors and user permission, allows for a great location-based experience even when you aren’t outside.
Current positioning solutions have poor accuracy, particularly in multi-level buildings, or generalize poorly to small datasets. Additionally, GPS was built for a time before smartphones. Today’s use cases often require more granularity than is typically available indoors.
In this competition, your task is to predict the indoor position of smartphones based on real-time sensor data, provided by indoor positioning technology company XYZ10 in partnership with Microsoft Research. You'll locate devices using “active” localization data, which is made available with the cooperation of the user. Unlike passive localization methods (e.g. radar, camera), the data provided for this competition requires explicit user permission. You'll work with a dataset of nearly 30,000 traces from over 200 buildings.
If successful, you’ll contribute to research with broad-reaching possibilities, including industries like manufacturing, retail, and autonomous devices. With more accurate positioning, existing location-based apps could even be improved. Perhaps you’ll even see the benefits yourself the next time you hit the mall.
Acknowledgments
XYZ10 is a rising indoor positioning technology company in China. Since 2017, XYZ10 has been accumulating a privacy-sensitive indoor location dataset of WiFi, geomagnetic, and Bluetooth signatures with ground truths from nearly 1,000 buildings.
Microsoft Research is the research subsidiary of Microsoft. Its goal is to advance state-of-the-art computing and solve difficult world research-motivated competition problems through technological innovation in collaboration with academic, government, and industry researchers.",30495 files,59.96 GB,This competition awarded ranking points,This competition counted towards tiers,Research Prediction Competition
Human Protein Atlas - Single Cell Classification,"$25,000 ",2 years ago,,"['image', 'multiclass classification', 'multilabel classification', 'custom metric']",757,991,"19,058","There are billions of humans on this earth, and each of us is made up of trillions of cells. Just like every individual is unique, even genetically identical twins, scientists observe differences between the genetically identical cells in our bodies.
Differences in the location of proteins can give rise to such cellular heterogeneity. Proteins play essential roles in virtually all cellular processes. Often, many different proteins come together at a specific location to perform a task, and the exact outcome of this task depends on which proteins are present. As you can imagine, different subcellular distributions of one protein can give rise to great functional heterogeneity between cells. Finding such differences, and figuring out how and why they occur, is important for understanding how cells function, how diseases develop, and ultimately how to develop better treatments for those diseases.
To see more, start with less. That may seem counterintuitive, but the study of a single cell enables the discovery of mechanisms too difficult to see with multi-cell research. The importance of studying single cells is reflected in the ongoing revolution in biology centered around technologies for single cell analysis. Microscopy offers an opportunity to study differences in protein localizations within a population of cells. Current machine learning models for classifying protein localization patterns in microscope images gives a summary of the entire population of cells. However, the single-cell revolution in biology demands models that can precisely classify patterns in each individual cell in the image.
The Human Protein Atlas is an initiative based in Sweden that is aimed at mapping proteins in all human cells, tissues, and organs. The data in the Human Protein Atlas database is freely accessible to scientists all around the world that allows them to explore the cellular makeup of the human body. Solving the single-cell image classification challenge will help us characterize single-cell heterogeneity in our large collection of images by generating more accurate annotations of the subcellular localizations for thousands of human proteins in individual cells. Thanks to you, we will be able to more accurately model the spatial organization of the human cell and provide new open-access cellular data to the scientific community, which may accelerate our growing understanding of how human cells functions and how diseases develop.
This is a weakly supervised multi-label classification problem and a code competition. Given images of cells from our microscopes and labels of protein location assigned together for all cells in the image, Kagglers will develop models capable of segmenting and classifying each individual cell with precise labels. If successful, you'll contribute to the revolution of single-cell biology!
The scientific journal Nature Methods is interested in considering a paper discussing the outcome and approaches of the challenge. The Human Protein Atlas team, led by Professor Emma Lundberg, would like to invite top performing teams to join as co-authors in writing this paper. Please follow the discussion forum for more details on how you can help.
This is a Code Competition. Refer to Code Requirements for details.",89542 files,170.03 GB,This competition awarded ranking points,This competition counted towards tiers,Featured Code Competition
Hungry Geese,,2 years ago,,['custom metric'],875,"1,039","33,296","Whether it be in an arcade, on a phone, as an app, on a computer, or maybe stumbled upon in a web search, many of us have likely developed fond memories playing a version of Snake. It’s addicting to control a slithering serpent and watch it grow along the grid until you make one… wrong… move. Then you have to try again because surely you won’t make the same mistake twice!
With Hungry Geese, Kaggle has taken this classic in the video game industry and put a multi-player, simulation spin to it. You will create an AI agent to play against others and survive the longest. You must make sure your goose doesn’t starve or run into other geese; it’s a good thing that geese love peppers, donuts, and pizza—which show up across the board.
Extensive research exists in building Snake models using reinforcement learning, Q-learning, neural networks, and more (maybe you’ll use… Python?). Take your grid-based reinforcement learning knowledge to the next level with this exciting new challenge!",,,This competition awarded ranking points,This competition counted towards tiers,Playground Simulation Competition
Tabular Playground Series - Jan 2021,,2 years ago,2 years ago,"['tabular', 'regression', 'rmse']","1,728","1,728","16,104","Kaggle competitions are incredibly fun and rewarding, but they can also be intimidating for people who are relatively new in their data science journey. In the past, we've launched many Playground competitions that are more approachable than our Featured competitions, and thus more beginner-friendly.
In order to have a more consistent offering of these competitions for our community, we're trying a new experiment in 2021. We'll be launching a month-long tabular Playground competition on the 1st of every month, and continue the experiment as long as there's sufficient interest and participation.
The goal of these competitions is to provide a fun, but less challenging, tabular dataset. These competitions will be great for people looking for something in between the Titanic Getting Started competition and a Featured competition. If you're an established competitions master or grandmaster, these probably won't be much of a challenge for you. We encourage you to avoid saturating the leaderboard.
For each monthly competition, we'll be offering Kaggle Merchandise for the top three teams. And finally, because we want these competitions to be more about learning, we're limiting team sizes to 3 individuals.
Good luck and have fun!
Getting Started
Check out this Starter Notebook which walks you through how to make your very first submission!
For more ideas on how to improve your score, check out the Intro to Machine Learning and Intermediate Machine Learning courses on Kaggle Learn.",3 files,144.43 MB,This competition did not award ranking points,This competition did not count towards tiers,Playground Prediction Competition
VinBigData Chest X-ray Abnormalities Detection,"$50,000 ",2 years ago,,"['image', 'healthcare', 'custom metric']","1,275","1,697","29,249","When you have a broken arm, radiologists help save the day—and the bone. These doctors diagnose and treat medical conditions using imaging techniques like CT and PET scans, MRIs, and, of course, X-rays. Yet, as it happens when working with such a wide variety of medical tools, radiologists face many daily challenges, perhaps the most difficult being the chest radiograph. The interpretation of chest X-rays can lead to medical misdiagnosis, even for the best practicing doctor. Computer-aided detection and diagnosis systems (CADe/CADx) would help reduce the pressure on doctors at metropolitan hospitals and improve diagnostic quality in rural areas.
Existing methods of interpreting chest X-ray images classify them into a list of findings. There is currently no specification of their locations on the image which sometimes leads to inexplicable results. A solution for localizing findings on chest X-ray images is needed for providing doctors with more meaningful diagnostic assistance.
Established in August 2018 and funded by the Vingroup JSC, the Vingroup Big Data Institute (VinBigData) aims to promote fundamental research and investigate novel and highly-applicable technologies. The Institute focuses on key fields of data science and artificial intelligence: computational biomedicine, natural language processing, computer vision, and medical image processing. The medical imaging team at VinBigData conducts research in collecting, processing, analyzing, and understanding medical data. They're working to build large-scale and high-precision medical imaging solutions based on the latest advancements in artificial intelligence to facilitate effective clinical workflows.
In this competition, you’ll automatically localize and classify 14 types of thoracic abnormalities from chest radiographs. You'll work with a dataset consisting of 18,000 scans that have been annotated by experienced radiologists. You can train your model with 15,000 independently-labeled images and will be evaluated on a test set of 3,000 images. These annotations were collected via VinBigData's web-based platform, VinLab. Details on building the dataset can be found in our recent paper “VinDr-CXR: An open dataset of chest X-rays with radiologist's annotations”.
If successful, you'll help build what could be a valuable second opinion for radiologists. An automated system that could accurately identify and localize findings on chest radiographs would relieve the stress of busy doctors while also providing patients with a more accurate diagnosis.
Acknowledgments
Challenge Organizing Team
Ha Q. Nguyen, PhD - Vingroup Big Data Institute
Hieu H. Pham, PhD - Vingroup Big Data Institute
Nhan T. Nguyen, MSc - Vingroup Big Data Institute
Dung B. Nguyen, BSc - Vingroup Big Data Institute
Minh Dao, PhD - Vingroup Big Data Institute
Van Vu, PhD - Vingroup Big Data Institute
Khanh Lam, MD, PhD - Hospital 108
Linh T. Le, MD, PhD - Hanoi Medical University Hospital
Data Contributors
The dataset used in this competition was created by assembling de-identified Chest X-ray studies provided by two hospitals in Vietnam: the Hospital 108 and the Hanoi Medical University Hospital.",18002 files,205.96 GB,This competition awarded ranking points,This competition counted towards tiers,Featured Prediction Competition
RANZCR CLiP - Catheter and Line Position Challenge,"$50,000 ",2 years ago,,"['image', 'multilabel classification', 'mcauc']","1,547","1,854","28,112","Serious complications can occur as a result of malpositioned lines and tubes in patients. Doctors and nurses frequently use checklists for placement of lifesaving equipment to ensure they follow protocol in managing patients. Yet, these steps can be time consuming and are still prone to human error, especially in stressful situations when hospitals are at capacity.
Hospital patients can have catheters and lines inserted during the course of their admission and serious complications can arise if they are positioned incorrectly. Nasogastric tube malpositioning into the airways has been reported in up to 3% of cases, with up to 40% of these cases demonstrating complications [1-3]. Airway tube malposition in adult patients intubated outside the operating room is seen in up to 25% of cases [4,5]. The likelihood of complication is directly related to both the experience level and specialty of the proceduralist. Early recognition of malpositioned tubes is the key to preventing risky complications (even death), even more so now that millions of COVID-19 patients are in need of these tubes and lines.
The gold standard for the confirmation of line and tube positions are chest radiographs. However, a physician or radiologist must manually check these chest x-rays to verify that the lines and tubes are in the optimal position. Not only does this leave room for human error, but delays are also common as radiologists can be busy reporting other scans. Deep learning algorithms may be able to automatically detect malpositioned catheters and lines. Once alerted, clinicians can reposition or remove them to avoid life-threatening complications.
The Royal Australian and New Zealand College of Radiologists (RANZCR) is a not-for-profit professional organisation for clinical radiologists and radiation oncologists in Australia, New Zealand, and Singapore. The group is one of many medical organisations around the world (including the NHS) that recognizes malpositioned tubes and lines as preventable. RANZCR is helping design safety systems where such errors will be caught.
In this competition, you’ll detect the presence and position of catheters and lines on chest x-rays. Use machine learning to train and test your model on 40,000 images to categorize a tube that is poorly placed.
The dataset has been labelled with a set of definitions to ensure consistency with labelling. The normal category includes lines that were appropriately positioned and did not require repositioning. The borderline category includes lines that would ideally require some repositioning but would in most cases still function adequately in their current position. The abnormal category included lines that required immediate repositioning.
If successful, your efforts may help clinicians save lives. Earlier detection of malpositioned catheters and lines is even more important as COVID-19 cases continue to surge. Many hospitals are at capacity and more patients are in need of these tubes and lines. Quick feedback on catheter and line placement could help clinicians better treat these patients. Beyond COVID-19, detection of line and tube position will ALWAYS be a requirement in many ill hospital patients.
This is a Code Competition. Refer to Code Requirements for details.
Koopmann MC, Kudsk KA, Szotkowski MJ, Rees SM. A Team-Based Protocol and Electromagnetic Technology Eliminate Feeding Tube Placement Complications [Internet]. Vol. 253, Annals of Surgery. 2011. p. 297–302. Available from: http://dx.doi.org/10.1097/sla.0b013e318208f550
Sorokin R, Gottlieb JE. Enhancing patient safety during feeding-tube insertion: a review of more than 2,000 insertions. JPEN J Parenter Enteral Nutr. 2006 Sep;30(5):440–5.
Marderstein EL, Simmons RL, Ochoa JB. Patient safety: effect of institutional protocols on adverse events related to feeding tube placement in the critically ill. J Am Coll Surg. 2004 Jul;199(1):39–47; discussion 47–50.
Jemmett ME. Unrecognized Misplacement of Endotracheal Tubes in a Mixed Urban to Rural Emergency Medical Services Setting [Internet]. Vol. 10, Academic Emergency Medicine. 2003. p. 961–5. Available from: http://dx.doi.org/10.1197/s1069-6563(03)00315-4
Lotano R, Gerber D, Aseron C, Santarelli R, Pratter M. Utility of postintubation chest radiographs in the intensive care unit. Crit Care. 2000 Jan 24;4(1):50–3.",33699 files,13.13 GB,This competition awarded ranking points,This competition counted towards tiers,Featured Code Competition
Acea Smart Water Analytics,"$25,000 ",2 years ago,2 years ago,"['data analytics', 'tabular', 'water bodies']",,,,"Welcome
The Acea Group is one of the leading Italian multiutility operators. Listed on the Italian Stock Exchange since 1999, the company manages and develops water and electricity networks and environmental services. Acea is the foremost Italian operator in the water services sector supplying 9 million inhabitants in Lazio, Tuscany, Umbria, Molise, Campania.
In this competition we will focus only on the water sector to help Acea Group preserve precious waterbodies. As it is easy to imagine, a water supply company struggles with the need to forecast the water level in a waterbody (water spring, lake, river, or aquifer) to handle daily consumption. During fall and winter waterbodies are refilled, but during spring and summer they start to drain. To help preserve the health of these waterbodies it is important to predict the most efficient water availability, in terms of level and water flow for each day of the year.
Data
The reality is that each waterbody has such unique characteristics that their attributes are not linked to each other. This analytics competition uses datasets that are completely independent from each other. However, it is critical to understand total availability in order to preserve water across the country.
Each dataset represents a different kind of waterbody. As each waterbody is different from the other, the related features are also different. So, if for instance we consider a water spring we notice that its features are different from those of a lake. These variances are expected based upon the unique behavior and characteristics of each waterbody. The Acea Group deals with four different type of waterbodies: water springs, lakes, rivers and aquifers.
Challenge
Can you build a story to predict the amount of water in each unique waterbody? The challenge is to determine how features influence the water availability of each presented waterbody. To be more straightforward, gaining a better understanding of volumes, they will be able to ensure water availability for each time interval of the year.
The time interval is defined as day/month depending on the available measures for each waterbody. Models should capture volumes for each waterbody(for instance, for a model working on a monthly interval a forecast over the month is expected).
The desired outcome is a notebook that can generate four mathematical models, one for each category of waterbody (acquifers, water springs, river, lake) that might be applicable to each single waterbody.






See the Submission Evaluation criteria.",11 files,3.45 MB,,,Analytics Competition
Santa 2020 - The Candy Cane Contest,,2 years ago,2 years ago,['custom metric'],788,911,"24,724","It's the most wonderful time of the year
With the elves eating candy
They’ll feel super dandy and be of good cheer
It's the most wonderful time of the year
It's the hap-happiest season of all
When spirits are lifted the toys will be gifted
And games to enthrall!
It's the hap-happiest season of all
The party for throwing
Has snow cones a’glowing
With bragging rights out on display.
So now you must plan it,
To beat the armed bandits
who keep all the candy away.
It's the most wonderful time of the year!
Morale has been low at the North Pole this year. But Santa really believes in “making spirits bright!” So he has planned a friendly competition among the elves to keep the Christmas cheer alive and make as many toys as possible! And the winning team gets a snow cone party!
As one of the team leaders, you know that nothing keeps your fellow elves more productive and motivated than a steady supply of candy canes! But all seven levels of the Candy Cane Forest are closed for revegetation, so the only ones available are stuck in the break room vending machines. And even though you receive free snacks on the job, the vending machines are always broken and don’t always give you what you want.
Due to social distancing, only two elves can be in the break room at once. You and another team leader will take turns trying to get candy canes out of the 100 possible vending machines in the room, but each machine is unpredictable in how likely it is to work. You do know, however, that the more often you try to use a machine, the less likely it will give you a candy cane. Plus, you only have time to try 2000 times on the vending machines until you need to get back to the workshop!
If you can collect more candy canes than the other team leaders, you’ll surely be able to help your team win Santa's contest! Try your hand at this multi-armed candy cane challenge!
Image Credit: Photos by Joanna Kosinska and Misty Ladd on Unsplash.",,,This competition awarded ranking points,This competition counted towards tiers,Featured Simulation Competition
Jane Street Market Prediction,"$100,000 ",2 years ago,,"['tabular', 'finance', 'custom metric']","4,245",,,"“Buy low, sell high.” It sounds so easy….
In reality, trading for profit has always been a difficult problem to solve, even more so in today’s fast-moving and complex financial markets. Electronic trading allows for thousands of transactions to occur within a fraction of a second, resulting in nearly unlimited opportunities to potentially find and take advantage of price differences in real time.
In a perfectly efficient market, buyers and sellers would have all the agency and information needed to make rational trading decisions. As a result, products would always remain at their “fair values” and never be undervalued or overpriced. However, financial markets are not perfectly efficient in the real world.
Developing trading strategies to identify and take advantage of inefficiencies is challenging. Even if a strategy is profitable now, it may not be in the future, and market volatility makes it impossible to predict the profitability of any given trade with certainty. As a result, it can be hard to distinguish good luck from having made a good trading decision.
In the first three months of this challenge, you will build your own quantitative trading model to maximize returns using market data from a major global stock exchange. Next, you’ll test the predictiveness of your models against future market returns and receive feedback on the leaderboard.
Your challenge will be to use the historical data, mathematical tools, and technological tools at your disposal to create a model that gets as close to certainty as possible. You will be presented with a number of potential trading opportunities, which your model must choose whether to accept or reject.
In general, if one is able to generate a highly predictive model which selects the right trades to execute, they would also be playing an important role in sending the market signals that push prices closer to “fair” values. That is, a better model will mean the market will be more efficient going forward. However, developing good models will be challenging for many reasons, including a very low signal-to-noise ratio, potential redundancy, strong feature correlation, and difficulty of coming up with a proper mathematical formulation.
Jane Street has spent decades developing their own trading models and machine learning solutions to identify profitable opportunities and quickly decide whether to execute trades. These models help Jane Street trade thousands of financial products each day across 200 trading venues around the world.
Admittedly, this challenge far oversimplifies the depth of the quantitative problems Jane Streeters work on daily, and Jane Street is happy with the performance of its existing trading model for this particular question. However, there’s nothing like a good puzzle, and this challenge will hopefully serve as a fun introduction to a type of data science problem that a Jane Streeter might tackle on a daily basis. Jane Street looks forward to seeing the new and creative approaches the Kaggle community will take to solve this trading challenge.
This is a Code Competition. Refer to Code Requirements for details.",,,This competition awarded ranking points,This competition counted towards tiers,Featured Code Competition
NFL Big Data Bowl 2021,"$100,000 ",2 years ago,2 years ago,"['data analytics', 'tabular', 'football']",,,,"When a quarterback takes a snap and drops back to pass, what happens next may seem like chaos. As offensive players move in various patterns, the defense works together to prevent successful pass completions and then to quickly tackle receivers that do catch the ball. In this year’s Kaggle competition, your goal is to use data science to better understand the schemes and players that make for a successful defense against passing plays.
In American football, there are a plethora of defensive strategies and outcomes. The National Football League (NFL) has used previous Kaggle competitions to focus on offensive plays, but as the old proverb goes, “defense wins championships.” Though metrics for analyzing quarterbacks, running backs, and wide receivers are consistently a part of public discourse, techniques for analyzing the defensive part of the game trail and lag behind. Identifying player, team, or strategic advantages on the defensive side of the ball would be a significant breakthrough for the game.
This competition uses NFL’s Next Gen Stats data, which includes the position and speed of every player on the field during each play. You’ll employ player tracking data for all drop-back pass plays from the 2018 regular season. The goal of submissions is to identify unique and impactful approaches to measure defensive performance on these plays. There are several different directions for participants to ‘tackle’ (ha)—which may require levels of football savvy, data aptitude, and creativity. As examples:
What are coverage schemes (man, zone, etc) that the defense employs? What coverage options tend to be better performing?
Which players are the best at closely tracking receivers as they try to get open?
Which players are the best at closing on receivers when the ball is in the air?
Which players are the best at defending pass plays when the ball arrives?
Is there any way to use player tracking data to predict whether or not certain penalties – for example, defensive pass interference – will be called?
Who are the NFL’s best players against the pass?
How does a defense react to certain types of offensive plays?
Is there anything about a player – for example, their height, weight, experience, speed, or position – that can be used to predict their performance on defense?
What does data tell us about defending the pass play? You are about to find out.
Note: Are you a university participant? Students have the option to participate in a college-only Competition, where you’ll work on the identical themes above. Students can opt-in for either the Open or College Competitions, but not both.",20 files,2.33 GB,,,Analytics Competition
CDP - Unlocking Climate Solutions,"$91,000 ",2 years ago,2 years ago,"['environment', 'survey analysis', 'pollution']",,,,"CDP is a global non-profit that drives companies and governments to reduce their greenhouse gas emissions, safeguard water resources, and protect forests. Each year, CDP takes the information supplied in its annual reporting process and scores companies and cities based on their journey through disclosure and towards environmental leadership.
CDP houses the world’s largest, most comprehensive dataset on environmental action. As the data grows to include thousands more companies and cities each year, there is increasing potential for the data to be utilized in impactful ways. Because of this potential, CDP is excited to launch an analytics challenge for the Kaggle community. Data scientists will scour environmental information provided to CDP by disclosing companies and cities, searching for solutions to our most pressing problems related to climate change, water security, deforestation, and social inequity.
How do you help cities adapt to a rapidly changing climate amidst a global pandemic, but do it in a way that is socially equitable?
What are the projects that can be invested in that will help pull cities out of a recession, mitigate climate issues, but not perpetuate racial/social inequities?
What are the practical and actionable points where city and corporate ambition join, i.e. where do cities have problems that corporations affected by those problems could solve, and vice versa?
How can we measure the intersection between environmental risks and social equity, as a contributor to resiliency?
PROBLEM STATEMENT
Develop a methodology for calculating key performance indicators (KPIs) that relate to the environmental and social issues that are discussed in the CDP survey data. Leverage external data sources and thoroughly discuss the intersection between environmental issues and social issues. Mine information to create automated insight generation demonstrating whether city and corporate ambitions take these factors into account.
HOW TO PARTICIPATE
To make a submission, complete the submission form. Only one submission will be judged per participant, so if you make multiple submissions we will only review the most recent entry. A starter notebook demonstrates how to load and work with the data.
To be valid, a submission must be contained in one or more notebook, and made public on or before the submission deadline. Participants are free to use any datasets in addition to the official Kaggle dataset, but those datasets must be public and hosted on Kaggle for the submission to be valid.",49 files,1.97 GB,,,Analytics Competition
INGV - Volcanic Eruption Prediction,,2 years ago,2 years ago,"['physics', 'geology', 'signal processing', 'mae']",620,760,"9,202","What if scientists could anticipate volcanic eruptions as they predict the weather? While determining rain or shine days in advance is more difficult, weather reports become more accurate on shorter time scales. A similar approach with volcanoes could make a big impact. Just one unforeseen eruption can result in tens of thousands of lives lost. If scientists could reliably predict when a volcano will next erupt, evacuations could be more timely and the damage mitigated.
Currently, scientists often identify “time to eruption” by surveying volcanic tremors from seismic signals. In some volcanoes, this intensifies as volcanoes awaken and prepare to erupt. Unfortunately, patterns of seismicity are difficult to interpret. In very active volcanoes, current approaches predict eruptions some minutes in advance, but they usually fail at longer-term predictions.
Enter Italy's Istituto Nazionale di Geofisica e Vulcanologia (INGV), with its focus on geophysics and volcanology. The INGV's main objective is to contribute to the understanding of the Earth's system while mitigating the associated risks. Tasked with the 24-hour monitoring of seismicity and active volcano activity across the country, the INGV seeks to find the earliest detectable precursors that provide information about the timing of future volcanic eruptions.
In this competition, using your data science skills, you’ll predict when a volcano's next eruption will occur. You'll analyze a large geophysical dataset collected by sensors deployed on active volcanoes. If successful, your algorithms will identify signatures in seismic waveforms that characterize the development of an eruption.
With enough notice, areas around a volcano can be safely evacuated prior to their destruction. Seismic activity is a good indicator of an impending eruption, but earlier precursors must be identified to improve longer-term predictability. The impact of your participation could be felt worldwide with tens of thousands of lives saved by more predictable volcanic ruptures and earlier evacuations.",8953 files,31.25 GB,This competition did not award ranking points,This competition did not count towards tiers,Playground Prediction Competition
Riiid Answer Correctness Prediction,"$100,000 ",2 years ago,,"['education', 'tabular', 'auc']","3,395","4,387","64,378","Riiid AIEd Challenge 2020
Challenge Website
Thank you for all those who attended the AAAI-2021 workshop on AI Education! Prize-winning teams presented their models at the AAAI-2021 Workshop on AI Education - Imagining Post-COVID Education with AI - on February 9, 2021. You can find the model write-ups on the workshop website.
Think back to your favorite teacher. They motivated and inspired you to learn. And they knew your strengths and weaknesses. The lessons they taught were based on your ability. For example, teachers would make sure you understood algebra before advancing to calculus. Yet, many students don’t have access to personalized learning. In a world full of information, data scientists like you can help. Machine learning can offer a path to success for young people around the world, and you are invited to be part of this mission.
In 2018, 260 million children weren't attending school. At the same time, more than half of these young students didn't meet minimum reading and math standards. Education was already in a tough place when COVID-19 forced most countries to temporarily close schools. This further delayed learning opportunities and intellectual development. The equity gaps in every country could grow wider. We need to re-think the current education system in terms of attendance, engagement, and individualized attention.
Riiid Labs, an AI solutions provider delivering creative disruption to the education market, empowers global education players to rethink traditional ways of learning leveraging AI. With a strong belief in equal opportunity in education, Riiid launched an AI tutor based on deep-learning algorithms in 2017 that attracted more than one million South Korean students. This year, the company released EdNet, the world’s largest open database for AI education containing more than 100 million student interactions.
In this competition, your challenge is to create algorithms for ""Knowledge Tracing,"" the modeling of student knowledge over time. The goal is to accurately predict how students will perform on future interactions. You will pair your machine learning skills using Riiid’s EdNet data.
Your innovative algorithms will help tackle global challenges in education. If successful, it’s possible that any student with an Internet connection can enjoy the benefits of a personalized learning experience, regardless of where they live. With your participation, we can build a better and more equitable model for education in a post-COVID-19 world.
Acknowledgements
Academic Advisors
Paul Kim, Stanford Graduate School of Education
Neil Heffernan, WPI & ASSISTments
Partners",7 files,5.85 GB,This competition awarded ranking points,This competition counted towards tiers,Featured Code Competition
Google Research Football with Manchester City F.C.,"$6,000 ",2 years ago,,"['reinforcement learning', 'simulations', 'custom metric']","1,138","1,288","14,015","Manchester City F.C. and Google Research are proud to present AI football competition using the Google Research Football Environment.
A word from Manchester City F.C.
Brian Prestidge, Director of Data Insights & Decision Technology at City Football Group, the owners of Manchester City F.C., sets out the challenge. “Football is a tough environment to perform in and an even tougher environment to learn in. Learning is all about harnessing failure, but failure in football is seldom accepted. Working with Google Research’s physics based football environment provides us with a new place to learn through simulation and offers us the capabilities to test tactical concepts and refine principles so that they are strong enough for a coach to stake their career on.”
“We are therefore very pleased to be working with Google’s research team in creating this competition and are looking forward to the opportunity to support some of the most creative and successful competitors through funding and exclusive prizes. We hope to establish ongoing collaboration with the winners beyond this competition, and that it will provide us all with the platform to explore and establish fundamental principles of football tactics, thus improving our ability to perform and be successful on the pitch.”
Greg Swimer, Chief Technology Officer at City Football Group added ""Technologies such as Machine Learning and Artificial Intelligence have huge future potential to enhance the understanding and enjoyment of football for players, coaches and fans. We are delighted to be collaborating with Google's research team to help broaden the knowledge, talent, and innovation working in this exciting and transformational area"".
The Google Research football environment competition
The world gets a kick out of football (soccer in the United States). As the most popular sport on the planet, millions of fans enjoy watching Sergio Agüero, Raheem Sterling, and Kevin de Bruyne on the field. Football video games are less lively, but still immensely popular, and we wonder if AI agents would be able to play those properly.
Researchers want to explore AI agents' ability to play in complex settings like football. The sport requires a balance of short-term control, learned concepts such as passing, and high-level strategy, which can be difficult to teach agents. A current environment exists to train and test agents, but other solutions may offer better results.
The teams at Google Research aspire to make discoveries that impact everyone. Essential to their approach is sharing research and tools to fuel progress in the field. Together with Manchester City F.C., Google Research has put forth this competition to get help in reaching their goal.
In this competition, you’ll create AI agents that can play football. Teams compete in “steps,” where agents react to a game state. Each agent in an 11 vs 11 game controls a single active player and takes actions to improve their team’s situation. As with a typical football game, you want your team to score more than the other side. You can optionally see your efforts rendered in a physics-based 3D football simulation.
If controlling 11 football players with code sounds difficult, don't be discouraged! You only need to control one player at a time (the one with the ball on offense, or the one closest to the ball on defense) and your code gets to pick from 1 of 19 possible actions. We have prepared a getting started example to show you how simple a basic strategy can be. Before implementing your own strategy, however, you might want to learn more about the Google Research football environment, especially observations provided to you by the environment and available actions. You can also play the game yourself on your computer locally to get better understanding of the environment's dynamics and explore different scenarios.
If successful, you'll help researchers explore the ability of AI agents to play in complex settings. This could offer new insights into the strategies of the world's most-watched sport. Additionally, this research could pave the way for a new generation of AI agents that can be trained to learn complex skills.",,,This competition awarded ranking points,This competition counted towards tiers,Featured Simulation Competition
Halite by Two Sigma - Playground Edition,,2 years ago,2 years ago,['custom metric'],79,102,241,"Note: This simulation is a playground competition extending the fourth season of Halite for participation. We have modified the rules to serve as a two-player game instead of four-player game. No points or medals will be awarded for this competition.
Ahoy there! There's halite to be had and ships to be deployed! Are you ready to navigate the skies and secure your territory?
Halite by Two Sigma (""Halite"") is a resource management game where you build and control a small armada of ships. Your algorithms determine their movements to collect halite, a luminous energy source. The most halite at the end of the match wins, but it's up to you to figure out how to make effective and efficient moves. You control your fleet, build new ships, create shipyards, and mine the regenerating halite on the game board.
Created by Two Sigma in 2016, more than 15,000 people around the world have participated in a Halite challenge. Players apply advanced algorithms in a dynamic, open source game setting. The strategic depth and immersive, interactive nature of Halite games make each challenge a unique learning environment.
Halite IV builds on the core game design of Halite III with a number of key changes that shift the focus of the game towards tighter competition on a smaller board. New game features include regenerating halite, shipyard creation, no more ship movement costs, and stealing halite from other players!
So dust off your halite meters and fasten your seatbelts. The fourth season of Halite is about to begin!",,,This competition did not award ranking points,This competition did not count towards tiers,Playground Simulation Competition
OpenVaccine: COVID-19 mRNA Vaccine Degradation Prediction,"$25,000 ",2 years ago,,"['biology', 'covid19', 'public health', 'biotechnology', 'custom metric']","1,636","1,839","35,806","Winning the fight against the COVID-19 pandemic will require an effective vaccine that can be equitably and widely distributed. Building upon decades of research has allowed scientists to accelerate the search for a vaccine against COVID-19, but every day that goes by without a vaccine has enormous costs for the world nonetheless. We need new, fresh ideas from all corners of the world. Could online gaming and crowdsourcing help solve a worldwide pandemic? Pairing scientific and crowdsourced intelligence could help computational biochemists make measurable progress.
mRNA vaccines have taken the lead as the fastest vaccine candidates for COVID-19, but currently, they face key potential limitations. One of the biggest challenges right now is how to design super stable messenger RNA molecules (mRNA). Conventional vaccines (like your seasonal flu shots) are packaged in disposable syringes and shipped under refrigeration around the world, but that is not currently possible for mRNA vaccines.
Researchers have observed that RNA molecules have the tendency to spontaneously degrade. This is a serious limitation--a single cut can render the mRNA vaccine useless. Currently, little is known on the details of where in the backbone of a given RNA is most prone to being affected. Without this knowledge, current mRNA vaccines against COVID-19 must be prepared and shipped under intense refrigeration, and are unlikely to reach more than a tiny fraction of human beings on the planet unless they can be stabilized.
The Eterna community, led by Professor Rhiju Das, a computational biochemist at Stanford’s School of Medicine, brings together scientists and gamers to solve puzzles and invent medicine. Eterna is an online video game platform that challenges players to solve scientific problems such as mRNA design through puzzles. The solutions are synthesized and experimentally tested at Stanford by researchers to gain new insights about RNA molecules. The Eterna community has previously unlocked new scientific principles, made new diagnostics against deadly diseases, and engaged the world’s most potent intellectual resources for the betterment of the public. The Eterna community has advanced biotechnology through its contribution in over 20 publications, including advances in RNA biotechnology.
In this competition, we are looking to leverage the data science expertise of the Kaggle community to develop models and design rules for RNA degradation. Your model will predict likely degradation rates at each base of an RNA molecule, trained on a subset of an Eterna dataset comprising over 3000 RNA molecules (which span a panoply of sequences and structures) and their degradation rates at each position. We will then score your models on a second generation of RNA sequences that have just been devised by Eterna players for COVID-19 mRNA vaccines. These final test sequences are currently being synthesized and experimentally characterized at Stanford University in parallel to your modeling efforts -- Nature will score your models!
Improving the stability of mRNA vaccines was a problem that was being explored before the pandemic but was expected to take many years to solve. Now, we must solve this deep scientific challenge in months, if not weeks, to accelerate mRNA vaccine research and deliver a refrigerator-stable vaccine against SARS-CoV-2, the virus behind COVID-19. The problem we are trying to solve has eluded academic labs, industry R&D groups, and supercomputers, and so we are turning to you. To help, you can join the team of video game players, scientists, and developers at Eterna to unlock the key in our fight against this devastating pandemic.
                  ",6273 files,2.68 GB,This competition awarded ranking points,This competition counted towards tiers,Research Prediction Competition
RSNA STR Pulmonary Embolism Detection,"$30,000 ",2 years ago,,"['health', 'image', 'weightedmeancolumnwiselogloss']",784,981,"11,408","If every breath is strained and painful, it could be a serious and potentially life-threatening condition. A pulmonary embolism (PE) is caused by an artery blockage in the lung. It is time consuming to confirm a PE and prone to overdiagnosis. Machine learning could help to more accurately identify PE cases, which would make management and treatment more effective for patients.
Currently, CT pulmonary angiography (CTPA), is the most common type of medical imaging to evaluate patients with suspected PE. These CT scans consist of hundreds of images that require detailed review to identify clots within the pulmonary arteries. As the use of imaging continues to grow, constraints of radiologists’ time may contribute to delayed diagnosis.
The Radiological Society of North America (RSNA®) has teamed up with the Society of Thoracic Radiology (STR) to help improve the use of machine learning in the diagnosis of PE.
In this competition, you’ll detect and classify PE cases. In particular, you'll use chest CTPA images (grouped together as studies) and your data science skills to enable more accurate identification of PE. If successful, you'll help reduce human delays and errors in detection and treatment.
With 60,000-100,000 PE deaths annually in the United States, it is among the most fatal cardiovascular diseases. Timely and accurate diagnosis will help these patients receive better care and may also improve outcomes.
This is a Code Competition. Refer to Code Requirements for details.
Acknowledgments
The Radiological Society of North America (RSNA®) is an international society of radiologists, medical physicists, and other medical professionals with more than 53,400 members worldwide. RSNA hosts the world’s premier radiology forum and publishes two top peer-reviewed journals: Radiology, the highest-impact scientific journal in the field, and RadioGraphics, the only journal dedicated to continuing education in radiology.
The Society of Thoracic Radiology (STR) was founded in 1982. The STR is dedicated to advancing cardiothoracic imaging in clinical application, education, and research in radiology and allied disciplines. Continuing professional development opportunities provided by the STR include educational and scientific meetings, mentorship programs, grant support and award opportunities, our society journal, Journal of Thoracic Imaging, and global collaboration activities.
A full set of acknowledgments can be found on this page.",1937450 files,980.24 GB,This competition awarded ranking points,This competition counted towards tiers,Featured Code Competition
Hash Code Archive - Drone Delivery,,2 years ago,2 years ago,"['internet', 'optimization', 'custom metric']",130,130,"1,291","This is a synthetic code challenge to sharpen your programming skills. This problem was first released during the 2016 qualification round of Google's annual coding competition, Hash Code. We’ve re-released it as a Playground Code Competition to help you sharpen your skills. Along with the Photo Slideshow Optimization competition, open for late submissions, you can use it as practice in advance of Hash Code 2021.
The Internet has profoundly changed the way we buy things, but the online shopping of today is likely not the end of that change; the expectations for purchase delivery has gone from a week, to two days, to one day, to same day. What about in just a few hours? With drones, this may be possible, and they’ll bring a whole new fleet of problems to solve with data science.
Drones are­ autonomous, electric vehicles often used to deliver online purchases. Current experiments use flying drones, so they’re never stuck in traffic. As drone technology improves every year, there remains a major issue: how would we manage and coordinate all those drones?
In this competition, you are given a hypothetical fleet of drones, a list of customer orders, and availability of the individual products in warehouses. Can you schedule the drone operations so that the orders are completed as soon as possible?
When flying delivery drones become the norm, scheduling is one of the many problems to be solved. Get a head start—and improve your data science skills at the same time.
This is a Code Competition. Refer to Code Requirements for details.
Photo by Ian Usher on Unsplash",2 files,395.18 kB,This competition did not award ranking points,This competition did not count towards tiers,Playground Code Competition
Mechanisms of Action (MoA) Prediction,"$30,000 ",2 years ago,,"['tabular', 'biology', 'drugs and medications', 'genetics', 'meancolumnwiselogloss']","4,373","5,323","88,732","The Connectivity Map, a project within the Broad Institute of MIT and Harvard, the Laboratory for Innovation Science at Harvard (LISH), and the NIH Common Funds Library of Integrated Network-Based Cellular Signatures (LINCS), present this challenge with the goal of advancing drug development through improvements to MoA prediction algorithms.
What is the Mechanism of Action (MoA) of a drug? And why is it important?
In the past, scientists derived drugs from natural products or were inspired by traditional remedies. Very common drugs, such as paracetamol, known in the US as acetaminophen, were put into clinical use decades before the biological mechanisms driving their pharmacological activities were understood. Today, with the advent of more powerful technologies, drug discovery has changed from the serendipitous approaches of the past to a more targeted model based on an understanding of the underlying biological mechanism of a disease. In this new framework, scientists seek to identify a protein target associated with a disease and develop a molecule that can modulate that protein target. As a shorthand to describe the biological activity of a given molecule, scientists assign a label referred to as mechanism-of-action or MoA for short.
How do we determine the MoAs of a new drug?
One approach is to treat a sample of human cells with the drug and then analyze the cellular responses with algorithms that search for similarity to known patterns in large genomic databases, such as libraries of gene expression or cell viability patterns of drugs with known MoAs.
In this competition, you will have access to a unique dataset that combines gene expression and cell viability data. The data is based on a new technology that measures simultaneously (within the same samples) human cells’ responses to drugs in a pool of 100 different cell types (thus solving the problem of identifying ex-ante, which cell types are better suited for a given drug). In addition, you will have access to MoA annotations for more than 5,000 drugs in this dataset.
As is customary, the dataset has been split into testing and training subsets. Hence, your task is to use the training dataset to develop an algorithm that automatically labels each case in the test set as one or more MoA classes. Note that since drugs can have multiple MoA annotations, the task is formally a multi-label classification problem.
How to evaluate the accuracy of a solution?
Based on the MoA annotations, the accuracy of solutions will be evaluated on the average value of the logarithmic loss function applied to each drug-MoA annotation pair.
If successful, you’ll help to develop an algorithm to predict a compound’s MoA given its cellular signature, thus helping scientists advance the drug discovery process.
This is a Code Competition. Refer to Code Requirements for details.",6 files,215.96 MB,This competition awarded ranking points,This competition counted towards tiers,Research Code Competition
Conway's Reverse Game of Life 2020,,2 years ago,2 years ago,"['simulations', 'board games', 'custom metric']",188,259,"1,377","This is a relaunch of a previous competition, Conway's Reverse Game of Life, with the following changes:
The grid size is larger (25 vs. 25) and the grid wraps around from top to bottom and left to right
Submissions are solved forward by the appropriate number of steps, so that any correct starting solution will achieve a maximum score. This article contains the stepping function that is used for this competition.
Obligatory Disclaimer: A lot has changed since the original competition was launched 6 years ago. With the change from ""exact starting point"" to ""any correct starting point"", it is possible to get a perfect score. We just don't know how difficult that will be. Use it as a fun learning experience, and don't spoil it for others by posting perfect solutions!
~~~~~~~~~
The Game of Life is a cellular automaton created by mathematician John Conway in 1970. The game consists of a board of cells that are either on or off. One creates an initial configuration of these on/off states and observes how it evolves. There are four simple rules to determine the next state of the game board, given the current state:
Overpopulation: if a living cell is surrounded by more than three living cells, it dies.
Stasis: if a living cell is surrounded by two or three living cells, it survives.
Underpopulation: if a living cell is surrounded by fewer than two living cells, it dies.
Reproduction: if a dead cell is surrounded by exactly three cells, it becomes a live cell.
These simple rules result in many interesting behaviors and have been the focus of a large body of mathematics. As Wikipedia states
Ever since its publication, Conway's Game of Life has attracted much interest, because of the surprising ways in which the patterns can evolve. Life provides an example of emergence and self-organization. It is interesting for computer scientists, physicists, biologists, biochemists, economists, mathematicians, philosophers, generative scientists and others to observe the way that complex patterns can emerge from the implementation of very simple rules. The game can also serve as a didactic analogy, used to convey the somewhat counter-intuitive notion that ""design"" and ""organization"" can spontaneously emerge in the absence of a designer. For example, philosopher and cognitive scientist Daniel Dennett has used the analogue of Conway's Life ""universe"" extensively to illustrate the possible evolution of complex philosophical constructs, such as consciousness and free will, from the relatively simple set of deterministic physical laws governing our own universe.
The emergence of order from simple rules begs an interesting question—what happens if we set time backwards?
This competition is an experiment to see if machine learning (or optimization, or any method) can predict the game of life in reverse. Is the chaotic start of Life predictable from its orderly ends? We have created many games, evolved them, and provided only the end boards. You are asked to predict the starting board that resulted in each end board.
This is a Code Competition. Refer to Code Requirements for details.",3 files,251.11 MB,This competition did not award ranking points,This competition did not count towards tiers,Playground Code Competition
I’m Something of a Painter Myself,,,,"['image', 'gan', 'custom metric']",98,108,424,"“Every artist dips his brush in his own soul, and paints his own nature into his pictures.”
-Henry Ward Beecher
We recognize the works of artists through their unique style, such as color choices or brush strokes. The “je ne sais quoi” of artists like Claude Monet can now be imitated with algorithms thanks to generative adversarial networks (GANs). In this getting started competition, you will bring that style to your photos or recreate the style from scratch!
Computer vision has advanced tremendously in recent years and GANs are now capable of mimicking objects in a very convincing way. But creating museum-worthy masterpieces is thought of to be, well, more art than science. So can (data) science, in the form of GANs, trick classifiers into believing you’ve created a true Monet? That’s the challenge you’ll take on!
The Challenge:
A GAN consists of at least two neural networks: a generator model and a discriminator model. The generator is a neural network that creates the images. For our competition, you should generate images in the style of Monet. This generator is trained using a discriminator.
The two models will work against each other, with the generator trying to trick the discriminator, and the discriminator trying to accurately classify the real vs. generated images.
Your task is to build a GAN that generates 7,000 to 10,000 Monet-style images.
Getting Started:
Details on the dataset can be found here and an overview of the evaluation process can be found here.
To learn how to submit and answers to other FAQs, review the Frequently Asked Questions.
Recommended Tutorial
We highly recommend Amy Jang's notebook that goes over the basics of loading data from TFRecords, using TPUs, and building a CycleGAN.
Although the competition dataset only includes Monet images, check out this dataset for Cezanne, Ukiyo-e, and Van Gogh paintings to run your GAN on.",7363 files,385.87 MB,This competition does not award ranking points,This competition does not count towards tiers,Getting Started Code Competition
Lyft Motion Prediction for Autonomous Vehicles,"$30,000 ",2 years ago,,"['tabular', 'image', 'automobiles and vehicles', 'transportation', 'custom metric']",935,"1,254","14,900","Autonomous vehicles (AVs) are expected to dramatically redefine the future of transportation. However, there are still significant engineering challenges to be solved before one can fully realize the benefits of self-driving cars. One such challenge is building models that reliably predict the movement of traffic agents around the AV, such as cars, cyclists, and pedestrians.
The ridesharing company Lyft started Level 5 to take on the self-driving challenge and build a full self-driving system (they’re hiring!). Their previous competition tasked participants with identifying 3D objects, an important step prior to detecting their movement. Now, they’re challenging you to predict the motion of these traffic agents.
In this competition, you’ll apply your data science skills to build motion prediction models for self-driving vehicles. You'll have access to the largest Prediction Dataset ever released to train and test your models. Your knowledge of machine learning will then be required to predict how cars, cyclists,and pedestrians move in the AV's environment.
Lyft’s mission is to improve people’s lives with the world’s best transportation. They believe in a future where self-driving cars make transportation safer, environment-friendly and more accessible for everyone. Their goal is to accelerate development across the industry by sharing data with researchers. As a result of your participation, you can have a hand in propelling the industry forward and helping people around the world benefit from self-driving cars sooner.
This is a Code Competition. Refer to Code Requirements for details.",108580 files,23.71 GB,This competition awarded ranking points,This competition counted towards tiers,Featured Code Competition
"Contradictory, My Dear Watson",,,,"['nlp', 'text', 'multiclass classification', 'categorizationaccuracy']",71,86,203,"""…when you have eliminated the impossible, whatever remains, however improbable, must be the truth""
-Sir Arthur Conan Doyle
Our brains process the meaning of a sentence like this rather quickly.
We're able to surmise:
Some things to be true: ""You can find the right answer through the process of elimination.”
Others that may have truth: ""Ideas that are improbable are not impossible!""
And some claims are clearly contradictory: ""Things that you have ruled out as impossible are where the truth lies.""
Natural language processing (NLP) has grown increasingly elaborate over the past few years. Machine learning models tackle question answering, text extraction, sentence generation, and many other complex tasks. But, can machines determine the relationships between sentences, or is that still left to humans? If NLP can be applied between sentences, this could have profound implications for fact-checking, identifying fake news, analyzing text, and much more.
The Challenge:
If you have two sentences, there are three ways they could be related: one could entail the other, one could contradict the other, or they could be unrelated. Natural Language Inferencing (NLI) is a popular NLP problem that involves determining how pairs of sentences (consisting of a premise and a hypothesis) are related.
Your task is to create an NLI model that assigns labels of 0, 1, or 2 (corresponding to entailment, neutral, and contradiction) to pairs of premises and hypotheses. To make things more interesting, the train and test set include text in fifteen different languages! You can find more details on the dataset by reviewing the Data page.
Today, the most common approaches to NLI problems include using embeddings and transformers like BERT. In this competition, we’re providing a starter notebook to try your hand at this problem using the power of Tensor Processing Units (TPUs). TPUs are powerful hardware accelerators specialized in deep learning tasks, including Natural Language Processing. Kaggle provides all users TPU Quota at no cost, which you can use to explore this competition. Check out our TPU documentation and Kaggle’s YouTube playlist for more information and resources.
Recommended Tutorial
We highly recommend Ana Sofia Uzsoy’s Tutorial that walks you through creating your very first submission step by step with TPUs and BERT.
This is a great opportunity to flex your NLP muscles and solve an exciting problem!
Disclaimer: The dataset for this competition contains text that may be considered profane, vulgar, or offensive.",3 files,4.02 MB,This competition does not award ranking points,This competition does not count towards tiers,Getting Started Code Competition
Google Landmark Recognition 2020,"$25,000 ",2 years ago,,"['image', 'computer vision', 'custom metric']",736,937,"18,783","Welcome to the third Landmark Recognition competition! This year, we have worked to set this up as a code competition and collected a new set of test images.
Have you ever gone through your vacation photos and asked yourself: What was the name of that temple I visited in China? or Who created this monument I saw in France? Landmark recognition can help! This technology can predict landmark labels directly from image pixels, to help people better understand and organize their photo collections. This competition challenges Kagglers to build models that recognize the correct landmark (if any) in a dataset of challenging test images.
Many Kagglers are familiar with image classification challenges like the ImageNet Large Scale Visual Recognition Challenge (ILSVRC), which aims to recognize 1K general object categories. Landmark recognition is a little different from that: it contains a much larger number of classes (there are more than 81K classes in this challenge), and the number of training examples per class may not be very large. Landmark recognition is challenging in its own way.
In the previous editions of this challenge (2018 and 2019), submissions were handled by uploading prediction files to the system. This year's competition is structured in a synchronous rerun format, where participants need to submit their Kaggle notebooks for scoring.
This challenge is organized in conjunction with the Landmark Retrieval Challenge 2020, which was launched June 30, 2020. Both challenges are affiliated with the Instance-Level Recognition workshop in ECCV’20.
This is a Code Competition. Refer to Code Requirements for details.",1590817 files,105.52 GB,This competition awarded ranking points,This competition counted towards tiers,Research Code Competition
OSIC Pulmonary Fibrosis Progression,"$55,000 ",2 years ago,,"['image', 'healthcare', 'laplaceloglikelihood']","2,097","2,530","44,505","Imagine one day, your breathing became consistently labored and shallow. Months later you were finally diagnosed with pulmonary fibrosis, a disorder with no known cause and no known cure, created by scarring of the lungs. If that happened to you, you would want to know your prognosis. That’s where a troubling disease becomes frightening for the patient: outcomes can range from long-term stability to rapid deterioration, but doctors aren’t easily able to tell where an individual may fall on that spectrum. Your help, and data science, may be able to aid in this prediction, which would dramatically help both patients and clinicians.
Current methods make fibrotic lung diseases difficult to treat, even with access to a chest CT scan. In addition, the wide range of varied prognoses create issues organizing clinical trials. Finally, patients suffer extreme anxiety—in addition to fibrosis-related symptoms—from the disease’s opaque path of progression.
Open Source Imaging Consortium (OSIC) is a not-for-profit, co-operative effort between academia, industry and philanthropy. The group enables rapid advances in the fight against Idiopathic Pulmonary Fibrosis (IPF), fibrosing interstitial lung diseases (ILDs), and other respiratory diseases, including emphysematous conditions. Its mission is to bring together radiologists, clinicians and computational scientists from around the world to improve imaging-based treatments.
In this competition, you’ll predict a patient’s severity of decline in lung function based on a CT scan of their lungs. You’ll determine lung function based on output from a spirometer, which measures the volume of air inhaled and exhaled. The challenge is to use machine learning techniques to make a prediction with the image, metadata, and baseline FVC as input.
If successful, patients and their families would better understand their prognosis when they are first diagnosed with this incurable lung disease. Improved severity detection would also positively impact treatment trial design and accelerate the clinical development of novel treatments.

This is a Code Competition. Refer to Code Requirements for details.",34290 files,23.99 GB,This competition awarded ranking points,This competition counted towards tiers,Featured Code Competition
Google Landmark Retrieval 2020,"$25,000 ",2 years ago,,"['image', 'computer vision', 'custom metric']",541,665,"4,983","Welcome to the third Landmark Retrieval competition! This year, we have worked to set this up as a code competition and we have completely refreshed the test and index image sets.
Image retrieval is a fundamental problem in computer vision: given a query image, can you find similar images in a large database? This is especially important for query images containing landmarks, which accounts for a large portion of what people like to photograph.
In this competition, the developed models are expected to retrieve relevant database images to a given query image (ie, the model should retrieve database images containing the same landmark as the query). This challenge is organized in conjunction with the Landmark Recognition Challenge 2020. Both challenges will be discussed at the Instance-Level Recognition workshop in ECCV’20.
In the previous editions of this challenge (2018 and 2019), submissions were handled by uploading prediction files to the system. This year's competition is structured in a representation learning format: rather than creating a submission file with retrieved images, you will create a model that extracts a feature embedding for the images and submit the model via Kaggle Notebooks. Kaggle will run your model on a held-out test set, perform a k-nearest-neighbors lookup, and score the resulting embedding quality with mean average precision.
This is a Code Competition. Refer to Code Requirements for details.",1657776 files,109.51 GB,This competition awarded ranking points,This competition counted towards tiers,Research Code Competition
Petals to the Metal - Flower Classification on TPU,,,,"['image', 'multiclass classification', 'fscoremacro']",144,156,556,"Learn how to use Tensor Processing Units (TPUs) on Kaggle
TPUs are powerful hardware accelerators specialized in deep learning tasks. They were developed (and first used) by Google to process large image databases, such as extracting all the text from Street View. This competition is designed for you to give TPUs a try.
TPU quotas are available on Kaggle at no cost to users.
Watch the video below to see how to get started! You can follow along with this notebook.
The Challenge
It’s difficult to fathom just how vast and diverse our natural world is.
There are over 5,000 species of mammals, 10,000 species of birds, 30,000 species of fish – and astonishingly, over 400,000 different types of flowers.
In this competition, you’re challenged to build a machine learning model that identifies the type of flowers in a dataset of images (for simplicity, we’re sticking to just over 100 types).
Recommended Tutorial
We highly recommend Ryan Holbrook’s Tutorial that walks you through making your very first submission step by step.
Have Questions?
Kaggle Data Scientists will be actively monitoring the competition forum - your fellow data scientists and TPU users will be there too! If you have a question or need help troubleshooting, that’s the best place to find help.
Learn More
Check out Kaggle’s Youtube playlist for more videos introducing TPUs.
Read the TPU documentation for more information and resources.
Many thanks to Martin Görner, Google Developer Advocate and author of Tensorflow without a PhD for his tireless work on the dataset, the notebooks, and the original competition that this Getting Started competition draws from.",193 files,5.15 GB,This competition does not award ranking points,This competition does not count towards tiers,Getting Started Code Competition
Cornell Birdcall Identification,"$25,000 ",2 years ago,,"['audio', 'fscorebetamicro']","1,390","1,630","17,938","Do you hear the birds chirping outside your window? Over 10,000 bird species occur in the world, and they can be found in nearly every environment, from untouched rainforests to suburbs and even cities. Birds play an essential role in nature. They are high up in the food chain and integrate changes occurring at lower levels. As such, birds are excellent indicators of deteriorating habitat quality and environmental pollution. However, it is often easier to hear birds than see them. With proper sound detection and classification, researchers could automatically intuit factors about an area’s quality of life based on a changing bird population.
There are already many projects underway to extensively monitor birds by continuously recording natural soundscapes over long periods. However, as many living and nonliving things make noise, the analysis of these datasets is often done manually by domain experts. These analyses are painstakingly slow, and results are often incomplete. Data science may be able to assist, so researchers have turned to large crowdsourced databases of focal recordings of birds to train AI models. Unfortunately, there is a domain mismatch between the training data (short recording of individual birds) and the soundscape recordings (long recordings with often multiple species calling at the same time) used in monitoring applications. This is one of the reasons why the performance of the currently used AI models has been subpar.
To unlock the full potential of these extensive and information-rich sound archives, researchers need good machine listeners to reliably extract as much information as possible to aid data-driven conservation.
The Cornell Lab of Ornithology’s Center for Conservation Bioacoustics (CCB)’s mission is to collect and interpret sounds in nature. The CCB develops innovative conservation technologies to inspire and inform the conservation of wildlife and habitats globally. By partnering with the data science community, the CCB hopes to further its mission and improve the accuracy of soundscape analyses.
In this competition, you will identify a wide variety of bird vocalizations in soundscape recordings. Due to the complexity of the recordings, they contain weak labels. There might be anthropogenic sounds (e.g., airplane overflights) or other bird and non-bird (e.g., chipmunk) calls in the background, with a particular labeled bird species in the foreground. Bring your new ideas to build effective detectors and classifiers for analyzing complex soundscape recordings!
If successful, your work will help researchers better understand changes in habitat quality, levels of pollution, and the effectiveness of restoration efforts. Reliable machine listeners would also allow conservationists to deploy more recording units worldwide and would enable data-driven conservation at a scale not yet possible. The eventual conservation outcomes could greatly improve the quality of life for many living organisms—birds and human beings included.",21382 files,25.35 GB,This competition awarded ranking points,This competition counted towards tiers,Research Code Competition
Halite by Two Sigma,,2 years ago,,"['video games', 'simulations', 'custom metric']","1,139","1,291","15,037","Ahoy there! There's halite to be had and ships to be deployed! Are you ready to navigate the skies and secure your territory?
Halite by Two Sigma (""Halite"") is a resource management game where you build and control a small armada of ships. Your algorithms determine their movements to collect halite, a luminous energy source. The most halite at the end of the match wins, but it's up to you to figure out how to make effective and efficient moves. You control your fleet, build new ships, create shipyards, and mine the regenerating halite on the game board.
Created by Two Sigma in 2016, more than 15,000 people around the world have participated in a Halite challenge. Players apply advanced algorithms in a dynamic, open source game setting. The strategic depth and immersive, interactive nature of Halite games make each challenge a unique learning environment.
Halite IV builds on the core game design of Halite III with a number of key changes that shift the focus of the game towards tighter competition on a smaller board. New game features include regenerating halite, shipyard creation, no more ship movement costs, and stealing halite from other players!
So dust off your halite meters and fasten your seatbelts. The fourth season of Halite is about to begin!",,,This competition awarded ranking points,This competition counted towards tiers,Featured Simulation Competition
SIIM-ISIC Melanoma Classification,"$30,000 ",3 years ago,,"['image', 'mcauc']","3,308","4,110","101,845","Skin cancer is the most prevalent type of cancer. Melanoma, specifically, is responsible for 75% of skin cancer deaths, despite being the least common skin cancer. The American Cancer Society estimates over 100,000 new melanoma cases will be diagnosed in 2020. It's also expected that almost 7,000 people will die from the disease. As with other cancers, early and accurate detection—potentially aided by data science—can make treatment more effective.
Currently, dermatologists evaluate every one of a patient's moles to identify outlier lesions or “ugly ducklings” that are most likely to be melanoma. Existing AI approaches have not adequately considered this clinical frame of reference. Dermatologists could enhance their diagnostic accuracy if detection algorithms take into account “contextual” images within the same patient to determine which images represent a melanoma. If successful, classifiers would be more accurate and could better support dermatological clinic work.
As the leading healthcare organization for informatics in medical imaging, the Society for Imaging Informatics in Medicine (SIIM)'s mission is to advance medical imaging informatics through education, research, and innovation in a multi-disciplinary community. SIIM is joined by the International Skin Imaging Collaboration (ISIC), an international effort to improve melanoma diagnosis. The ISIC Archive contains the largest publicly available collection of quality-controlled dermoscopic images of skin lesions.
In this competition, you’ll identify melanoma in images of skin lesions. In particular, you’ll use images within the same patient and determine which are likely to represent a melanoma. Using patient-level contextual information may help the development of image analysis tools, which could better support clinical dermatologists.
Melanoma is a deadly disease, but if caught early, most melanomas can be cured with minor surgery. Image analysis tools that automate the diagnosis of melanoma will improve dermatologists' diagnostic accuracy. Better detection of melanoma has the opportunity to positively impact millions of people.",88251 files,116.16 GB,This competition awarded ranking points,This competition counted towards tiers,Featured Prediction Competition
TREC-COVID Information Retrieval,,3 years ago,3 years ago,"['nlp', 'text', 'covid19', 'text mining', 'ndcg@{k}']",19,19,66,"LAUNCHED
This competition was launched and opened for submissions on May 27th 2020. Submissions will close in 1 week at 11:00 AM UTC on June 3rd 2020. The public leaderboard is based on the TREC-COVID Round 2 dataset. The private leaderboard will be based on the Round 3 dataset, which will be evaluated after the competition closes. Review the Data page for more details.
Researchers, clinicians, and policy makers involved with the response to COVID-19 are constantly searching for reliable information on the virus and its impact. This presents a unique opportunity for the information retrieval (IR) and text processing communities to contribute to the response to this pandemic, as well as to study methods for quickly standing up information systems for similar future events. The results of the TREC-COVID Challenge will identify answers for some of today's questions while building infrastructure to improve tomorrow's search systems.
Kaggle first teamed up with the Allen Institute for AI in the launch of the COVID-19 Open Research Dataset (CORD-19). TREC-COVID builds on the CORD-19 Challenge by using the same document set, a collection of biomedical literature articles that has been updated on a weekly rolling basis.
This is the 3rd Round of the TREC-COVID Challenge. Prior runs were hosted directly on the TREC-COVID Site. For this round, you have the option to submit on Kaggle or directly to the TREC-COVID platform. The organizers have added 5 additional COVID-related topics to the 35 topics from the first two rounds, for a total of 40 topics. You will create a retrieval system that returns ranked lists of documents from CORD-19 for (a) each of these additional Round 3 topics (""runs"") and as well as (b) residual rankings on the completed Round 1 & 2 topics, i.e., for any documents not judged in the CORD-19 dataset (not previously included as a ranked document). The eligible population of documents for Round 3 is anything included in the CORD-19 release up to Round 3's launch date, last updated on May 19th 2020.
Following the close of Round 3, NIST will gather the collective set of participants' runs, to include those participants submitting directly through TREC-COVID. The organizers will then assess some reasonable subset of these submissions for relevance by human annotators with biomedical expertise. The results of the human annotation, known as relevance judgments, will then be used to score the submitted runs. It is important to understand that not all documents will be assessed, and thus the private leaderboard score will be based on partial document assessment.
With your help, the final document and topic sets together with the cumulative relevance judgments will comprise a COVID test collection. The incremental nature of the collection will support research on search systems for dynamic environments.
Acknowledgments
The Text REtrieval Conference (TREC) was founded in 1992 to support research within the information retrieval community by providing the infrastructure necessary for large-scale evaluation of text retrieval methodologies.
The TREC-COVID Challenge is being organized by the Allen Institute for Artificial Intelligence (AI2), the National Institute of Standards and Technology (NIST), the National Library of Medicine (NLM), Oregon Health and Science University (OHSU), and the University of Texas Health Science Center at Houston (UTHealth).
See the NIST press release for more information.",103321 files,12.73 GB,This competition did not award ranking points,This competition did not count towards tiers,Research Prediction Competition
Open Images Instance Segmentation RVC 2020 edition,,3 years ago,2 years ago,"['image', 'custom metric']",18,26,27,"Introduction
Computer vision has advanced considerably but is still challenged in matching the precision of human perception.
Open Images is a collaborative release of ~9 million images annotated with image-level labels, object bounding boxes, object segmentation masks, visual relationships, and localized narratives. This uniquely large and diverse dataset is designed to spur state of the art advances in analyzing and understanding images.
This year the Open Images Instance Segmentation competition is a part of the larger Robust Vision Challenge 2020. This challenge encourages the participants to develop robust computer vision algorithms able to perform well across multiple datasets. Please refer to the RVC 2020 page and the Open Images Challenge page for more details.
Participants are also welcome to submit to this playground competition beyond the context of RVC.
Instance Segmentation Track
In this track of the Challenge, you are asked to provide segmentation masks of objects.
This track’s training set represents 2.1M segmentation masks for object instances in 300 categories; with a validation set containing an additional 23k masks. The train set masks were produced by our state-of-the-art interactive segmentation process, where professional human annotators iteratively correct the output of a segmentation neural network. The validation and test set masks have been annotated manually with a strong focus on quality.
Example train set annotations. Left: Wuxi science park, 1995 by Gary Stevens. Right: Cat Cafe Shinjuku calico by Ari Helminen. Both images used under CC BY 2.0 license.

The training data, format, and submission modalities are identical to the 2019 Open Images Challenge.",100001 files,10.46 GB,This competition did not award ranking points,This competition did not count towards tiers,Playground Prediction Competition
Open Images Object Detection RVC 2020 edition,,3 years ago,2 years ago,"['image', 'custom metric']",89,107,224,"Introduction
Computer vision has advanced considerably but is still challenged in matching the precision of human perception.
Open Images is a collaborative release of ~9 million images annotated with image-level labels, object bounding boxes, object segmentation masks, visual relationships, and localized narratives. This uniquely large and diverse dataset is designed to spur state of the art advances in analyzing and understanding images.
This year the Open Images Object Detection competition is a part of the larger Robust Vision Challenge 2020. This challenge encourages the participants to develop robust computer vision algorithms able to perform well across multiple datasets. Please refer to the RVC 2020 page and the Open Images Challenge page for more details.
Participants are also welcome to submit to this playground competition beyond the context of RVC.
Object Detection Track
In this track, you are asked to predict a tight bounding box around object instances.
The training set contains 12.2M bounding-boxes across 500 categories on 1.7M images. The boxes have been largely manually drawn by professional annotators to ensure accuracy and consistency. The images are very diverse and often contain complex scenes with several objects (7 per image on average).
Example annotations. Left: Mark Paul Gosselaar plays the guitar by Rhys A. Right: the house by anita kluska. Both images used under CC BY 2.0 license.
The training data, format, and submission modalities are identical to the 2019 Open Images Challenge.",100000 files,10.46 GB,This competition did not award ranking points,This competition did not count towards tiers,Playground Prediction Competition
COVID19 Global Forecasting (Week 5),,3 years ago,3 years ago,"['tabular', 'covid19', 'weightedpinballloss']",173,688,,"This is week 5 of Kaggle's COVID-19 forecasting series, following the Week 4 competition. This competition has some changes from prior weeks - be sure to check the Evaluation and Data pages for more details. All of the prior discussion forums have been migrated to this competition for continuity.
Background
The White House Office of Science and Technology Policy (OSTP) pulled together a coalition research groups and companies (including Kaggle) to prepare the COVID-19 Open Research Dataset (CORD-19) to attempt to address key open scientific questions on COVID-19. Those questions are drawn from National Academies of Sciences, Engineering, and Medicine’s (NASEM) and the World Health Organization (WHO).
The Challenge
Kaggle is launching a companion COVID-19 forecasting challenges to help answer a subset of the NASEM/WHO questions. While the challenge involves developing quantile estimates intervals for confirmed cases and fatalities between May 12 and June 7 by region, the primary goal isn't only to produce accurate forecasts. It’s also to identify factors that appear to impact the transmission rate of COVID-19.
You are encouraged to pull in, curate and share data sources that might be helpful. If you find variables that look like they impact the transmission rate, please share your finding in a notebook.
As the data becomes available, we will update the leaderboard with live results based on data made available from the Johns Hopkins University Center for Systems Science and Engineering (JHU CSSE).
We have received support and guidance from health and policy organizations in launching these challenges. We're hopeful the Kaggle community can make valuable contributions to developing a better understanding of factors that impact the transmission of COVID-19.
Companies and Organizations
There is also a call to action for companies and other organizations: If you have datasets that might be useful, please upload them to Kaggle’s dataset platform and reference them in this forum thread. That will make them accessible to those participating in this challenge and a resource to the wider scientific community.
Acknowledgements
JHU CSSE for making the data available to the public. The White House OSTP for pulling together the key open questions. The image comes from the Center for Disease Control.
This is a Code Competition. Refer to Code Requirements for details.",3 files,111.69 MB,This competition awarded ranking points,This competition did not count towards tiers,Research Code Competition
Global Wheat Detection,"$15,000 ",3 years ago,,"['image', 'plants', 'custom metric']","2,245","8,187",,"Open up your pantry and you’re likely to find several wheat products. Indeed, your morning toast or cereal may rely upon this common grain. Its popularity as a food and crop makes wheat widely studied. To get large and accurate data about wheat fields worldwide, plant scientists use image detection of ""wheat heads""—spikes atop the plant containing grain. These images are used to estimate the density and size of wheat heads in different varieties. Farmers can use the data to assess health and maturity when making management decisions in their fields.
However, accurate wheat head detection in outdoor field images can be visually challenging. There is often overlap of dense wheat plants, and the wind can blur the photographs. Both make it difficult to identify single heads. Additionally, appearances vary due to maturity, color, genotype, and head orientation. Finally, because wheat is grown worldwide, different varieties, planting densities, patterns, and field conditions must be considered. Models developed for wheat phenotyping need to generalize between different growing environments. Current detection methods involve one- and two-stage detectors (Yolo-V3 and Faster-RCNN), but even when trained with a large dataset, a bias to the training region remains.
The Global Wheat Head Dataset is led by nine research institutes from seven countries: the University of Tokyo, Institut national de recherche pour l’agriculture, l’alimentation et l’environnement, Arvalis, ETHZ, University of Saskatchewan, University of Queensland, Nanjing Agricultural University, and Rothamsted Research. These institutions are joined by many in their pursuit of accurate wheat head detection, including the Global Institute for Food Security, DigitAg, Kubota, and Hiphen.
In this competition, you’ll detect wheat heads from outdoor images of wheat plants, including wheat datasets from around the globe. Using worldwide data, you will focus on a generalized solution to estimate the number and size of wheat heads. To better gauge the performance for unseen genotypes, environments, and observational conditions, the training dataset covers multiple regions. You will use more than 3,000 images from Europe (France, UK, Switzerland) and North America (Canada). The test data includes about 1,000 images from Australia, Japan, and China.
Wheat is a staple across the globe, which is why this competition must account for different growing conditions. Models developed for wheat phenotyping need to be able to generalize between environments. If successful, researchers can accurately estimate the density and size of wheat heads in different varieties. With improved detection farmers can better assess their crops, ultimately bringing cereal, toast, and other favorite dishes to your table.
This is a Code Competition. Refer to Code Requirements for details.",3434 files,643.57 MB,This competition awarded ranking points,This competition counted towards tiers,Research Code Competition
ALASKA2 Image Steganalysis,"$25,000 ",3 years ago,,['custom metric'],"1,095","1,322","20,918","That file you downloaded may contain hidden messages that aren’t part of its regular contents. The same technology employed for digital watermarking is also misused by crime rings. Law enforcement must now use steganalysis to detect these messages as part of their investigations. Machine learning is an important tool in the discovery of this secret data.
Current methods produce unreliable results, raising false alarms. One reason for inaccuracy is the many different devices and processing combinations. Yet, detection models are trained on a homogeneous dataset. To increase accuracy, researchers must put data hidden within digital images “into the wild” (hence the name ALASKA) to mimic real world conditions.
In the competition, you’ll create an efficient and reliable method to detect secret data hidden within innocuous-seeming digital images. Rather than limiting the data source, these images have been acquired with as many as 50 different cameras (from smartphone to full-format high end) and processed in different fashions. Successful entries will include robust detection algorithms with minimal false positives.
The IEEE WIFS (Workshop on Information Forensics and Security) is eager to make this happen again, as a follow up to the ALASKA#1 Challenge. WIFS is an annual event where researchers gather to discuss emerging challenges, exchange fresh ideas, and share state-of-the-art results and technical expertise in the areas of information security and forensics. WIFS has teamed up with Troyes University of Technology, CRIStAL Lab, Lille University, and CNRS to enable more accurate steganalysis.
Law enforcement officers need better methods to combat criminals using hidden messages. The data science community and other researchers can help with better automated detection. More accurate methods could help catch criminals whose communications are hidden in plain sight.
The challenge is organized by Rémi COGRANNE (UTT), Patrick BAS (CRIStAL / CNRS) and Quentin Giboulot (UTT) ; in addition to Kaggle, we have been greatly helped by the following sponsors:",305001 files,32.27 GB,This competition awarded ranking points,This competition counted towards tiers,Research Prediction Competition
Hash Code Archive - Photo Slideshow Optimization,,3 years ago,2 years ago,"['internet', 'optimization', 'custom metric']",89,110,405,"Note: Put your heads together to solve programming challenges. Google's coding competition, Hash Code, has just finished for 2020. Use this online qualifier from 2019 to keep your skills sharp for future competitions!
As the saying goes, ""a picture is worth a thousand words."" We agree – photos are an important part of contemporary digital and cultural life. How we experience photos largely depends on the story they’re arranged to tell. The same shots could be a monotonous series of snaps or form a narrative masterpiece.
Approximately 2.5 billion people around the world carry a camera – in the form of a smartphone – in their pocket every day. We tend to make good use of it, too, taking more photos than ever (back in 2017, Google Photos announced it was backing up more than 1.2 billion photos and videos per day)! The rise of digital photography creates an interesting challenge: what should we do with all of these photos? In this competition, you will compose a slideshow out of a photo collection.
Given a list of photos and the tags associated with each photo, you are challenged to arrange the photos into a slideshow that is as interesting as possible (the evaluation section explains what we mean by “interesting”)
Will your slideshow tell a good story or be a major snoozefest?",3 files,4.11 MB,This competition did not award ranking points,This competition did not count towards tiers,Playground Code Competition
TReNDS Neuroimaging,"$25,000 ",3 years ago,,"['computer vision', 'neuroscience', 'wmae']","1,047","1,214","14,309","Human brain research is among the most complex areas of study for scientists. We know that age and other factors can affect its function and structure, but more research is needed into what specifically occurs within the brain. With much of the research using MRI scans, data scientists are well positioned to support future insights. In particular, neuroimaging specialists look for measurable markers of behavior, health, or disorder to help identify relevant brain regions and their contribution to typical or symptomatic effects.
In this competition, you will predict multiple assessments plus age from multimodal brain MRI features. You will be working from existing results from other data scientists, doing the important work of validating the utility of multimodal features in a normative population of unaffected subjects. Due to the complexity of the brain and differences between scanners, generalized approaches will be essential to effectively propel multimodal neuroimaging research forward.
The Tri-Institutional Georgia State University/Georgia Institute of Technology/Emory University Center for Translational Research in Neuroimaging and Data Science (TReNDS) leverages advanced brain imaging to promote research into brain health. The organization is focused on developing, applying and sharing advanced analytic approaches and neuroinformatics tools. Among its software projects are the GIFT and FIT neuroimaging toolboxes, the COINS data management system, and the COINSTAC toolkit for federated learning, all aimed at supporting data scientists and other neuroimaging researchers.
Making the leap from research to clinical application is particularly difficult in brain health. In order to translate to clinical settings, research findings have to be reproduced consistently and validated in out-of-sample instances. The problem is particularly well-suited for data science, but current approaches typically do not generalize well. With this large dataset and competition, your efforts could directly address an important area of brain research.
Acknowledgments",1 files,652.36 kB,This competition awarded ranking points,This competition counted towards tiers,Research Prediction Competition
Prostate cANcer graDe Assessment (PANDA) Challenge,"$25,000 ",3 years ago,,"['image', 'medicine', 'quadraticweightedkappa']","1,010","1,290","19,723","With more than 1 million new diagnoses reported every year, prostate cancer (PCa) is the second most common cancer among males worldwide that results in more than 350,000 deaths annually. The key to decreasing mortality is developing more precise diagnostics. Diagnosis of PCa is based on the grading of prostate tissue biopsies. These tissue samples are examined by a pathologist and scored according to the Gleason grading system. In this challenge, you will develop models for detecting PCa on images of prostate tissue samples, and estimate severity of the disease using the most extensive multi-center dataset on Gleason grading yet available.
The grading process consists of finding and classifying cancer tissue into so-called Gleason patterns (3, 4, or 5) based on the architectural growth patterns of the tumor (Fig. 1). After the biopsy is assigned a Gleason score, it is converted into an ISUP grade on a 1-5 scale. The Gleason grading system is the most important prognostic marker for PCa, and the ISUP grade has a crucial role when deciding how a patient should be treated. There is both a risk of missing cancers and a large risk of overgrading resulting in unnecessary treatment. However, the system suffers from significant inter-observer variability between pathologists, limiting its usefulness for individual patients. This variability in ratings could lead to unnecessary treatment, or worse, missing a severe diagnosis.
Automated deep learning systems have shown some promise in accurately grading PCa. Recent research, including two studies independently conducted by the groups hosting this challenge, have shown that these systems can achieve pathologist-level performance. However, these systems/results were not tested with multi-center datasets at scale.
Your work here will improve on these efforts using the most extensive multi-center dataset on Gleason grading yet. The training set consists of around 11,000 whole-slide images of digitized H&E-stained biopsies originating from two centers. This is the largest public whole-slide image dataset available, roughly 8 times the size of the CAMELYON17 challenge, one of the largest digital pathology datasets and best known challenges in the field. Furthermore, in contrast to previous challenges, we are making full diagnostic biopsy images available. Using a sizable multi-center test set, graded by expert uro-pathologists, we will evaluate challenge submissions on their applicability to improve this critical diagnostic function.
Figure 1: An illustration of the Gleason grading process for an example biopsy containing prostate cancer. The most common (blue outline, Gleason pattern 3) and second most common (red outline, Gleason pattern 4) cancer growth patterns present in the biopsy dictate the Gleason score (3+4 for this biopsy), which in turn is converted into an ISUP grade (2 for this biopsy) following guidelines of the International Society of Urological Pathology. Biopsies not containing cancer are represented by an ISUP grade of 0 in this challenge.
Radboud University Medical Center and Karolinska Institute have teamed up to organize this competition in collaboration with colleagues from Tampere University. The Computational Pathology Group (CPG) of the Radboud University Medical Center is a research group that develops computer algorithms to aid clinicians. Karolinska Institute’s Department of Medical Epidemiology and Biostatistics (MEB) includes an interdisciplinary research group to improve the diagnostics and treatment of prostate cancer. Together, they hope to further their existing research to make a significant impact on the healthcare of prostate cancer patients.
Challenge organizer team: Wouter Bulten, Geert Litjens, Hans Pinckaers, Peter Ström, Martin Eklund, Lars Egevad, Henrik Grönberg, Kimmo Kartasalo, Pekka Ruusuvuori, Tomi Häkkinen, Sohier Dane, Maggie Demkin.
Sponsors
The PANDA workshop at MICCAI 2020 is sponsored by ContextVision, Ibex and Google.
Published results
The paper on the PANDA challenge has been published as Open Access in Nature Medicine. In the paper, we took a deep dive into the solutions, tested the methods to see if they generalize well to unseen data, and performed a comparison with pathologists. You can read the full paper and all results here:
https://www.nature.com/articles/s41591-021-01620-2
Bulten, W., Kartasalo, K., Chen, PH.C. et al. Artificial intelligence for diagnosis and Gleason grading of prostate cancer: the PANDA challenge. Nat Med (2022). https://doi.org/10.1038/s41591-021-01620-2
Using the data outside of the competition
With the paper's publication, the embargo on the data is now lifted (see forum post). If you want, you can now use the dataset for further scientific work and publish your results on the dataset. If you do so, please take the license (CC BY-SA-NC 4.0) into account (non-commercial) and make sure you cite the PANDA paper. The test sets will not be made public at this time, to allow further late submissions to be used for benchmarking algorithms. We are looking forward to seeing new scientific projects coming out of this dataset!",21135 files,411.9 GB,This competition awarded ranking points,This competition counted towards tiers,Featured Code Competition
COVID19 Global Forecasting (Week 4),,3 years ago,3 years ago,"['tabular', 'covid19', 'mcrmsle']",472,"1,925",,"This is week 4 of Kaggle's COVID-19 forecasting series, following the Week 3 competition. This is the 4th competition we've launched in this series. All of the prior discussion forums have been migrated to this competition for continuity.
Background
The White House Office of Science and Technology Policy (OSTP) pulled together a coalition research groups and companies (including Kaggle) to prepare the COVID-19 Open Research Dataset (CORD-19) to attempt to address key open scientific questions on COVID-19. Those questions are drawn from National Academies of Sciences, Engineering, and Medicine’s (NASEM) and the World Health Organization (WHO).
The Challenge
Kaggle is launching a companion COVID-19 forecasting challenges to help answer a subset of the NASEM/WHO questions. While the challenge involves forecasting confirmed cases and fatalities between April 15 and May 14 by region, the primary goal isn't only to produce accurate forecasts. It’s also to identify factors that appear to impact the transmission rate of COVID-19.
You are encouraged to pull in, curate and share data sources that might be helpful. If you find variables that look like they impact the transmission rate, please share your finding in a notebook.
As the data becomes available, we will update the leaderboard with live results based on data made available from the Johns Hopkins University Center for Systems Science and Engineering (JHU CSSE).
We have received support and guidance from health and policy organizations in launching these challenges. We're hopeful the Kaggle community can make valuable contributions to developing a better understanding of factors that impact the transmission of COVID-19.
Companies and Organizations
There is also a call to action for companies and other organizations: If you have datasets that might be useful, please upload them to Kaggle’s dataset platform and reference them in this forum thread. That will make them accessible to those participating in this challenge and a resource to the wider scientific community.
Acknowledgements
JHU CSSE for making the data available to the public. The White House OSTP for pulling together the key open questions. The image comes from the Center for Disease Control.
This is a Code Competition. Refer to Code Requirements for details.",3 files,1.95 MB,This competition awarded ranking points,This competition did not count towards tiers,Research Code Competition
COVID19 Global Forecasting (Week 3),,3 years ago,3 years ago,"['tabular', 'covid19', 'mcrmsle']",452,"1,408",,"This week 3 forecasting task is now closed for submissions. Click here to visit the week 4 version, and make a submission there.
This is week 3 of Kaggle's COVID19 forecasting series, following the Week 2 competition. This is the 3rd of at least 4 competitions we plan to launch in this series. All of the prior discussion forums have been migrated to this competition for continuity.
Background
The White House Office of Science and Technology Policy (OSTP) pulled together a coalition research groups and companies (including Kaggle) to prepare the COVID-19 Open Research Dataset (CORD-19) to attempt to address key open scientific questions on COVID-19. Those questions are drawn from National Academies of Sciences, Engineering, and Medicine’s (NASEM) and the World Health Organization (WHO).
The Challenge
Kaggle is launching a companion COVID-19 forecasting challenges to help answer a subset of the NASEM/WHO questions. While the challenge involves forecasting confirmed cases and fatalities between April 1 and April 30 by region, the primary goal isn't only to produce accurate forecasts. It’s also to identify factors that appear to impact the transmission rate of COVID-19.
You are encouraged to pull in, curate and share data sources that might be helpful. If you find variables that look like they impact the transmission rate, please share your finding in a notebook.
As the data becomes available, we will update the leaderboard with live results based on data made available from the Johns Hopkins University Center for Systems Science and Engineering (JHU CSSE).
We have received support and guidance from health and policy organizations in launching these challenges. We're hopeful the Kaggle community can make valuable contributions to developing a better understanding of factors that impact the transmission of COVID-19.
Companies and Organizations
There is also a call to action for companies and other organizations: If you have datasets that might be useful, please upload them to Kaggle’s dataset platform and reference them in this forum thread. That will make them accessible to those participating in this challenge and a resource to the wider scientific community.
Acknowledgements
JHU CSSE for making the data available to the public. The White House OSTP for pulling together the key open questions. The image comes from the Center for Disease Control.
This is a Code Competition. Refer to Code Requirements for details.",3 files,1.41 MB,This competition awarded 0.5X ranking points,This competition did not count towards tiers,Research Code Competition
iMet Collection 2020 - FGVC7,,3 years ago,,['fscorebetamicro'],96,123,980,"The Metropolitan Museum of Art in New York, also known as The Met, has a diverse collection of over 1.5M objects of which over 200K have been digitized with imagery. Can you help find the significant attributes to identify a specific work of art? Help advance this research in this notebook competition.
The online cataloguing information is generated by subject matter experts and includes a wide range of data. These include, but are not limited to: multiple object classifications, artist, title, period, date, medium, culture, size, provenance, geographic location, and other related museum objects within The Met’s collection. While the annotations describe the object from an art history perspective, they can also be indirect in describing finer-grained attributes for the museum-goer’s understanding. Adding fine-grained attributes to aid in the visual understanding of the museum objects will enable the ability to search for visually related objects.
This is a Code Competition. Refer to Code Requirements for details.
About
This is an FGVCx competition hosted as part of the FGVC7 workshop at CVPR 2020.",168080 files,29.46 GB,This competition did not award ranking points,This competition did not count towards tiers,Research Code Competition
COVID19 Global Forecasting (Week 2),,3 years ago,3 years ago,"['tabular', 'covid19', 'mcrmsle']",215,263,"1,368","This week 2 forecasting task is now closed for submissions. Click here to visit the week 3 version, and make a submission there.
This is week 2 of Kaggle's COVID19 forecasting series, following the Week 1 competition. This is the 2nd of at least 4 competitions we plan to launch in this series.
Background
The White House Office of Science and Technology Policy (OSTP) pulled together a coalition research groups and companies (including Kaggle) to prepare the COVID-19 Open Research Dataset (CORD-19) to attempt to address key open scientific questions on COVID-19. Those questions are drawn from National Academies of Sciences, Engineering, and Medicine’s (NASEM) and the World Health Organization (WHO).
The Challenge
Kaggle is launching a companion COVID-19 forecasting challenges to help answer a subset of the NASEM/WHO questions. While the challenge involves forecasting confirmed cases and fatalities between April 1 and April 30 by region, the primary goal isn't only to produce accurate forecasts. It’s also to identify factors that appear to impact the transmission rate of COVID-19.
You are encouraged to pull in, curate and share data sources that might be helpful. If you find variables that look like they impact the transmission rate, please share your finding in a notebook.
As the data becomes available, we will update the leaderboard with live results based on data made available from the Johns Hopkins University Center for Systems Science and Engineering (JHU CSSE).
We have received support and guidance from health and policy organizations in launching these challenges. We're hopeful the Kaggle community can make valuable contributions to developing a better understanding of factors that impact the transmission of COVID-19.
Companies and Organizations
There is also a call to action for companies and other organizations: If you have datasets that might be useful, please upload them to Kaggle’s dataset platform and reference them in this forum thread. That will make them accessible to those participating in this challenge and a resource to the wider scientific community.
Acknowledgements
JHU CSSE for making the data available to the public. The White House OSTP for pulling together the key open questions. The image comes from the Center for Disease Control.
This is a Code Competition. Refer to Code Requirements for details.",3 files,1.26 MB,This competition awarded 0.5X ranking points,This competition did not count towards tiers,Research Code Competition
Tweet Sentiment Extraction,"$15,000 ",3 years ago,,"['internet', 'text', 'custom metric']","2,225","2,817","37,917","""My ridiculous dog is amazing."" [sentiment: positive]
With all of the tweets circulating every second it is hard to tell whether the sentiment behind a specific tweet will impact a company, or a person's, brand for being viral (positive), or devastate profit because it strikes a negative tone. Capturing sentiment in language is important in these times where decisions and reactions are created and updated in seconds. But, which words actually lead to the sentiment description? In this competition you will need to pick out the part of the tweet (word or phrase) that reflects the sentiment.
Help build your skills in this important area with this broad dataset of tweets. Work on your technique to grab a top spot in this competition. What words in tweets support a positive, negative, or neutral sentiment? How can you help make that determination using machine learning tools?
In this competition we've extracted support phrases from Figure Eight's Data for Everyone platform. The dataset is titled Sentiment Analysis: Emotion in Text tweets with existing sentiment labels, used here under creative commons attribution 4.0. international licence. Your objective in this competition is to construct a model that can do the same - look at the labeled sentiment for a given tweet and figure out what word or phrase best supports it.
Disclaimer: The dataset for this competition contains text that may be considered profane, vulgar, or offensive.",3 files,3.86 MB,This competition awarded ranking points,This competition counted towards tiers,Featured Code Competition
Jigsaw Multilingual Toxic Comment Classification,"$50,000 ",3 years ago,,"['tpu', 'text', 'languages', 'auc']","1,621","1,922","36,072","It only takes one toxic comment to sour an online discussion. The Conversation AI team, a research initiative founded by Jigsaw and Google, builds technology to protect voices in conversation. A main area of focus is machine learning models that can identify toxicity in online conversations, where toxicity is defined as anything rude, disrespectful or otherwise likely to make someone leave a discussion. If these toxic contributions can be identified, we could have a safer, more collaborative internet.
In the previous 2018 Toxic Comment Classification Challenge, Kagglers built multi-headed models to recognize toxicity and several subtypes of toxicity. In 2019, in the Unintended Bias in Toxicity Classification Challenge, you worked to build toxicity models that operate fairly across a diverse range of conversations. This year, we're taking advantage of Kaggle's new TPU support and challenging you to build multilingual models with English-only training data.
Jigsaw's API, Perspective, serves toxicity models and others in a growing set of languages (see our documentation for the full list). Over the past year, the field has seen impressive multilingual capabilities from the latest model innovations, including few- and zero-shot learning. We're excited to learn whether these results ""translate"" (pun intended!) to toxicity classification. Your training data will be the English data provided for our previous two competitions and your test data will be Wikipedia talk page comments in several different languages.
As our computing resources and modeling capabilities grow, so does our potential to support healthy conversations across the globe. Develop strategies to build effective multilingual models and you'll help Conversation AI and the entire industry realize that potential.
Disclaimer: The dataset for this competition contains text that may be considered profane, vulgar, or offensive.
To get started with TPUs:
Read the TPU documentation one-pager
Then jump right into the Getting Started Notebooks for this competition
Quick note: a TPU is a network-connected accelerator and requires a couple extra lines in your code. Flipping the TPU switch in your notebook will not, by itself, accelerate your code.",10 files,5.03 GB,This competition awarded ranking points,This competition counted towards tiers,Featured Code Competition
COVID19 Global Forecasting (Week 1),,3 years ago,3 years ago,"['tabular', 'covid19', 'mcrmsle']",545,640,"2,913","This week 1 forecasting task is now closed for submissions. Click here to visit the week 2 version, and make a submission there.
This is one of the two complementary forecasting tasks to predict COVID-19 spread. This task is based on various regions across the world. To start on a single state-level subcomponent, please see the companion forecasting task for California, USA.
Background
The White House Office of Science and Technology Policy (OSTP) pulled together a coalition research groups and companies (including Kaggle) to prepare the COVID-19 Open Research Dataset (CORD-19) to attempt to address key open scientific questions on COVID-19. Those questions are drawn from National Academies of Sciences, Engineering, and Medicine’s (NASEM) and the World Health Organization (WHO).
The Challenge
Kaggle is launching two companion COVID-19 forecasting challenges to help answer a subset of the NASEM/WHO questions. While the challenge involves forecasting confirmed cases and fatalities between March 25 and April 22 by region, the primary goal isn't to produce accurate forecasts. It’s to identify factors that appear to impact the transmission rate of COVID-19.
You are encouraged to pull in, curate and share data sources that might be helpful. If you find variables that look like they impact the transmission rate, please share your finding in a notebook.
As the data becomes available, we will update the leaderboard with live results based on data made available from the Johns Hopkins University Center for Systems Science and Engineering (JHU CSSE).
We have received support and guidance from health and policy organizations in launching these challenges. We're hopeful the Kaggle community can make valuable contributions to developing a better understanding of factors that impact the transmission of COVID-19.
Companies and Organizations
There is also a call to action for companies and other organizations: If you have datasets that might be useful, please upload them to Kaggle’s dataset platform and reference them in this forum thread. That will make them accessible to those participating in this challenge and a resource to the wider scientific community.
Acknowledgements
JHU CSSE for making the data available to the public. The White House OSTP for pulling together the key open questions. The image comes from the Center for Disease Control.
This is a Code Competition. Refer to Code Requirements for details.",3 files,1.63 MB,This competition did not award ranking points,This competition did not count towards tiers,Research Code Competition
COVID19 Local US-CA Forecasting (Week 1),,3 years ago,3 years ago,"['tabular', 'covid19', 'mcrmsle']",190,216,896,"This is one of the two complementary forecasting tasks to predict COVID-19 spread. This one is based on a single state-level subcomponent in California, USA. Our intent in having this region-specific version is to offer a more manageable starting point for the global forecasting task. To start on the global version, please see the companion forecasting task.
Background
The White House Office of Science and Technology Policy (OSTP) pulled together a coalition research groups and companies (including Kaggle) to prepare the COVID-19 Open Research Dataset (CORD-19) to attempt to address key open scientific questions on COVID-19. Those questions are drawn from National Academies of Sciences, Engineering, and Medicine’s (NASEM) and the World Health Organization (WHO).
The Challenge
Kaggle is launching two companion COVID-19 forecasting challenges to help answer a subset of the NASEM/WHO questions. While the challenge involves forecasting confirmed cases and fatalities between March 25 and April 22 in California, the primary goal isn't to produce accurate forecasts. It’s to identify factors that appear to impact the transmission rate of COVID-19.
You are encouraged to pull in, curate and share data sources that might be helpful. If you find variables that look like they impact the transmission rate, please share your finding in a notebook.
As the data becomes available, we will update the leaderboard with live results based on data made available from the Johns Hopkins University Center for Systems Science and Engineering (JHU CSSE).
We have received support and guidance from health and policy organizations in launching these challenges. We're hopeful the Kaggle community can make valuable contributions to developing a better understanding of factors that impact the transmission of COVID-19.
Companies and Organizations
There is also a call to action for companies and other organizations: If you have datasets that might be useful, please upload them to Kaggle’s dataset platform and reference them in this forum thread. That will make them accessible to those participating in this challenge and a resource to the wider scientific community.
Acknowledgements
JHU CSSE for making the data available to the public. The White House OSTP for pulling together the key open questions. The image comes from the Center for Disease Control.
This is a Code Competition. Refer to Code Requirements for details.",3 files,5.86 kB,This competition did not award ranking points,This competition did not count towards tiers,Research Code Competition
Plant Pathology 2020 - FGVC7,,3 years ago,,"['image', 'agriculture', 'mcauc']","1,317","1,510","21,784","Problem Statement
Misdiagnosis of the many diseases impacting agricultural crops can lead to misuse of chemicals leading to the emergence of resistant pathogen strains, increased input costs, and more outbreaks with significant economic loss and environmental impacts. Current disease diagnosis based on human scouting is time-consuming and expensive, and although computer-vision based models have the promise to increase efficiency, the great variance in symptoms due to age of infected tissues, genetic variations, and light conditions within trees decreases the accuracy of detection.
Specific Objectives
Objectives of ‘Plant Pathology Challenge’ are to train a model using images of training dataset to 1) Accurately classify a given image from testing dataset into different diseased category or a healthy leaf; 2) Accurately distinguish between many diseases, sometimes more than one on a single leaf; 3) Deal with rare classes and novel symptoms; 4) Address depth perception—angle, light, shade, physiological age of the leaf; and 5) Incorporate expert knowledge in identification, annotation, quantification, and guiding computer vision to search for relevant features during learning.
Resources
Details and background information on the dataset and Kaggle competition ‘Plant Pathology 2020 Challenge’ were published. If you use the dataset for your project, please cite the following peer-reviewed research article
Thapa, Ranjita; Zhang, Kai; Snavely, Noah; Belongie, Serge; Khan, Awais. The Plant Pathology Challenge 2020 data set to classify foliar disease of apples. Applications in Plant Sciences, 8 (9), 2020.
Acknowledgments
We acknowledge financial support from Cornell Initiative for Digital Agriculture (CIDA) and special thanks to Zach Guillian for help with data collection.
Kaggle is excited to partner with research groups to push forward the frontier of machine learning. Research competitions make use of Kaggle's platform and experience, but are largely organized by the research group's data science team. Any questions or concerns regarding the competition data, quality, or topic will be addressed by them.",3645 files,823.79 MB,This competition did not award ranking points,This competition did not count towards tiers,Research Prediction Competition
iWildCam 2020 - FGVC7,,3 years ago,,"['biology', 'multiclass classification', 'categorizationaccuracy']",121,152,"1,449","Camera Traps (or Wild Cams) enable the automatic collection of large quantities of image data. Biologists all over the world use camera traps to monitor biodiversity and population density of animal species. We have recently been making strides towards automatic species classification in camera trap images. However, as we try to expand the scope of these models we are faced with an interesting problem: how do we train models that perform well on new (unseen during training) camera trap locations? Can we leverage data from other modalities, such as citizen science data and remote sensing data?
In order to tackle this problem, we have prepared a challenge where the training data and test data are from different cameras spread across the globe. The set of species seen in each camera overlap, but are not identical. The challenge is to classify species in the test cameras correctly. To explore multimodal solutions, we allow competitors to train on the following data: (i) our camera trap training set (data provided by WCS), (ii) iNaturalist 2017-2019 data, and (iii) multispectral imagery (from Landsat 8) for each of the camera trap locations. On the competition GitHub page we provide the multispectral data, a taxonomy file mapping our classes into the iNat taxonomy, a subset of iNat data mapped into our class set, and a camera trap detection model (the MegaDetector) along with the corresponding detections.
If you use this dataset in publication, please cite:
@article{beery2020iwildcam,
    title={The iWildCam 2020 Competition Dataset},
    author={Beery, Sara and Cole, Elijah and Gjoka, Arvi},
    journal={arXiv preprint arXiv:2004.10340},
    year={2020}
}
This is an FGVCx competition as part of the FGVC7 workshop at CVPR 2020, and is sponsored by Microsoft AI for Earth and Wildlife Insights. There is a GitHub page for the competition here. Please open an issue if you have questions or problems with the dataset.
You can find the iWildCam 2018 Competition here, and the iWildCam 2019 Competition here.
Kaggle is excited to partner with research groups to push forward the frontier of machine learning. Research competitions make use of Kaggle's platform and experience, but are largely organized by the research group's data science team. Any questions or concerns regarding the competition data, quality, or topic will be addressed by them.",280857 files,118.59 GB,This competition did not award ranking points,This competition did not count towards tiers,Research Prediction Competition
Herbarium 2020 - FGVC7,,3 years ago,,"['image', 'plants', 'fscoremacro']",153,192,"1,386","The Herbarium 2020 FGVC7 Challenge is to identify vascular plant species from a large, long-tailed collection herbarium specimens provided by the New York Botanical Garden (NYBG).
The Herbarium 2020 dataset contains over 1M images representing over 32,000 plant species. This is a dataset with a long tail; there are a minimum of 3 specimens per species. However, some species are represented by more than a hundred specimens. This dataset only contains vascular land plants which includes lycophytes, ferns, gymnosperms, and flowering plants. The extinct forms of lycophytes are the major component of coal deposits, ferns are indicators of ecosystem health, gymnosperms provide major habitats for animals, and flowering plants provide all of our crops, vegetables, and fruits.
The teams with the most accurate models will be contacted, with the intention of using them on the un-named plant collections in the NYBG herbarium collection, and assessed by the NYBG plant specialists.
Background
The New York Botanical Garden (NYBG) herbarium contains more than 7.8 million plant and fungal specimens. Herbaria are a massive repository of plant diversity data. These collections not only represent a vast amount of plant diversity, but since herbarium collections include specimens dating back hundreds of years, they provide snapshots of plant diversity through time. The integrity of the plant is maintained in herbaria as a pressed, dried specimen; a specimen collected nearly two hundred years ago by Darwin looks much the same as one collected a month ago by an NYBG botanist. All specimens not only maintain their morphological features but also include collection dates and locations, and the name of the person who collected the specimen. This information, multiplied by millions of plant collections, provides the framework for understanding plant diversity on a massive scale and learning how it has changed over time.
About
This is an FGVC competition hosted as part of the FGVC7 workshop at CVPR 2020 and sponsored by NYBG.
Details of this competition are mirrored on the github page. Please post in the forum or open an issue if you have any questions or problems with the dataset.",1169042 files,63.16 GB,This competition did not award ranking points,This competition did not count towards tiers,Research Prediction Competition
M5 Forecasting - Accuracy,"$50,000 ",3 years ago,,"['time series analysis', 'custom metric']","5,558","7,022","88,741","Note: This is one of the two complementary competitions that together comprise the M5 forecasting challenge. Can you estimate, as precisely as possible, the point forecasts of the unit sales of various products sold in the USA by Walmart? If you are interested in estimating the uncertainty distribution of the realized values of the same series, be sure to check out its companion competition
How much camping gear will one store sell each month in a year? To the uninitiated, calculating sales at this level may seem as difficult as predicting the weather. Both types of forecasting rely on science and historical data. While a wrong weather forecast may result in you carrying around an umbrella on a sunny day, inaccurate business forecasts could result in actual or opportunity losses. In this competition, in addition to traditional forecasting methods you’re also challenged to use machine learning to improve forecast accuracy.
The Makridakis Open Forecasting Center (MOFC) at the University of Nicosia conducts cutting-edge forecasting research and provides business forecast training. It helps companies achieve accurate predictions, estimate the levels of uncertainty, avoiding costly mistakes, and apply best forecasting practices. The MOFC is well known for its Makridakis Competitions, the first of which ran in the 1980s.
In this competition, the fifth iteration, you will use hierarchical sales data from Walmart, the world’s largest company by revenue, to forecast daily sales for the next 28 days. The data, covers stores in three US States (California, Texas, and Wisconsin) and includes item level, department, product categories, and store details. In addition, it has explanatory variables such as price, promotions, day of the week, and special events. Together, this robust dataset can be used to improve forecasting accuracy.
If successful, your work will continue to advance the theory and practice of forecasting. The methods used can be applied in various business areas, such as setting up appropriate inventory or service levels. Through its business support and training, the MOFC will help distribute the tools and knowledge so others can achieve more accurate and better calibrated forecasts, reduce waste and be able to appreciate uncertainty and its risk implications.
Acknowledgements
Additional thanks go to other partner organizations and prize sponsors, National Technical University of Athens (NTUA), INSEAD, Google, Uber and IIF.",5 files,450.47 MB,This competition awarded ranking points,This competition counted towards tiers,Featured Prediction Competition
M5 Forecasting - Uncertainty,"$50,000 ",3 years ago,,"['time series analysis', 'custom metric']",909,"1,103","10,075","Note: This is one of the two complementary competitions that together comprise the M5 forecasting challenge. Can you estimate, as precisely as possible, the uncertainty distribution of the unit sales of various products sold in the USA by Walmart? This specific competition is the first of its kind, opening up new directions for both academic research and how uncertainty could be assessed and used in organizations. If you are interested in providing point (accuracy) forecasts for the same series, be sure to check out its companion competition.
How much camping gear will one store sell each month in a year? To the uninitiated, calculating sales at this level may seem as difficult as predicting the weather. Both types of forecasting rely on science and historical data. While a wrong weather forecast may result in you carrying around an umbrella on a sunny day, inaccurate business forecasts could result in actual or opportunity losses. In this competition, in addition to traditional forecasting methods you’re also challenged to use machine learning to improve forecast accuracy.
The Makridakis Open Forecasting Center (MOFC) at the University of Nicosia conducts cutting-edge forecasting research and provides business forecast training. It helps companies achieve accurate predictions, estimate the levels of uncertainty, avoiding costly mistakes, and apply best forecasting practices. The MOFC is well known for its Makridakis Competitions, the first of which ran in the 1980s.
In this competition, the fifth iteration, you will use hierarchical sales data from Walmart, the world’s largest company by revenue, to forecast daily sales for the next 28 days and to make uncertainty estimates for these forecasts. The data, covers stores in three US States (California, Texas, and Wisconsin) and includes item level, department, product categories, and store details. In addition, it has explanatory variables such as price, promotions, day of the week, and special events. Together, this robust dataset can be used to improve forecasting accuracy.
If successful, your work will continue to advance the theory and practice of forecasting. The methods used can be applied in various business areas, such as setting up appropriate inventory or service levels. Through its business support and training, the MOFC will help distribute the tools and knowledge so others can achieve more accurate and better calibrated forecasts, reduce waste and be able to appreciate uncertainty and its risk implications.
Acknowledgements
Additional thanks go to other partner organizations and prize sponsors, National Technical University of Athens (NTUA), INSEAD, Google, Uber and IIF.",5 files,515.48 MB,This competition awarded ranking points,This competition counted towards tiers,Featured Prediction Competition
University of Liverpool - Ion Switching,"$25,000 ",3 years ago,,"['biology', 'fscoremacro']","2,618","3,004","53,547","Think you can use your data science skills to make big predictions at a submicroscopic level?
Many diseases, including cancer, are believed to have a contributing factor in common. Ion channels are pore-forming proteins present in animals and plants. They encode learning and memory, help fight infections, enable pain signals, and stimulate muscle contraction. If scientists could better study ion channels, which may be possible with the aid of machine learning, it could have a far-reaching impact.
When ion channels open, they pass electric currents. Existing methods of detecting these state changes are slow and laborious. Humans must supervise the analysis, which imparts considerable bias, in addition to being tedious. These difficulties limit the volume of ion channel current analysis that can be used in research. Scientists hope that technology could enable rapid automatic detection of ion channel current events in raw data.
The University of Liverpool’s Institute of Ageing and Chronic Disease is working to advance ion channel research. Their team of scientists have asked for your help. In this competition, you’ll use ion channel data to better model automatic identification methods. If successful, you’ll be able to detect individual ion channel events in noisy raw signals. The data is simulated and injected with real world noise to emulate what scientists observe in laboratory experiments.
Technology to analyze electrical data in cells has not changed significantly over the past 20 years. If we better understand ion channel activity, the research could impact many areas related to cell health and migration. From human diseases to how climate change affects plants, faster detection of ion channels could greatly accelerate solutions to major world problems.
Acknowledgements:
This would not be possible without the help of the Biotechnology and Biological Sciences Research Council (BBSRC).",3 files,146.08 MB,This competition awarded ranking points,This competition counted towards tiers,Research Prediction Competition
Google Cloud & NCAA® March Madness Analytics,"$25,000 ",3 years ago,3 years ago,"['sports', 'basketball', 'logloss']",,,,"There's a reason why it's called March Madness®. Upsets happen, underdogs become ""cinderellas,"" and games that analysts expected to be blowouts become nail-biters through the final seconds. A team's competitiveness is what keeps games exciting and the tournament truly ""mad.""
In addition to the predictive modeling competitions we typically host (NCAA Men's and Women’s), we are hosting a separate competition using Kaggle Notebooks that challenges you to present an exploratory analysis of the “Madness.” Can you quantify competitiveness? Can you explain ""cinderella…ness""?
Or perhaps, can you determine what dictates the ability of a team to “stay in the game” and increase their chance to win late in the contest? This may or may not be a scalar metric. It might be a clustering of types of competitiveness and then a rating within each. Does this metric have predictive power? The interpretation is up to you.
Your challenge is to tell a data story about college basketball through a combination of both narrative text and data exploration. A “story” could be defined any number of ways, and that’s deliberate. You are to deeply explore (through data) the mania of the Men’s and Women’s NCAA College Basketball tournaments. That story can be examined in the macro (for example: How does “competitiveness” differ from the regular season to their decisions in the tournament?) or the micro (for example: Does effectively neutralizing an opponent’s star players increase their ability to “stay in the game”?).
This is an opportunity to be creative and tell the story of a community you identify with or are passionate about!",90 files,4.13 GB,,,Analytics Competition
Google Cloud & NCAA® ML Competition 2020-NCAAM,,3 years ago,3 years ago,"['tabular', 'basketball', 'logloss']",,,,"Update: this competition has been cancelled on account of the COVID-19 pandemic.
As a result of the continued collaboration between Google Cloud and the NCAA®, the seventh annual Kaggle-backed March Madness® competition is underway! Another year, another chance to anticipate the upsets, call the probabilities, and put your bracketology skills to the leaderboard test. Kagglers will join the millions of fans who attempt to forecast the outcomes of March Madness during this year's NCAA Division I Men’s and Women’s Basketball Championships. But unlike most fans, you will pick your bracket using a combination of NCAA’s historical data and your computing power, while the ground truth unfolds on national television.
In the first stage of the competition, Kagglers will rely on results of past tournaments to build and test models. We encourage you to post any useful external data as a dataset.
In the second stage, competitors will forecast outcomes of all possible matchups in the 2020 NCAA Division I Men’s and Women’s Basketball Championships. You don't need to participate in the first stage to enter the second. The first stage exists to incentivize model building and provide a means to score predictions. The real competition is forecasting the 2020 results.
As the official public cloud provider of the NCAA, Google is proud to provide a competition to help participants strengthen their knowledge of basketball, statistics, data modeling, and cloud technology. As part of its journey to the cloud, the NCAA has migrated 80+ years of historical and play-by-play data, from 90 championships and 24 sports, to Google Cloud Platform (GCP). The NCAA has tapped into decades of historical basketball data using BigQuery, Cloud Spanner, Datalab, Cloud Machine Learning and Cloud Dataflow, to power the analysis of team and player performance. The mission of the NCAA has long been about serving the needs of schools, their teams and students. Google Cloud is proud to support that mission by helping the NCAA use data and machine learning to better engage with its millions of fans, 500,000 student-athletes, and more than 19,000 teams. Game on!
This page is for the NCAA Division I Men's tournament. Check out the NCAA Division I Women's tournament here. If you want to extend your analysis then try out our Analytics Competition here",52 files,2.19 GB,,,Featured Prediction Competition
Google Cloud & NCAA® ML Competition 2020-NCAAW,,3 years ago,3 years ago,"['tabular', 'basketball', 'logloss']",,,,"Update: this competition has been cancelled on account of the COVID-19 pandemic.
As a result of the continued collaboration between Google Cloud and the NCAA®, the seventh annual Kaggle-backed March Madness® competition is underway! Another year, another chance to anticipate the upsets, call the probabilities, and put your bracketology skills to the leaderboard test. Kagglers will join the millions of fans who attempt to forecast the outcomes of March Madness during this year's NCAA Division I Men’s and Women’s Basketball Championships. But unlike most fans, you will pick your bracket using a combination of NCAA’s historical data and your computing power, while the ground truth unfolds on national television.
In the first stage of the competition, Kagglers will rely on results of past tournaments to build and test models. We encourage you to post any useful external data as a dataset.
In the second stage, competitors will forecast outcomes of all possible matchups in the 2020 NCAA Division I Men’s and Women’s Basketball Championships. You don't need to participate in the first stage to enter the second. The first stage exists to incentivize model building and provide a means to score predictions. The real competition is forecasting the 2020 results.
As the official public cloud provider of the NCAA, Google Cloud is proud to provide a competition to help participants strengthen their knowledge of basketball, statistics, data modeling, and cloud technology. As part of its journey to the cloud, the NCAA has migrated 80+ years of historical and play-by-play data, from 90 championships and 24 sports, to Google Cloud Platform (GCP). The NCAA has tapped into decades of historical basketball data using BigQuery, Cloud Spanner, Datalab, Cloud Machine Learning and Cloud Dataflow, to power the analysis of team and player performance. The mission of the NCAA has long been about serving the needs of schools, their teams and students. Google Cloud is proud to support that mission by helping the NCAA use data and machine learning to better engage with its millions of fans, 500,000 student-athletes and more than 19,000 teams. Game on!
This page is for the NCAA Division I Women's tournament. Check out the NCAA Division I Men's tournament here. If you want to extend your analysis then try out our Analytics Competition here",40 files,1.94 GB,,,Featured Prediction Competition
Abstraction and Reasoning Challenge,"$20,000 ",3 years ago,,"['artificial intelligence', 'meanbesterroratk']",913,"1,025","13,018","Can a computer learn complex, abstract tasks from just a few examples?
Current machine learning techniques are data-hungry and brittle—they can only make sense of patterns they've seen before. Using current methods, an algorithm can gain new skills by exposure to large amounts of data, but cognitive abilities that could broadly generalize to many tasks remain elusive. This makes it very challenging to create systems that can handle the variability and unpredictability of the real world, such as domestic robots or self-driving cars.
However, alternative approaches, like inductive programming, offer the potential for more human-like abstraction and reasoning. The Abstraction and Reasoning Corpus (ARC) provides a benchmark to measure AI skill-acquisition on unknown tasks, with the constraint that only a handful of demonstrations are shown to learn a complex task. It provides a glimpse of a future where AI could quickly learn to solve new problems on its own. The Kaggle Abstraction and Reasoning Challenge invites you to try your hand at bringing this future into the present!
This competition is hosted by François Chollet, creator of the Keras neural networks library. Chollet’s paper on measuring intelligence provides the context and motivation behind the ARC benchmark.
In this competition, you’ll create an AI that can solve reasoning tasks it has never seen before. Each ARC task contains 3-5 pairs of train inputs and outputs, and a test input for which you need to predict the corresponding output with the pattern learned from the train examples.
If successful, you’ll help bring computers closer to human cognition and you'll open the door to completely new AI applications!",901 files,4.44 MB,This competition awarded ranking points,This competition counted towards tiers,Research Code Competition
DS4G - Environmental Insights Explorer,"$25,000 ",3 years ago,3 years ago,"['geospatial analysis', 'environment', 'pollution']",,,,"PROJECT OVERVIEW
Develop a methodology to calculate an average historical emissions factor of electricity generated for a sub-national region, using remote sensing data and techniques.
The Environmental Insights Explorer team at Google is keen to gather insights on ways to improve calculations of global emissions factors for sub-national regions. The ultimate goal of this challenge is to test if calculations of emissions factors using remote sensing techniques are possible and on par with calculations of emissions factors from current methodologies.
PROBLEM STATEMENT
Current emissions factors methodologies are based on time-consuming data collection and may include errors derived from a lack of access to granular datasets, inability to refresh data on a frequent basis, overly general modeling assumptions, and inaccurate reporting of emissions sources like fuel consumption. This begs the question: What if there was a different way to calculate or measure emissions factors? We’re challenging the Kaggle community to see if it’s possible to use remote sensing techniques to better model emissions factors. You will develop a methodology to calculate an average historical emissions factor for electricity generation in a sub-national region.
We’ve provided an initial list of datasets covering the geographic boundary of Puerto Rico to serve as the foundation for this analysis. As an island, there are fewer confounding factors from nearby areas. Puerto Rico also offers a unique fuel mix and distinctive energy system layout that should make it easier to isolate pollution attributable to power generation in the remote sensing data.
Participants will be tasked with developing a methodology to calculate an average annual historical emissions factor for the sub-national region. Participants will also be asked to provide an explanation of the conditions that would result in a higher/lower emissions factor, as well as a recommendation for how the methodology could be applied to calculate the emissions factor of electricity for another geospatial area using similar techniques. Bonus points will be awarded for smaller time slices of the average historical emissions factors, such as one per month for the 12-month period, and additional bonus points will be awarded for participants that develop methodologies for calculating marginal emissions factors for the sub-national region.
HOW TO PARTICIPATE
To make a submission, complete the submission form. Only one submission will be judged per participant, so if you make multiple submissions we will only review the most recent entry.
To be valid, a submission must be contained in one or more notebook, and made public on or before the submission deadline. Participants are free to use any datasets in addition to the official Kaggle dataset, but those datasets must also be publicly available on either Earth Engine or Kaggle for the submission to be valid.",4768 files,2.83 GB,,,Analytics Competition
Flower Classification with TPUs,,3 years ago,,"['tpu', 'image', 'plants', 'fscoremacro']",848,939,"10,387","Tensor Processing Units (TPUs) are Now Available on Kaggle
Tensor Processing Unit (TPU) quotas are now available on Kaggle, at no cost to you!
TPUs are powerful hardware accelerators specialized in deep learning tasks. They were developed (and first used) by Google to process large image databases, such as extracting all the text from Street View. This competition is designed for you to give TPUs a try.
The latest Tensorflow release (TF 2.1) was focused on TPUs and they’re now supported both through the Keras high-level API and at a lower level, in models using a custom training loop.
We can’t wait to see how your solutions are accelerated by TPUs!
The Challenge
It’s difficult to fathom just how vast and diverse our natural world is.
There are over 5,000 species of mammals, 10,000 species of birds, 30,000 species of fish – and astonishingly, over 400,000 different types of flowers.
In this competition, you’re challenged to build a machine learning model that identifies the type of flowers in a dataset of images (for simplicity, we’re sticking to just over 100 types).
To get started with TPUs:
Read the TPU documentation one-pager
Then jump right into the Getting Started Notebook for this competition
Quick note: a TPU is a network-connected accelerator and requires a couple extra lines in your code. Flipping the TPU switch in your notebook will not, by itself, accelerate your code.
Have Questions?
Martin Görner, Google Developer Advocate and author of Tensorflow without a PhD will be actively engaged in the competition forum. If you have a question or need help troubleshooting, that’s the best place to find help.",193 files,5.15 GB,This competition did not award ranking points,This competition did not count towards tiers,Playground Code Competition
Connect X,,,,['custom metric'],211,240,561,"We’re excited to announce a beta-version of a brand-new type of ML competition called Simulations. In Simulation Competitions, you’ll compete against a set of rules, rather than against an evaluation metric. To enter, accept the rules and create a python submission file that can “play” against a computer, or another user.
The Challenge
In this game, your objective is to get a certain number of your checkers in a row horizontally, vertically, or diagonally on the game board before your opponent. When it's your turn, you “drop” one of your checkers into one of the columns at the top of the board. Then, let your opponent take their turn. This means each move may be trying to either win for you, or trying to stop your opponent from winning. The default number is four-in-a-row, but we’ll have other options to come soon.
Background History
For the past 10 years, our competitions have been mostly focused on supervised machine learning. The field has grown, and we want to continue to provide the data science community cutting-edge opportunities to challenge themselves and grow their skills.
So, what’s next? Reinforcement learning is clearly a crucial piece in the next wave of data science learning. We hope that Simulation Competitions will provide the opportunity for Kagglers to practice and hone this burgeoning skill.
How is this Competition Different?
Instead of submitting a CSV file, or a Kaggle Notebook, you will submit a Python .py file (more submission options are in development). You’ll also notice that the leaderboard is not based on how accurate your model is but rather how well you’ve performed against other users. See Evaluation for more details.
We’d Love Your Feedback
This competition is a low-stakes, trial-run introduction. We’re considering this a beta launch – there are complicated new mechanics in play and we’re still working on refining the process. We’d love your help testing the experience and want to hear your feedback.
Please note that we may make changes throughout the competition that could include things like resetting the leaderboard, invalidating episodes, making changes to the interface, or changing the environment configuration (e.g. modifying the number of columns, rows, or tokens in a row required to win, etc).",,,This competition does not award ranking points,This competition does not count towards tiers,Getting Started Simulation Competition
Categorical Feature Encoding Challenge II,,3 years ago,3 years ago,"['binary classification', 'auc']","1,161","1,293","13,837","Can you find more cat in your dat?
We loved the participation and engagement with the first Cat in the Dat competition.
Because this is such a common task and important skill to master, we've put together a dataset that contains only categorical features, and includes:
binary features
low- and high-cardinality nominal features
low- and high-cardinality ordinal features
(potentially) cyclical features
This follow-up competition offers an even more challenging dataset so that you can continue to build your skills with the common machine learning task of encoding categorical variables. This challenge adds the additional complexity of feature interactions, as well as missing data.
This Playground competition will give you the opportunity to try different encoding schemes for different algorithms to compare how they perform. We encourage you to share what you find with the community.
If you're not sure how to get started, you can check out the Categorical Variables section of Kaggle's Intermediate Machine Learning course.
Have Fun!",3 files,145.84 MB,This competition did not award ranking points,This competition did not count towards tiers,Playground Prediction Competition
Santa 2019 - Revenge of the Accountants,,3 years ago,3 years ago,"['optimization', 'holidays and cultural events', 'custom metric']",106,110,436,"Santa was thrilled with the Kaggle community for minimizing his workshop costs! He had heard rumors that Kagglers were adept at cracking holiday challenges, but, wow, even Santa was surprised at this one.
Unfortunately, the North Pole accountants were less pleased. It turns out, the accountants didn't like being one-upped by machine learning experts on the internet.
To complicate matters, they've decided to allow an additional 1,000 families attend the workshop. And they've also ""fine tuned"" their accounting formula to try and trip up those fancy solvers some people have at their disposal.
Of course, we know that nothing trips up the Kaggle community! (Well, except for maybe over-fitting. But fortunately, that doesn't apply here!)
So this is a bonus Santa competition for those who want an additional challenge and the opportunity to continue to improve their optimization skills. Since Santa used up all his budget on accounting fees, this is strictly a Playground competition, with the chance to win some coveted Kaggle Swag.
Have fun, and Happy Holidays from the Kaggle Team!
Attribution
Banner/Listing Photo by Helloquence on Unsplash",2 files,258.68 kB,This competition did not award ranking points,This competition did not count towards tiers,Playground Prediction Competition
Natural Language Processing with Disaster Tweets,,,,"['nlp', 'binary classification', 'text', 'fscoremicro']",870,895,"3,524","Welcome to one of our ""Getting Started"" competitions 👋
This particular challenge is perfect for data scientists looking to get started with Natural Language Processing. The competition dataset is not too big, and even if you don’t have much personal computing power, you can do all of the work in our free, no-setup, Jupyter Notebooks environment called Kaggle Notebooks.
Competition Description
Twitter has become an important communication channel in times of emergency.
The ubiquitousness of smartphones enables people to announce an emergency they’re observing in real-time. Because of this, more agencies are interested in programatically monitoring Twitter (i.e. disaster relief organizations and news agencies).
But, it’s not always clear whether a person’s words are actually announcing a disaster. Take this example:




The author explicitly uses the word “ABLAZE” but means it metaphorically. This is clear to a human right away, especially with the visual aid. But it’s less clear to a machine.
In this competition, you’re challenged to build a machine learning model that predicts which Tweets are about real disasters and which one’s aren’t. You’ll have access to a dataset of 10,000 tweets that were hand classified. If this is your first time working on an NLP problem, we've created a quick tutorial to get you up and running.
Disclaimer: The dataset for this competition contains text that may be considered profane, vulgar, or offensive.
Acknowledgments
This dataset was created by the company figure-eight and originally shared on their ‘Data For Everyone’ website here.
Tweet source: https://twitter.com/AnyOtherAnnaK/status/629195955506708480",3 files,1.43 MB,This competition does not award ranking points,This competition does not count towards tiers,Getting Started Prediction Competition
Bengali.AI Handwritten Grapheme Classification,"$10,000 ",3 years ago,,"['image', 'multiclass classification', 'weightedcategorizationaccuracy']","2,059","2,623","38,927","Challenge and dataset summary paper available at https://arxiv.org/abs/2010.00170
Bengali is the 5th most spoken language in the world with hundreds of million of speakers. It’s the official language of Bangladesh and the second most spoken language in India. Considering its reach, there’s significant business and educational interest in developing AI that can optically recognize images of the language handwritten. This challenge hopes to improve on approaches to Bengali recognition.
Optical character recognition is particularly challenging for Bengali. While Bengali has 49 letters (to be more specific 11 vowels and 38 consonants) in its alphabet, there are also 18 potential diacritics, or accents. This means that there are many more graphemes, or the smallest units in a written language. The added complexity results in ~13,000 different grapheme variations (compared to English’s 250 graphemic units).
Bangladesh-based non-profit Bengali.AI is focused on helping to solve this problem. They build and release crowdsourced, metadata-rich datasets and open source them through research competitions. Through this work, Bengali.AI hopes to democratize and accelerate research in Bengali language technologies and to promote machine learning education.
For this competition, you’re given the image of a handwritten Bengali grapheme and are challenged to separately classify three constituent elements in the image: grapheme root, vowel diacritics, and consonant diacritics.
By participating in the competition, you’ll hopefully accelerate Bengali handwritten optical character recognition research and help enable the digitalization of educational resources. Moreover, the methods introduced in the competition will also empower cousin languages in the Indian subcontinent.
Acknowledgements:
Apurba: Apurba is the exclusive sponsor of Bengali.AI for this competition. Apurba Technologies Inc. is founded by a group of technology veterans who have been working at the cutting edge of software development in Silicon Valley for many years. Apart from its many ventures, Apurba is a pioneer in Bengali NLP research today and is accelerating AI research in Bangladesh through its contributions.
Intelligent Machines Limited: Intelligent Machines Limited is the technical partner of Bengali.AI for this competition and is providing compute support to Bangladeshi students. IML is an Artificial Intelligence and Advanced Analytics startup offering customized solutions to businesses in Bangladesh. IML believes in the strength of Bangladeshi talented resources and in the possibility of a far greater and developed Bangladesh in the coming days.
If you use this dataset in your research, please cite this paper
@inproceedings{alam2021large,
title={A Large Multi-target Dataset of Common Bengali Handwritten Graphemes},
author={Alam, Samiul and Reasat, Tahsin and Sushmit, Asif Shahriyar and Siddique, Sadi Mohammad and Rahman, Fuad and Hasan, Mahady and Humayun, Ahmed Imtiaz},
booktitle={International Conference on Document Analysis and Recognition},
pages={383--398},
year={2021},
organization={Springer}
}",14 files,5.18 GB,This competition awarded ranking points,This competition counted towards tiers,Research Code Competition
Deepfake Detection Challenge,"$1,000,000 ",3 years ago,,"['video data', 'logloss']","2,265","8,581",,"This competition is closed for submissions. Participants' selected code submissions were re-run by the host on a privately-held test set and the private leaderboard results have been finalized. Late submissions will not be opened, due to an inability to replicate the unique design of this competition.
Deepfake techniques, which present realistic AI-generated videos of people doing and saying fictional things, have the potential to have a significant impact on how people determine the legitimacy of information presented online. These content generation and modification technologies may affect the quality of public discourse and the safeguarding of human rights—especially given that deepfakes may be used maliciously as a source of misinformation, manipulation, harassment, and persuasion. Identifying manipulated media is a technically demanding and rapidly evolving challenge that requires collaborations across the entire tech industry and beyond.
AWS, Facebook, Microsoft, the Partnership on AI’s Media Integrity Steering Committee, and academics have come together to build the Deepfake Detection Challenge (DFDC). The goal of the challenge is to spur researchers around the world to build innovative new technologies that can help detect deepfakes and manipulated media.
Challenge participants must submit their code into a black box environment for testing. Participants will have the option to make their submission open or closed when accepting the prize. Open proposals will be eligible for challenge prizes as long as they abide by the open source licensing terms. Closed proposals will be proprietary and not be eligible to accept the prizes. Regardless of which track is chosen, all submissions will be evaluated in the same way. Results will be shown on the leaderboard.
The PAI Steering Committee has emphasized the need to ensure that all technical efforts incorporate attention to how the resulting code and products based on it can be made as accessible and useful as possible to key frontline defenders of information quality such as journalists and civic leaders around the world. The DFDC results will be a contribution to this effort and building a robust response to the emergent threat deepfakes pose globally.",802 files,4.44 GB,This competition awarded ranking points,This competition counted towards tiers,Featured Code Competition
Santa's Workshop Tour 2019,"$25,000 ",3 years ago,,"['optimization', 'holidays and cultural events', 'custom metric']","1,618","1,781","13,783","Hammers ring, are you listenin’
In the shop, toys are glistenin’
Should they see the sights?
There might be a fight…
Walkin’ ‘round the Workshop Wonderland
Families said, they want to see it
Santa said, he’d guarantee it
They pick a date
But they may have to wait
Walkin’ ‘round the Workshop Wonderland
We told Santa that he was a madman
He just wants to make sure they all smile
He’ll say “Are you flexible?“, They’ll say “Yeah man,
But can you help us make it worth our while?”
“Give them food, or sweater
the more they wait, the gifts get better”
Please help us rank
Or we’ll break the bank!
Walkin’ ’round the Workshop Wonderland
Santa has exciting news! For 100 days before Christmas, he opened up tours to his workshop. Because demand was so strong, and because Santa wanted to make things as fair as possible, he let each of the 5,000 families that will visit the workshop choose a list of dates they'd like to attend the workshop.
Now that all the families have sent Santa their preferences, he's realized it's impossible for everyone to get their top picks, so he's decided to provide extra perks for families that don't get their preferences. In addition, Santa's accounting department has told him that, depending on how families are scheduled, there may be some unexpected and hefty costs incurred.
Santa needs the help of the Kaggle community to optimize which day each family is assigned to attend the workshop in order to minimize any extra expenses that would cut into next years toy budget! Can you help Santa out?
Attribution
Banner/Listing Photo by Nathan Lemon on Unsplash
Description Photo by Markus Spiske on Unsplash",2 files,215.1 kB,This competition awarded ranking points,This competition counted towards tiers,Featured Prediction Competition
NFL 1st and Future - Analytics,"$75,000 ",3 years ago,3 years ago,"['tabular', 'sports']",,,,"Welcome
In this challenge, you're tasked to investigate the relationship between the playing surface and the injury and performance of National Football League (NFL) athletes and to examine factors that may contribute to lower extremity injuries.
You'll also notice there isn't a leaderboard, and you are not required to develop a predictive model. This isn't a traditional supervised Kaggle machine learning competition. For more information on this challenge format, see this forum thread. This challenge is part of NFL 1st & Future, the NFL’s annual Super Bowl competition designed to spur innovation in player health, safety and performance.
The Challenge
In the NFL, 12 stadiums have fields with synthetic turf. Recent investigations of lower limb injuries among football athletes have indicated significantly higher injury rates on synthetic turf compared with natural turf (Mack et al., 2018; Loughran et al., 2019). In conjunction with the epidemiologic investigations, biomechanical studies of football cleat-surface interactions have shown that synthetic turf surfaces do not release cleats as readily as natural turf and may contribute to the incidence of non-contact lower limb injuries (Kent et al., 2015). Given these differences in cleat-turf interactions, it has yet to be determined whether player movement patterns and other measures of player performance differ across playing surfaces and how these may contribute to the incidence of lower limb injury.
Now, the NFL is challenging Kagglers to help them examine the effects that playing on synthetic turf versus natural turf can have on player movements and the factors that may contribute to lower extremity injuries. NFL player tracking, also known as Next Gen Stats, is the capture of real time location data, speed and acceleration for every player, every play on every inch of the field. As part of this challenge, the NFL has provided full player tracking of on-field position for 250 players over two regular season schedules. One hundred of the athletes in the study data set sustained one or more injuries during the study period that were identified as a non-contact injury of a type that may have turf interaction as a contributing factor to injury. The remaining 150 athletes serve as a representative sample of the larger NFL population that did not sustain a non-contact lower-limb injury during the study period. Details of the surface type and environmental parameters that may influence performance and outcome are also provided.
Your challenge is to characterize any differences in player movement between the playing surfaces and identify specific scenarios (e.g., field surface, weather, position, play type, etc.) that interact with player movement to present an elevated risk of injury. More details on the entry criteria are available in Evaluation Tab.
About The NFL
The National Football League is America's most popular sports league, comprised of 32 franchises that compete each year to win the Super Bowl, the world's biggest annual sporting event. Founded in 1920, the NFL developed the model for the successful modern sports league, including national and international distribution, extensive revenue sharing, competitive excellence, and strong franchises across the country.
The NFL is committed to advancing progress in the diagnosis, prevention and treatment of sports-related injuries. The NFL's ongoing health and safety efforts include support for independent medical research and engineering advancements and a commitment to work to better protect players and make the game safer, including enhancements to medical protocols and improvements to how our game is taught and played.
As more is learned, the league evaluates and changes rules to evolve the game and try to improve protections for players. Since 2002 alone, the NFL has made 50 rules changes intended to eliminate potentially dangerous tactics and reduce the risk of injuries.
For more information about the NFL's health and safety efforts, please visit www.PlaySmartPlaySafe.com
Evaluation",3 files,4 GB,,,Analytics Competition
Google QUEST Q&A Labeling,"$25,000 ",3 years ago,,"['nlp', 'text', 'mcspearmanr']","1,571","1,904","27,817","Computers are really good at answering questions with single, verifiable answers. But, humans are often still better at answering questions about opinions, recommendations, or personal experiences.
Humans are better at addressing subjective questions that require a deeper, multidimensional understanding of context - something computers aren't trained to do well…yet.. Questions can take many forms - some have multi-sentence elaborations, others may be simple curiosity or a fully developed problem. They can have multiple intents, or seek advice and opinions. Some may be helpful and others interesting. Some are simple right or wrong.
Unfortunately, it’s hard to build better subjective question-answering algorithms because of a lack of data and predictive models. That’s why the CrowdSource team at Google Research, a group dedicated to advancing NLP and other types of ML science via crowdsourcing, has collected data on a number of these quality scoring aspects.
In this competition, you’re challenged to use this new dataset to build predictive algorithms for different subjective aspects of question-answering. The question-answer pairs were gathered from nearly 70 different websites, in a ""common-sense"" fashion. Our raters received minimal guidance and training, and relied largely on their subjective interpretation of the prompts. As such, each prompt was crafted in the most intuitive fashion so that raters could simply use their common-sense to complete the task. By lessening our dependency on complicated and opaque rating guidelines, we hope to increase the re-use value of this data set. What you see is what you get!
Demonstrating these subjective labels can be predicted reliably can shine a new light on this research area. Results from this competition will inform the way future intelligent Q&A systems will get built, hopefully contributing to them becoming more human-like.",3 files,14.85 MB,This competition awarded ranking points,This competition counted towards tiers,Featured Code Competition
2019 Kaggle Machine Learning & Data Science Survey,"$30,000 ",3 years ago,3 years ago,"['jobs and career', 'survey analysis']",,,,"Overview
Welcome to Kaggle's third annual Machine Learning and Data Science Survey ― and our second-ever survey data challenge. You can read our executive summary here.
This year, as in 2017 and 2018, we set out to conduct an industry-wide survey that presents a truly comprehensive view of the state of data science and machine learning. The survey was live for three weeks in October, and after cleaning the data we finished with 19,717 responses!
There's a lot to explore here. The results include raw numbers about who is working with data, what’s happening with machine learning in different industries, and the best ways for new data scientists to break into the field. We've published the data in as raw a format as possible without compromising anonymization, which makes it an unusual example of a survey dataset.
Challenge
This year Kaggle is launching the second annual Data Science Survey Challenge, where we will be awarding a prize pool of $30,000 to notebook authors who tell a rich story about a subset of the data science and machine learning community.
In our third year running this survey, we were once again awed by the global, diverse, and dynamic nature of the data science and machine learning industry. This survey data EDA provides an overview of the industry on an aggregate scale, but it also leaves us wanting to know more about the many specific communities comprised within the survey. For that reason, we’re inviting the Kaggle community to dive deep into the survey datasets and help us tell the diverse stories of data scientists from around the world.
The challenge objective: tell a data story about a subset of the data science community represented in this survey, through a combination of both narrative text and data exploration. A “story” could be defined any number of ways, and that’s deliberate. The challenge is to deeply explore (through data) the impact, priorities, or concerns of a specific group of data science and machine learning practitioners. That group can be defined in the macro (for example: anyone who does most of their coding in Python) or the micro (for example: female data science students studying machine learning in masters programs). This is an opportunity to be creative and tell the story of a community you identify with or are passionate about!
Submissions will be evaluated on the following:
Composition - Is there a clear narrative thread to the story that’s articulated and supported by data? The subject should be well defined, well researched, and well supported through the use of data and visualizations.
Originality - Does the reader learn something new through this submission? Or is the reader challenged to think about something in a new way? A great entry will be informative, thought provoking, and fresh all at the same time.
Documentation - Are your code, and notebook, and additional data sources well documented so a reader can understand what you did? Are your sources clearly cited? A high quality analysis should be concise and clear at each step so the rationale is easy to follow and the process is reproducible
To be valid, a submission must be contained in one notebook, made public on or before the submission deadline. Participants are free to use any datasets in addition to the Kaggle Data Science survey, but those datasets must also be publicly available on Kaggle by the deadline for a submission to be valid.
How to Participate
To make a submission, complete the submission form. Only one submission will be judged per participant, so if you make multiple submissions we will review the last (most recent) entry.
No submission is necessary for the Weekly Notebook Award. To be eligible, a notebook must be public and use the 2019 Data Science Survey as a data source.
Submission deadline: 11:59PM UTC, December 2nd, 2019.
Survey Methodology
This survey received 19,717 usable respondents from 171 countries and
territories. If a country or territory received less than 50
respondents, we grouped them into a group named “Other” for
anonymity.
We excluded respondents who were flagged by our survey system as
“Spam”.
Most of our respondents were found primarily through Kaggle channels,
like our email list, discussion forums and social media channels.
The survey was live from October 8th to October 28th. We allowed
respondents to complete the survey at any time during that window.
The median response time for those who participated in the survey was
approximately 10 minutes.
Not every question was shown to every respondent. You can learn more
about the different segments we used in the survey_schema.csv file. In general, respondents with more experience were asked more questions and respondents with less experience were asked less questions.
To protect the respondents’ identity, the answers to multiple choice
questions have been separated into a separate data file from the
open-ended responses. We do not provide a key to match up the
multiple choice and free form responses. Further, the free form
responses have been randomized column-wise such that the responses
that appear on the same row did not necessarily come from the same
survey-taker.
Multiple choice single response questions fit into individual columns whereas multiple choice multiple response questions were split into multiple columns. Text responses were encoded to protect user privacy and countries with fewer than 50 respondents were grouped into the category ""other"".
Data has been released under a CC 2.0 license: https://creativecommons.org/licenses/by/2.0/",4 files,22.21 MB,,,Analytics Competition
TensorFlow 2.0 Question Answering,"$50,000 ",3 years ago,,"['text', 'text mining', 'custom metric']","1,233","1,417","9,846","“Why is the sky blue?”
This is a question an open-domain question answering (QA) system should be able to respond to. QA systems emulate how people look for information by reading the web to return answers to common questions. Machine learning can be used to improve the accuracy of these answers.
Existing natural language models have been focused on extracting answers from a short paragraph rather than reading an entire page of content for proper context. As a result, the responses can be complicated or lengthy. A good answer will be both succinct and relevant.
In this competition, your goal is to predict short and long answer responses to real questions about Wikipedia articles. The dataset is provided by Google's Natural Questions, but contains its own unique private test set. A visualization of examples shows long and—where available—short answers. In addition to prizes for the top teams, there is a special set of awards for using TensorFlow 2.0 APIs.
If successful, this challenge will help spur the development of more effective and robust QA systems.
About TensorFlow
TensorFlow is an open source platform for machine learning. With TensorFlow 2.0, tf.keras is the preferred high-level API for TensorFlow, to make model building easier and more intuitive. You may use the tf.keras built-in compile()/fit() methods, or write your own custom training loops. See the Effective TensorFlow 2.0 guide and the tf.keras guide for more details.
TensorFlow 2.0 was recently released and this competition is to challenge Kagglers to use TensorFlow 2.0’s APIs focused on usability, and easier, more intuitive development, to make advancements on Question Answering.",3 files,17.47 GB,This competition awarded ranking points,This competition counted towards tiers,Featured Code Competition
Peking University/Baidu - Autonomous Driving,"$25,000 ",3 years ago,,"['image', 'computer vision', 'custom metric']",864,"1,105","10,001","Who do you think hates traffic more - humans or self-driving cars? The position of nearby automobiles is a key question for autonomous vehicles ― and it's at the heart of our newest challenge.
Self-driving cars have come a long way in recent years, but they're still not flawless. Consumers and lawmakers remain wary of adoption, in part because of doubts about vehicles’ ability to accurately perceive objects in traffic.
Baidu's Robotics and Autonomous Driving Lab (RAL), along with Peking University, hopes to close the gap once and for all with this challenge. They’re providing Kagglers with more than 60,000 labeled 3D car instances from 5,277 real-world images, based on industry-grade CAD car models.
Your challenge: develop an algorithm to estimate the absolute pose of vehicles (6 degrees of freedom) from a single image in a real-world traffic environment.
Succeed and you'll help improve computer vision. That, in turn, will bring autonomous vehicles a big step closer to widespread adoption, so they can help reduce the environmental impact of our growing societies.
Please cite the following paper when using the dataset:
ApolloCar3D: A Large 3D Car Instance Understanding Benchmark for Autonomous Driving
@inproceedings{song2019apollocar3d,
title={Apollocar3d: A large 3d car instance understanding benchmark for autonomous driving},
author={Song, Xibin and Wang, Peng and Zhou, Dingfu and Zhu, Rui and Guan, Chenye and Dai, Yuchao and Su, Hao and Li, Hongdong and Yang, Ruigang},
booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
pages={5452--5462},
year={2019}
}",12358 files,6.3 GB,This competition awarded ranking points,This competition counted towards tiers,Featured Prediction Competition
ASHRAE - Great Energy Predictor III,"$25,000 ",3 years ago,,"['tabular', 'energy', 'rmsle']","3,614","39,402",,"Q: How much does it cost to cool a skyscraper in the summer?
A: A lot! And not just in dollars, but in environmental impact.
Thankfully, significant investments are being made to improve building efficiencies to reduce costs and emissions. The question is, are the improvements working? That’s where you come in. Under pay-for-performance financing, the building owner makes payments based on the difference between their real energy consumption and what they would have used without any retrofits. The latter values have to come from a model. Current methods of estimation are fragmented and do not scale well. Some assume a specific meter type or don’t work with different building types.
In this competition, you’ll develop accurate models of metered building energy usage in the following areas: chilled water, electric, hot water, and steam meters. The data comes from over 1,000 buildings over a three-year timeframe. With better estimates of these energy-saving investments, large scale investors and financial institutions will be more inclined to invest in this area to enable progress in building efficiencies.
About the Host
Founded in 1894, ASHRAE serves to advance the arts and sciences of heating, ventilation, air conditioning refrigeration and their allied fields. ASHRAE members represent building system design and industrial process professionals around the world. With over 54,000 members serving in 132 countries, ASHRAE supports research, standards writing, publishing and continuing education - shaping tomorrow’s built environment today.
Banner photo by Federico Beccari on Unsplash",6 files,2.61 GB,This competition awarded ranking points,This competition counted towards tiers,Featured Prediction Competition
NFL Big Data Bowl,"$75,000 ",3 years ago,,"['sports', 'football', 'crps']","2,038","3,097",,"“The running back takes the handoff… he breaks a tackle…spins… and breaks free! One man to beat! Past the 50-yard-line! To the 40! The 30! He! Could! Go! All! The! Way!”
But will he?
American football is a complex sport. From the 22 players on the field to specific characteristics that ebb and flow throughout the game, it can be challenging to quantify the value of specific plays and actions within a play. Fundamentally, the goal of football is for the offense to run (rush) or throw (pass) the ball to gain yards, moving towards, then across, the opposing team’s side of the field in order to score. And the goal of the defense is to prevent the offensive team from scoring.
In the National Football League (NFL), roughly a third of teams’ offensive yardage comes from run plays.. Ball carriers are generally assigned the most credit for these plays, but their teammates (by way of blocking), coach (by way of play call), and the opposing defense also play a critical role. Traditional metrics such as ‘yards per carry’ or ‘total rushing yards’ can be flawed; in this competition, the NFL aims to provide better context into what contributes to a successful run play.
As an “armchair quarterback” watching the game, you may think you can predict the result of a play when a ball carrier takes the handoff - but what does the data say? In this competition, you will develop a model to predict how many yards a team will gain on given rushing plays as they happen. You'll be provided game, play, and player-level data, including the position and speed of players as provided in the NFL’s Next Gen Stats data. And the best part - you can see how your model performs from your living room, as the leaderboard will be updated week after week on the current season’s game data as it plays out.
Deeper insight into rushing plays will help teams, media, and fans better understand the skill of players and the strategies of coaches. It will also assist the NFL and its teams evaluate the ball carrier, his teammates, his coach, and the opposing defense, in order to make adjustments as necessary.
Additionally, the winning model will be provided to the NFL’s Next Gen Stats group to potentially share with teams. You could help the NFL Network generate models to use during games, or for pre-game/post-game breakdowns.",5 files,289.9 MB,This competition awarded ranking points,This competition counted towards tiers,Featured Code Competition
Kannada MNIST,,3 years ago,3 years ago,"['image', 'computer vision', 'categorizationaccuracy']","1,212","1,342","11,077","Bored of MNIST?
The goal of this competition is to provide a simple extension to the classic MNIST competition we're all familiar with. Instead of using Arabic numerals, it uses a recently-released dataset of Kannada digits.
Kannada is a language spoken predominantly by people of Karnataka in southwestern India. The language has roughly 45 million native speakers and is written using the Kannada script. Wikipedia
This competition uses the same format as the MNIST competition in terms of how the data is structured, but it's different in that it is a synchronous re-run Kernels competition. You write your code in a Kaggle Notebook, and when you submit the results, your code is scored on both the public test set, as well as a private (unseen) test set.
Technical Information
All details of the dataset curation has been captured in the paper titled: Prabhu, Vinay Uday. ""Kannada-MNIST: A new handwritten digits dataset for the Kannada language."" arXiv preprint arXiv:1908.01242 (2019)
The github repo of the author can be found here.
On the originally-posted dataset, the author suggests some interesting questions you may be interested in exploring. Please note, although this dataset has been released in full, the purpose of this competition is for practice, not to find the labels to submit a perfect score.
In addition to the main dataset, the author also disseminated an additional real world handwritten dataset (with 10k images), termed as the 'Dig-MNIST dataset' that can serve as an out-of-domain test dataset. It was created with the help of volunteers that were non-native users of the language, authored on a smaller sheet and scanned with different scanner settings compared to the main dataset. This 'dig-MNIST' dataset serves as a more difficult test-set (An accuracy of 76.1% was reported in the paper cited above) and achieving ~98+% accuracy on this test dataset would be rather commendable.
Acknowledgments
Kaggle thanks Vinay Prabhu for providing this interesting dataset for a Playground competition.
Image reference: https://www.researchgate.net/figure/speech-for-Kannada-numbers_fig2_313113588",4 files,131.85 MB,This competition did not award ranking points,This competition did not count towards tiers,Playground Code Competition
RSNA Intracranial Hemorrhage Detection,"$25,000 ",3 years ago,,"['image', 'weightedmeancolumnwiselogloss']","1,345","2,553",,"Intracranial hemorrhage, bleeding that occurs inside the cranium, is a serious health problem requiring rapid and often intensive medical treatment. For example, intracranial hemorrhages account for approximately 10% of strokes in the U.S., where stroke is the fifth-leading cause of death. Identifying the location and type of any hemorrhage present is a critical step in treating the patient.
Diagnosis requires an urgent procedure. When a patient shows acute neurological symptoms such as severe headache or loss of consciousness, highly trained specialists review medical images of the patient’s cranium to look for the presence, location and type of hemorrhage. The process is complicated and often time consuming.
In this competition, your challenge is to build an algorithm to detect acute intracranial hemorrhage and its subtypes.
You’ll develop your solution using a rich image dataset provided by the Radiological Society of North America (RSNA®) in collaboration with members of the American Society of Neuroradiology and MD.ai.
If successful, you’ll help the medical community identify the presence, location and type of hemorrhage in order to quickly and effectively treat affected patients.
Challenge participants may be invited to present their AI models and methodologies during an award ceremony at the RSNA Annual Meeting which will be held in Chicago, Illinois, USA, from December 1-6, 2019.
Collaborators
Four research institutions provided large volumes of de-identified CT studies that were assembled to create the challenge dataset: Stanford University, Thomas Jefferson University, Unity Health Toronto and Universidade Federal de São Paulo (UNIFESP), The American Society of Neuroradiology (ASNR) organized a cadre of more than 60 volunteers to label over 25,000 exams for the challenge dataset. ASNR is the world’s leading organization for the future of neuroradiology representing more than 5,300 radiologists, researchers, interventionalists, and imaging scientists. MD.ai provided tooling and support for the data annotation process.
The RSNA is an international society of radiologists, medical physicists and other medical professionals with more than 54,000 members from 146 countries across the globe. They see the potential for AI to assist in detection and classification of hemorrhages in order to prioritize and expedite their clinical work.
A full set of acknowledgments can be found on this page.",874037 files,458.97 GB,This competition awarded ranking points,This competition counted towards tiers,Featured Prediction Competition
BigQuery-Geotab Intersection Congestion,,3 years ago,,"['tabular', 'regression', 'geospatial analysis', 'cities and urban areas', 'rmse']",432,487,"3,372","We’ve all been there: Stuck at a traffic light, only to be given mere seconds to pass through an intersection, behind a parade of other commuters. Imagine if you could help city planners and governments anticipate traffic hot spots ahead of time and reduce the stop-and-go stress of millions of commuters like you.
Geotab provides a wide variety of aggregate datasets gathered from commercial vehicle telematics devices. Harnessing the insights from this data has the power to improve safety, optimize operations, and identify opportunities for infrastructure challenges.
The dataset for this competition includes aggregate stopped vehicle information and intersection wait times. Your task is to predict congestion, based on an aggregate measure of stopping distance and waiting times, at intersections in 4 major US cities: Atlanta, Boston, Chicago & Philadelphia.
This competition is being hosted in partnership with BigQuery, a data warehouse for manipulating, joining, and querying large scale tabular datasets. BigQuery also offers BigQuery ML, an easy way for users to create and run machine learning models to generate predictions through a SQL query interface.
Kaggle recently released a BigQuery integration within our kernels notebook environment, and this starter kernel gives you a great starting point for how to use BQ & BQML. You’re encouraged to use your data savvy, resourcefulness & intuition to find and join in additional external datasets that will increase your models’ predictive power.
Alright, stop waiting and get started!
Acknowledgments
A big thanks to Geotab for providing the dataset for this competition! Geotab is advancing security, connecting commercial vehicles to the internet and providing web-based analytics to help customers better manage their fleets. Geotab’s open platform and Marketplace, offering hundreds of third-party solution options, allows both small and large businesses to automate operations by integrating vehicle data with their other data assets. As an IoT hub, the in-vehicle device provides additional functionality through IOX Add-Ons. Processing billions of data points a day, Geotab leverages data analytics and machine learning to help customers improve productivity, optimize fleets through the reduction of fuel consumption, enhance driver safety, and achieve strong compliance to regulatory changes. Geotab’s products are represented and sold worldwide through Authorized Geotab Resellers. To learn more, please visit www.geotab.com and follow us @GEOTAB and on LinkedIn.",6 files,577.97 MB,This competition did not award ranking points,This competition did not count towards tiers,Playground Prediction Competition
Lyft 3D Object Detection for Autonomous Vehicles,"$25,000 ",3 years ago,,"['image', 'custom metric']",546,659,"5,697","Self-driving technology presents a rare opportunity to improve the quality of life in many of our communities. Avoidable collisions, single-occupant commuters, and vehicle emissions are choking cities, while infrastructure strains under rapid urban growth. Autonomous vehicles are expected to redefine transportation and unlock a myriad of societal, environmental, and economic benefits. You can apply your data analysis skills in this competition to advance the state of self-driving technology.
Lyft, whose mission is to improve people’s lives with the world’s best transportation, is investing in the future of self-driving vehicles. Level 5, their self-driving division, is working on a fleet of autonomous vehicles, and currently has a team of 450+ across Palo Alto, London, and Munich working to build a leading self-driving system (they’re hiring!). Their goal is to democratize access to self-driving technology for hundreds of millions of Lyft passengers.
From a technical standpoint, however, the bar to unlock technical research and development on higher-level autonomy functions like perception, prediction, and planning is extremely high. This implies technical R&D on self-driving cars has traditionally been inaccessible to the broader research community.
This dataset aims to democratize access to such data, and foster innovation in higher-level autonomy functions for everyone, everywhere. By conducting a competition, we hope to encourage the research community to focus on hard problems in this space—namely, 3D object detection over semantic maps.
In this competition, you will build and optimize algorithms based on a large-scale dataset. This dataset features the raw sensor camera inputs as perceived by a fleet of multiple, high-end, autonomous vehicles in a restricted geographic area.
If successful, you’ll make a significant contribution towards stimulating further development in autonomous vehicles and empowering communities around the world.",409273 files,125.79 GB,This competition awarded ranking points,This competition counted towards tiers,Featured Prediction Competition
Categorical Feature Encoding Challenge,,3 years ago,3 years ago,"['tabular', 'binary classification', 'auc']","1,338","1,387","13,527","Is there a cat in your dat?
A common task in machine learning pipelines is encoding categorical variables for a given algorithm in a format that allows as much useful signal as possible to be captured.
Because this is such a common task and important skill to master, we've put together a dataset that contains only categorical features, and includes:
binary features
low- and high-cardinality nominal features
low- and high-cardinality ordinal features
(potentially) cyclical features
This Playground competition will give you the opportunity to try different encoding schemes for different algorithms to compare how they perform. We encourage you to share what you find with the community.
If you're not sure how to get started, you can check out the Categorical Variables section of Kaggle's Intermediate Machine Learning course.
Have Fun!",3 files,67.96 MB,This competition did not award ranking points,This competition did not count towards tiers,Playground Prediction Competition
Understanding Clouds from Satellite Images,"$10,000 ",3 years ago,,"['image', 'atmospheric science', 'dice']","1,531","1,857","29,163","Climate change has been at the top of our minds and on the forefront of important political decision-making for many years. We hope you can use this competition’s dataset to help demystify an important climatic variable. Scientists, like those at Max Planck Institute for Meteorology, are leading the charge with new research on the world’s ever-changing atmosphere and they need your help to better understand the clouds.
Shallow clouds play a huge role in determining the Earth's climate. They’re also difficult to understand and to represent in climate models. By classifying different types of cloud organization, researchers at Max Planck hope to improve our physical understanding of these clouds, which in turn will help us build better climate models.
There are many ways in which clouds can organize, but the boundaries between different forms of organization are murky. This makes it challenging to build traditional rule-based algorithms to separate cloud features. The human eye, however, is really good at detecting features—such as clouds that resemble flowers.
In this challenge, you will build a model to classify cloud organization patterns from satellite images. If successful, you’ll help scientists to better understand how clouds will shape our future climate. This research will guide the development of next-generation models which could reduce uncertainties in climate projections.
Help us remove the haze from climate models and bring clarity to cloud identification.
For more information on the scientific background and how the labels were created see the following paper.",9246 files,6.41 GB,This competition awarded ranking points,This competition counted towards tiers,Research Prediction Competition
Ciphertext Challenge III,,3 years ago,3 years ago,"['text', 'categorizationaccuracy']",103,110,346,"Ciphertext Challenge III: Wherefore Art Thou, Simple Ciphers?
We've done the 2010's, the 1990s… now it's time for the 80s.
The 1580s!!
In this new decryption competition's dataset, we've gone from perfectly respectable sources of electronic horror to a time before computers—heck, before calculus was called ""calculus""! Shakespeare's plays are encrypted, and we time travelers must un-encrypt them so people can do innovative stage productions with intricate makeup, costumes, and possibly—possibly!—Leonardo DiCaprio. Think about it, folks: Leo.*
As in previous ciphertext challenges, simple classic ciphers have been used to encrypt this dataset, along with a slightly less simple surprise that expands our definition of ""classic"" into the modern age. The mission is the same: to correctly match each piece of ciphertext with its corresponding piece of plaintext. Daunting! Meta-puzzles and difficulty await!
Swag prizes go to the first three teams to crack all four ciphers OR to the top three teams on the leaderboard (in case the ciphers are not all cracked). Additionally, swag prizes will be awarded to the best competition-related kernels, in both visualization and cryptanalysis, based on upvotes. Last, the coveted ""Phil Prize""—for the team that correctly deduces the form AND key of the final cipher—is up for grabs again.
Go ahead. Get cracking!
* - Leo!
Acknowledgements
Many thanks to Kaggler LiamLarson for their excellent Shakespeare dataset.",3 files,52.6 MB,This competition did not award ranking points,This competition did not count towards tiers,Playground Prediction Competition
Severstal: Steel Defect Detection,"$120,000 ",3 years ago,,"['image', 'manufacturing', 'dice']","2,427","2,867","51,874","Steel is one of the most important building materials of modern times. Steel buildings are resistant to natural and man-made wear which has made the material ubiquitous around the world. To help make production of steel more efficient, this competition will help identify defects.
Severstal is leading the charge in efficient steel mining and production. They believe the future of metallurgy requires development across the economic, ecological, and social aspects of the industry—and they take corporate responsibility seriously. The company recently created the country’s largest industrial data lake, with petabytes of data that were previously discarded. Severstal is now looking to machine learning to improve automation, increase efficiency, and maintain high quality in their production.
The production process of flat sheet steel is especially delicate. From heating and rolling, to drying and cutting, several machines touch flat steel by the time it’s ready to ship. Today, Severstal uses images from high frequency cameras to power a defect detection algorithm.
In this competition, you’ll help engineers improve the algorithm by localizing and classifying surface defects on a steel sheet.
If successful, you’ll help keep manufacturing standards for steel high and enable Severstal to continue their innovation, leading to a stronger, more efficient world all around us.",18076 files,1.7 GB,This competition awarded ranking points,This competition counted towards tiers,Featured Code Competition
Kuzushiji Recognition,"$15,000 ",3 years ago,,"['image', 'multiclass classification', 'history', 'japan', 'custom metric']",293,338,"2,652","Build a model to transcribe ancient Kuzushiji into contemporary Japanese characters
Imagine the history contained in a thousand years of books. What stories are in those books? What knowledge can we learn from the world before our time? What was the weather like 500 years ago? What happened when Mt. Fuji erupted? How can one fold 100 cranes using only one piece of paper? The answers to these questions are in those books.
Japan has millions of books and over a billion historical documents such as personal letters or diaries preserved nationwide. Most of them cannot be read by the majority of Japanese people living today because they were written in “Kuzushiji”.
Even though Kuzushiji, a cursive writing style, had been used in Japan for over a thousand years, there are very few fluent readers of Kuzushiji today (only 0.01% of modern Japanese natives). Due to the lack of available human resources, there has been a great deal of interest in using Machine Learning to automatically recognize these historical texts and transcribe them into modern Japanese characters. Nevertheless, several challenges in Kuzushiji recognition have made the performance of existing systems extremely poor. (More information in About Kuzushiji)
This is where you come in. The hosts need help from machine learning experts to transcribe Kuzushiji into contemporary Japanese characters. With your help, Center for Open Data in the Humanities (CODH) will be able to develop better algorithms for Kuzushiji recognition. The model is not only a great contribution to the machine learning community, but also a great help for making millions of documents more accessible and leading to new discoveries in Japanese history and culture.
Hosts
Center for Open Data in the Humanities (CODH) conducts research and development to enhance access to humanities data using state-of-the-art technology in informatics and statistics.
The National Institute of Japanese Literature (NIJL) is an institution which strives to serve researchers in the field of Japanese literature as well as those working in various other humanities, by collecting in one location a vast storage of materials related to Japanese literature gathered from all corners of the country.
The National Institute of Informatics (NII) is Japan's only general academic research institution seeking to create future value in the new discipline of informatics. NII seeks to advance integrated research and development activities in information-related fields, including networking, software, and content.
Official Collaborators
Mikel Bober-Irizar (anokas) Kaggle Grandmaster and Alex Lamb (MILA. Quebec Artificial Intelligence Institute)",5 files,4.51 GB,This competition did not award ranking points,This competition did not count towards tiers,Playground Prediction Competition
IEEE-CIS Fraud Detection,"$20,000 ",3 years ago,3 years ago,"['tabular', 'binary classification', 'auc']","6,351","7,389","125,219","Imagine standing at the check-out counter at the grocery store with a long line behind you and the cashier not-so-quietly announces that your card has been declined. In this moment, you probably aren’t thinking about the data science that determined your fate.
Embarrassed, and certain you have the funds to cover everything needed for an epic nacho party for 50 of your closest friends, you try your card again. Same result. As you step aside and allow the cashier to tend to the next customer, you receive a text message from your bank. “Press 1 if you really tried to spend $500 on cheddar cheese.”
While perhaps cumbersome (and often embarrassing) in the moment, this fraud prevention system is actually saving consumers millions of dollars per year. Researchers from the IEEE Computational Intelligence Society (IEEE-CIS) want to improve this figure, while also improving the customer experience. With higher accuracy fraud detection, you can get on with your chips without the hassle.
IEEE-CIS works across a variety of AI and machine learning areas, including deep neural networks, fuzzy systems, evolutionary computation, and swarm intelligence. Today they’re partnering with the world’s leading payment service company, Vesta Corporation, seeking the best solutions for fraud prevention industry, and now you are invited to join the challenge.
In this competition, you’ll benchmark machine learning models on a challenging large-scale dataset. The data comes from Vesta's real-world e-commerce transactions and contains a wide range of features from device type to product features. You also have the opportunity to create new features to improve your results.
If successful, you’ll improve the efficacy of fraudulent transaction alerts for millions of people around the world, helping hundreds of thousands of businesses reduce their fraud loss and increase their revenue. And of course, you will save party people just like you the hassle of false positives.
Acknowledgements:
Vesta Corporation provided the dataset for this competition. Vesta Corporation is the forerunner in guaranteed e-commerce payment solutions. Founded in 1995, Vesta pioneered the process of fully guaranteed card-not-present (CNP) payment transactions for the telecommunications industry. Since then, Vesta has firmly expanded data science and machine learning capabilities across the globe and solidified its position as the leader in guaranteed ecommerce payments. Today, Vesta guarantees more than $18B in transactions annually.
Header Photo by Tim Evans on Unsplash",5 files,1.35 GB,This competition awarded ranking points,This competition counted towards tiers,Research Prediction Competition
Open Images 2019 - Instance Segmentation,"$25,000 ",3 years ago,,"['image', 'custom metric']",193,232,"1,566","Introduction
Computer vision has advanced considerably but is still challenged in matching the precision of human perception.
Open Images is a collaborative release of ~9 million images annotated with image-level labels, object bounding boxes, object segmentation masks, and visual relationships. This uniquely large and diverse dataset is designed to spur state of the art advances in analyzing and understanding images.
This year’s Open Images V5 release enabled the second Open Images Challenge to include the following 3 tracks:
Object detection track for detecting bounding boxes around object instances, relaunched from 2018.
Visual relationship detection track for detecting pairs of objects in particular relations, also relaunched from 2018.
Instance segmentation track for segmenting masks of objects in images, brand new for 2019.
Google AI hopes that having a single dataset with unified annotations for image classification, object detection, visual relationship detection, and instance segmentation will stimulate progress towards genuine scene understanding.
Instance Segmentation Track
In this track of the Challenge, you are asked to provide segmentation masks of objects.
This track’s training set represents 2.1M segmentation masks for object instances in 300 categories; with a validation set containing an additional 23k masks. The train set masks were produced by our state-of-the-art interactive segmentation process, where professional human annotators iteratively correct the output of a segmentation neural network. The validation and test set masks have been annotated manually with a strong focus on quality.
Example train set annotations. Left: Wuxi science park, 1995 by Gary Stevens. Right: Cat Cafe Shinjuku calico by Ari Helminen. Both images used under CC BY 2.0 license.

The results of this Challenge will be presented at a workshop at the International Conference on Computer Vision.
We are excited to partner with Open Images for this second year of competitions, including this brand new track!",100001 files,10.46 GB,This competition awarded 0.5X ranking points,This competition counted towards tiers,Research Prediction Competition
Generative Dog Images,"$10,000 ",3 years ago,3 years ago,['custom metric'],927,"12,306",,"This competition is closed and no longer accepting submissions. The private leaderboard has been finalized as of 8/28/2019.
Important Warning: This competition has an experimental format and submission style (images as submission). Competitors must use generative methods to create their submission images and are not permitted to make submissions that include any images already classified as dogs or altered versions of such images.
To enforce and prevent cheating, we reserve the right to: (a) Visually inspect all participants' submitted images, (b) review any submitted source code, (c) use these reviews to identify violators or determine winners, and (d) disqualify participants from the competition who are found in violation. This is also specified in the competition's rules
Use your training skills to create images, rather than identify them. You’ll be using GANs, which are at the creative frontier of machine learning. You might think of GANs as robot artists in a sense—able to create eerily lifelike images, and even digital worlds.
""You might not think that programmers are artists, but programming is an extremely creative profession. It’s logic-based creativity. '' -
John Romero
A generative adversarial network (GAN) is a class of machine learning system invented by Ian Goodfellow in 2014. Two neural networks compete with each other in a game. Given a training set, this technique learns to generate new data with the same statistics as the training set.
In this competition, you’ll be training generative models to create images of dogs. Only this time… there’s no ground truth data for you to predict. Here, you’ll submit the images and be scored based on how well those images are classified as dogs from pre-trained neural networks. Take these images, for example. Can you tell which are real vs. generated?
Trick question; they are all generated!
Why dogs? We chose dogs because, well, who doesn’t love looking at photos of adorable pups? Moreover, dogs can be classified into many sub-categories (breed, color, size), making them ideal candidates for image generation.
Generative methods (in particular, GANs) are currently used in various places on Kaggle for data augmentation. Their potential is vast; they can learn to mimic any distribution of data across any domain: photographs, drawings, music, and prose. If successful, not only will you help advance the state of the art in generative image creation, but you’ll enable us to create more experiments across a variety of domains in the future.
This is a Kernels-only competition. Refer to Kernels Requirements for details.",2 files,792.84 MB,This competition awarded ranking points,This competition counted towards tiers,Research Code Competition
APTOS 2019 Blindness Detection,"$50,000 ",3 years ago,,"['image', 'multiclass classification', 'healthcare', 'medicine', 'quadraticweightedkappa']","2,928","3,507","71,433","Imagine being able to detect blindness before it happened.
Millions of people suffer from diabetic retinopathy, the leading cause of blindness among working aged adults. Aravind Eye Hospital in India hopes to detect and prevent this disease among people living in rural areas where medical screening is difficult to conduct. Successful entries in this competition will improve the hospital’s ability to identify potential patients. Further, the solutions will be spread to other Ophthalmologists through the 4th Asia Pacific Tele-Ophthalmology Society (APTOS) Symposium
Currently, Aravind technicians travel to these rural areas to capture images and then rely on highly trained doctors to review the images and provide diagnosis. Their goal is to scale their efforts through technology; to gain the ability to automatically screen images for disease and provide information on how severe the condition may be.
In this synchronous Kernels-only competition, you'll build a machine learning model to speed up disease detection. You’ll work with thousands of images collected in rural areas to help identify diabetic retinopathy automatically. If successful, you will not only help to prevent lifelong blindness, but these models may be used to detect other sorts of diseases in the future, like glaucoma and macular degeneration.
Get started today!",5593 files,10.22 GB,This competition awarded ranking points,This competition counted towards tiers,Featured Code Competition
Recursion Cellular Image Classification,"$13,000 ",3 years ago,,"['classification', 'image', 'biology', 'research', 'categorizationaccuracy']",865,"1,066","13,232","The cost of some drugs and medical treatments has risen so high in recent years that many patients are having to go without. You can help with a classification project that could make researchers more efficient.
One of the more surprising reasons behind the cost is how long it takes to bring new treatments to market. Despite improvements in technology and science, research and development continues to lag. In fact, finding new treatments takes, on average, more than 10 years and costs hundreds of millions of dollars.
Recursion Pharmaceuticals, creators of the industry’s largest dataset of biological images, generated entirely in-house, believes AI has the potential to dramatically improve and expedite the drug discovery process. More specifically, your efforts could help them understand how drugs interact with human cells.
This competition will have you disentangling experimental noise from real biological signals. Your entry will classify images of cells under one of 1,108 different genetic perturbations. You can help eliminate the noise introduced by technical execution and environmental variation between experiments.
If successful, you could dramatically improve the industry’s ability to model cellular images according to their relevant biology. In turn, applying AI could greatly decrease the cost of treatments, and ensure these treatments get to patients faster.
This competition is a part of the NeurIPS 2019 competition track. Winners will be invited to contribute their solutions towards the workshop presentation.
Acknowledgments
Thank you to the following sponsors & supporters of this competition:
Google Cloud: Google Cloud is widely recognized as a global leader in delivering a secure, open and intelligent enterprise cloud platform. Our technology is built on Google’s private network and is the product of nearly 20 years of innovation in security, network architecture, collaboration, artificial intelligence and open source software. We offer a simply engineered set of tools and unparalleled technology across Google Cloud Platform and G Suite that help bring people, insights and ideas together. Customers across more than 150 countries trust Google Cloud to modernize their computing environment for today’s digital world.
DoiT: You have the cloud and we have your back. For nearly a decade, we’ve been helping businesses build and scale cloud solutions with our world-class cloud engineering support. We help our customers with technical support and consulting on building and operating complex large-scale distributed systems, developing better machine learning models and setting up big data solutions using Google Cloud, Amazon AWS and Microsoft Azure.
NVIDIA: NVIDIA’s (NASDAQ: NVDA) invention of the GPU in 1999 sparked the growth of the PC gaming market, redefined modern computer graphics and revolutionized parallel computing. More recently, GPU deep learning ignited modern AI — the next era of computing — with the GPU acting as the brain of computers, robots and self-driving cars that can perceive and understand the world. More information at http://nvidianews.nvidia.com.
Lambda: Lambda provides Deep Learning workstations, servers, and GPU cloud services. Lambda Deep Learning infrastructure is used by the world's leading AI research & development organizations including Apple, Microsoft, MIT, Stanford, and the US Government. To learn more, visit www.lambdalabs.com.",753091 files,48.94 GB,This competition awarded ranking points,This competition counted towards tiers,Research Prediction Competition
The 3rd YouTube-8M Video Understanding Challenge,"$25,000 ",3 years ago,,"['video data', 'custom metric']",282,340,"3,747","Imagine being able to search for the moment in any video where an adorable kitten sneezes, even though the uploader didn’t title or describe the video with such descriptive metadata. Now, apply that same concept to videos that cover important or special events like a baby’s first steps or a game-winning goal -- and now we have the ability to quickly find and share special video moments. This technology is called temporal concept localization within video and Google Research can use your help to advance the state of the art in this area.
An example of the detected action ""blowing out candles""
In most web searches, video retrieval and ranking is performed by matching query terms to metadata and other video-level signals. However, we know that videos can contain an array of topics that aren’t always characterized by the uploader, and many of these miss localizations to brief but important moments within the video. Temporal localization can enable applications such as improved video search (including search within video), video summarization and highlight extraction, action moment detection, improved video content safety, and many others.
In previous years, participants worked on advancements in video-level annotations, building both unconstrained and constrained models. In this third challenge based on the YouTube 8M dataset, Kagglers will localize video-level labels to the precise time in the video where the label actually appears, and do this at an unprecedented scale. To put it another way: at what point in the video does the cat sneeze?
If successful, your new machine learning models will significantly improve video understanding for all, by not only identifying the topics relevant to a video, but also pinpointing where in the video they appear.
This competition is being hosted by Google Research as a part of the International Conference on Computer Vision (ICCV) 2019 selected workshop session. Please refer to the YouTube 8M Large-Scale Video Understanding Workshop Page for details about the workshop.",4 files,526.32 MB,This competition awarded ranking points,This competition counted towards tiers,Research Prediction Competition
SIIM-ACR Pneumothorax Segmentation,"$30,000 ",3 years ago,,"['image', 'custom metric']","1,475","1,969",,"Imagine suddenly gasping for air, helplessly breathless for no apparent reason. Could it be a collapsed lung? In the future, your entry in this competition could predict the answer.
Pneumothorax can be caused by a blunt chest injury, damage from underlying lung disease, or most horrifying—it may occur for no obvious reason at all. On some occasions, a collapsed lung can be a life-threatening event.
Pneumothorax is usually diagnosed by a radiologist on a chest x-ray, and can sometimes be very difficult to confirm. An accurate AI algorithm to detect pneumothorax would be useful in a lot of clinical scenarios. AI could be used to triage chest radiographs for priority interpretation, or to provide a more confident diagnosis for non-radiologists.
The Society for Imaging Informatics in Medicine (SIIM) is the leading healthcare organization for those interested in the current and future use of informatics in medical imaging. Their mission is to advance medical imaging informatics across the enterprise through education, research, and innovation in a multi-disciplinary community. Today, they need your help.
In this competition, you’ll develop a model to classify (and if present, segment) pneumothorax from a set of chest radiographic images. If successful, you could aid in the early recognition of pneumothoraces and save lives.
If you’re up for the challenge, take a deep breath, and get started now.
Note: As specified on the Data Page, the dataset must be retrieved from Cloud Healthcare. Review this tutorial (or in pdf format) for instructions on how to do so.
Acknowledgments
SIIM Machine Learning Committee Co-Chairs, Steven G. Langer, PhD, CIIP and George Shih, MD, MS for tirelessly leading this effort and making the challenge possible in such a short period of time.
SIIM Machine Learning Committee Members for their dedication in annotating the dataset, helping to define the most useful metrics and running tests to prepare the challenge for launch.
SIIM Hackathon Committee, especially Mohannad Hussain, for their crucial technical support with data conversion.
American College of Radiology (ACR), @RadiologyACR: For Co-hosting the challenge and Co-sponsoring the Prizes
Society of Thoracic Radiology (STR), @thoracicrad: For their unparalleled expertise in adjudicating the dataset
MD.ai: For providing the annotation tool and helping with the first layer of annotations",3209 files,439.11 MB,This competition awarded ranking points,This competition counted towards tiers,Featured Prediction Competition
Open Images 2019 - Object Detection,"$25,000 ",4 years ago,,"['image', 'computer vision', 'custom metric']",558,697,"7,375","Introduction
Computer vision has advanced considerably but is still challenged in matching the precision of human perception.
Open Images is a collaborative release of ~9 million images annotated with image-level labels, object bounding boxes, object segmentation masks, and visual relationships. This uniquely large and diverse dataset is designed to spur state of the art advances in analyzing and understanding images.
This year’s Open Images V5 release enabled the second Open Images Challenge to include the following 3 tracks:
Object detection track for detecting bounding boxes around object instances, relaunched from 2018.
Visual relationship detection track for detecting pairs of objects in particular relations, also relaunched from 2018.
Instance segmentation track for segmenting masks of objects in images, brand new for 2019.
Google AI hopes that having a single dataset with unified annotations for image classification, object detection, visual relationship detection, and instance segmentation will stimulate progress towards genuine scene understanding.
Object Detection Track
In this track of the Challenge, you are asked to predict a tight bounding box around object instances.
The training set contains 12.2M bounding-boxes across 500 categories on 1.7M images. The boxes have been largely manually drawn by professional annotators to ensure accuracy and consistency. The images are very diverse and often contain complex scenes with several objects (7 per image on average).

Example annotations. Left: Mark Paul Gosselaar plays the guitar by Rhys A. Right: the house by anita kluska. Both images used under CC BY 2.0 license.
Please refer to the Open Images 2019 Challenge page for additional details. The challenge contains a total of 3 tracks, which are linked above in the introduction. You are invited to explore and enter as many tracks as interest you.
The results of this Challenge will be presented at a workshop at the International Conference on Computer Vision.
We are excited to partner with Open Images for this second year of competitions. See link here for last year’s Object Detection competition.",100000 files,10.46 GB,This competition awarded 0.5X ranking points,This competition counted towards tiers,Research Prediction Competition
Open Images 2019 - Visual Relationship,"$25,000 ",4 years ago,,"['image', 'computer vision', 'custom metric']",200,246,"2,198","Introduction
Computer vision has advanced considerably but is still challenged in matching the precision of human perception.
Open Images is a collaborative release of ~9 million images annotated with image-level labels, object bounding boxes, object segmentation masks, and visual relationships. This uniquely large and diverse dataset is designed to spur state of the art advances in analyzing and understanding images.
This year’s Open Images V5 release enabled the second Open Images Challenge to include the following 3 tracks:
Object detection track for detecting bounding boxes around object instances, relaunched from 2018.
Visual relationship detection track for detecting pairs of objects in particular relations, also relaunched from 2018.
Instance segmentation track for segmenting masks of objects in images, brand new for 2019.
Google AI hopes that having a single dataset with unified annotations for image classification, object detection, visual relationship detection, and instance segmentation will stimulate progress towards genuine scene understanding.
Visual Relationship Track
In this track of the Challenge, you are asked to detect pairs of objects and the relationships that connect them.
The training set contains 329 relationship triplets with 375k training samples. These include both human-object relationships (e.g. ""woman playing guitar"", ""man holding microphone""), object-object relationships (e.g. ""beer on table"", ""dog inside car""), and also considers object-attribute relationships (e.g.""handbag is made of leather"" and ""bench is wooden"").

Left: Example of ‘man playing guitar’ - Radiofiera - Villa Cordellina Lombardi, Montecchio Maggiore (VI) - agosto 2010 by Andrea Sartorati. Right: Example of ‘chair at table’ - Epic Fireworks - Loads A Room by Epic Fireworks
Please refer to the Open Images 2019 Challenge page for additional details. The challenge contains a total of 3 tracks, which are linked above in the introduction. You are invited to explore and enter as many tracks as interest you.
The results of this Challenge will be presented at a workshop at the International Conference on Computer Vision.
We are excited to partner with Open Images for this second year of competitions. See link here for last year’s Visual Representation Detection competition.",100000 files,10.48 GB,This competition awarded 0.5X ranking points,This competition counted towards tiers,Research Prediction Competition
Predicting Molecular Properties,"$30,000 ",4 years ago,,"['tabular', 'regression', 'chemistry', 'custom metric']","2,737","3,296","47,719","Think you can use your data science smarts to make big predictions at a molecular level?
This challenge aims to predict interactions between atoms. Imaging technologies like MRI enable us to see and understand the molecular composition of tissues. Nuclear Magnetic Resonance (NMR) is a closely related technology which uses the same principles to understand the structure and dynamics of proteins and molecules.
Researchers around the world conduct NMR experiments to further understanding of the structure and dynamics of molecules, across areas like environmental science, pharmaceutical science, and materials science.
This competition is hosted by members of the CHemistry and Mathematics in Phase Space (CHAMPS) at the University of Bristol, Cardiff University, Imperial College and the University of Leeds. Winning teams will have an opportunity to partner with this multi-university research program on an academic publication
Your Challenge
In this competition, you will develop an algorithm that can predict the magnetic interaction between two atoms in a molecule (i.e., the scalar coupling constant).
Once the competition finishes, CHAMPS would like to invite the top teams to present their work, discuss the details of their models, and work with them to write a joint research publication which discusses an open-source implementation of the solution.
About Scalar Coupling
Using NMR to gain insight into a molecule’s structure and dynamics depends on the ability to accurately predict so-called “scalar couplings”. These are effectively the magnetic interactions between a pair of atoms. The strength of this magnetic interaction depends on intervening electrons and chemical bonds that make up a molecule’s three-dimensional structure.
Using state-of-the-art methods from quantum mechanics, it is possible to accurately calculate scalar coupling constants given only a 3D molecular structure as input. However, these quantum mechanics calculations are extremely expensive (days or weeks per molecule), and therefore have limited applicability in day-to-day workflows.
A fast and reliable method to predict these interactions will allow medicinal chemists to gain structural insights faster and cheaper, enabling scientists to understand how the 3D chemical structure of a molecule affects its properties and behavior.
Ultimately, such tools will enable researchers to make progress in a range of important problems, like designing molecules to carry out specific cellular tasks, or designing better drug molecules to fight disease.
Join the CHAMPS Scalar Coupling challenge to apply predictive analytics to chemistry and chemical biology.",130798 files,1.22 GB,This competition awarded ranking points,This competition counted towards tiers,Featured Prediction Competition
Instant Gratification,"$5,000 ",4 years ago,3 years ago,"['tabular', 'binary classification', 'auc']","1,818","2,021","35,776","Welcome to Instant (well, almost) Gratification!
In 2015, Kaggle introduced Kernels as a resource to competition participants. It was a controversial decision to add a code-sharing tool to a competitive coding space. We thought it was important to make Kaggle more than a place where competitions are solved behind closed digital doors. Since then, Kernels has grown from its infancy--essentially a blinking cursor in a docker container--into its teenage years. We now have more compute, longer runtimes, better datasets, GPUs, and an improved interface.
We have iterated and tested several Kernels-only (KO) competition formats with a true holdout test set, in particular deploying them when we would have otherwise substituted a two-stage competition. However, the experience of submitting to a Kernels-only competition has typically been asynchronous and imperfect; participants wait many days after a competition has concluded for their selected Kernels to be rerun on the holdout test dataset, the leaderboard updated, and the winners announced. This flow causes heartbreak to participants whose Kernels fail on the unseen test set, leaving them with no way to correct tiny errors that spoil months of hard work.
Say Hello to Synchronous KO
We're now pleased to announce general support for a synchronous Kernels-only format. When you submit from a Kernel, Kaggle will run the code against both the public test set and private test set in real time. This small-but-substantial tweak improves the experience for participants, the host, and Kaggle:
With a truly withheld test set, we are practicing proper, rigorous machine learning.
We will be able to offer more varieties of competitions and intend to run many fewer confusing two-stage competitions.
You will be able to see if your code runs successfully on the withheld test set and have the leeway to intervene if it fails.
We will run all submissions against the private data, not just selected ones. Participants will get the complete and familiar public/private scores available in a traditional competition.
The final leaderboard can be released at the end of the competition, without the delay of rerunning Kernels.
This competition is a low-stakes, trial-run introduction to our new synchronous KO implementation. We want to test that the process goes smoothly and gather feedback on your experiences. While it may feel like a normal KO competition, there are complicated new mechanics in play, such as the selection logic of Kernels that are still running when the deadline passes.
Since the competition also presents an authentic machine learning problem, it will also award Kaggle medals and points. Have fun, good luck, and welcome to the world of synchronous Kernels competitions!",3 files,972.6 MB,This competition awarded ranking points,This competition counted towards tiers,Featured Code Competition
Northeastern SMILE Lab - Recognizing Faces in the Wild,,4 years ago,,"['image', 'psychology', 'auc']",522,573,"6,911","Do you have your father’s nose?
Blood relatives often share facial features. Now researchers at Northeastern University want to improve their algorithm for facial image classification to bridge the gap between research and other familial markers like DNA results. That will be your challenge in this new Kaggle competition.
An automatic kinship classifier has been in the works at Northeastern since 2010. Yet this technology remains largely unseen in practice for a couple of reasons:
1. Existing image databases for kinship recognition tasks aren't large enough to capture and reflect the true data distributions of the families of the world.
2. Many hidden factors affect familial facial relationships, so a more discriminant model is needed than the computer vision algorithms used most often for higher-level categorizations (e.g. facial recognition or object classification).
In this competition, you’ll help researchers build a more complex model by determining if two people are blood-related based solely on images of their faces. If you think you can get it ""on the nose,"" this competition is for you.
The SMILE Lab at Northeastern focuses on the frontier research of applied machine learning, social media analytics, human-computer interaction, and high-level image and video understanding. Their research is driven by the explosion of diverse multimedia from the Internet, including both personal and publicly-available photos and videos. They start by treating fundamental theory from learning algorithms as the soul of machine intelligence and arm it with visual perception.",12 files,421.03 MB,This competition did not award ranking points,This competition did not count towards tiers,Playground Prediction Competition
Data Science for Good: City of Los Angeles,"$15,000 ",4 years ago,3 years ago,"['image', 'nlp', 'text', 'employment']",,,,"Data Science for Good: City of Los Angeles
Help the City of Los Angeles to structure and analyze its job descriptions
The City of Los Angeles faces a big hiring challenge: 1/3 of its 50,000 workers are eligible to retire by July of 2020. The city has partnered with Kaggle to create a competition to improve the job bulletins that will fill all those open positions.
Problem Statement
The content, tone, and format of job bulletins can influence the quality of the applicant pool. Overly-specific job requirements may discourage diversity. The Los Angeles Mayor’s Office wants to reimagine the city’s job bulletins by using text analysis to identify needed improvements.
The goal is to convert a folder full of plain-text job postings into a single structured CSV file and then to use this data to: (1) identify language that can negatively bias the pool of applicants; (2) improve the diversity and quality of the applicant pool; and/or (3) make it easier to determine which promotions are available to employees in each job class.
How to Participate
Accept the Rules
Accept the competition rules.
Make Your Submission
Follow the submission instructions.
WIth your help, Los Angeles will overcome a wave of retirements and fill those jobs with a strong and diverse workforce. Good luck and happy Kaggling!
Do you think companies can find better candidates by improving their job postings? We hope to create an open-sourced body of work focused on this topic by hosting another Data Science for Good competition, this time in partnership with the City of Los Angeles.",837 files,38.81 MB,,,Analytics Competition
iMaterialist (Fashion) 2019 at FGVC6,,4 years ago,,['custom metric'],241,298,"3,494","Designers know what they are creating, but what, and how, do people really wear their products? What combinations of products are people using? In this competition, we challenge you to develop algorithms that will help with an important step towards automatic product detection – to accurately assign segmentations and attribute labels for fashion images.
Visual analysis of clothing is a topic that has received increasing attention in recent years. Being able to recognize apparel products and associated attributes from pictures could enhance the shopping experience for consumers, and increase work efficiency for fashion professionals.
We present a new clothing dataset with the goal of introducing a novel fine-grained segmentation task by joining forces between the fashion and computer vision communities. The proposed task unifies both categorization and segmentation of rich and complete apparel attributes, an important step toward real-world applications.
While early work in computer vision addressed related clothing recognition tasks, these are not designed with fashion insiders’ needs in mind, possibly due to the research gap in fashion design and computer vision. To address this, we first propose a fashion taxonomy built by fashion experts, informed by product description from the internet. To capture the complex structure of fashion objects and ambiguity in descriptions obtained from crawling the web, our standardized taxonomy contains 46 apparel objects (27 main apparel items and 19 apparel parts), and 92 related fine-grained attributes. Secondly, a total of 50K clothing images (10K with both segmentation and fine-grained attributes, 40K with apparel instance segmentation) in daily-life, celebrity events, and online shopping are labeled by both domain experts and crowd workers for fine-grained segmentation.
Individuals/Teams with top submissions will be invited to present their work live at the FGVC6 workshop at the Conference on Computer Vision and Pattern Recognition (CVPR) 2019
Checkout the iMaterialist-Fashion Competition Github repo for the specifics of the dataset.
Acknowledgments
The iMat-Fashion Challenge 2019 is sponsored by Google AI, CVDF, Samasource and Fashionpedia.",48391 files,23.53 GB,This competition awarded ranking points,This competition counted towards tiers,Research Prediction Competition
Google Landmark Recognition 2019,"$25,000 ",4 years ago,,['custom metric'],281,"1,743",,"Did you ever go through your vacation photos and ask yourself: What is the name of this temple I visited in China? Who created this monument I saw in France? Landmark recognition can help! This technology can predict landmark labels directly from image pixels, to help people better understand and organize their photo collections.
Today, a great obstacle to landmark recognition research is the lack of large annotated datasets. In this competition, we present the largest worldwide dataset to date, to foster progress in this problem. This competition challenges Kagglers to build models that recognize the correct landmark (if any) in a dataset of challenging test images.
Many Kagglers are familiar with image classification challenges like the ImageNet Large Scale Visual Recognition Challenge (ILSVRC), which aims to recognize 1K general object categories. Landmark recognition is a little different from that: it contains a much larger number of classes (there are more than 200K classes in this challenge), and the number of training examples per class may not be very large. Landmark recognition is challenging in its own way.
This is the second edition of this challenge. Compared to the first edition, the new dataset is more comprehensive and diverse. See the Data tab for more in-depth discussion on the new released dataset.
This challenge is organized in conjunction with the Landmark Retrieval Challenge. In particular, note that the test set for both challenges is the same, to encourage participants to compete in both. We encourage participants to use the training data from the recognition challenge (either from this year’s or last year’s dataset) to develop models which could be useful for the retrieval challenge.",1 files,3.06 MB,This competition awarded ranking points,This competition counted towards tiers,Research Prediction Competition
Google Landmark Retrieval 2019,"$25,000 ",4 years ago,,['map@{k}'],144,946,,"Image retrieval is a fundamental problem in computer vision: given a query image, can you find similar images in a large database? This is especially important for query images containing landmarks, which accounts for a large portion of what people like to photograph.
In this competition, Kagglers are given query images and, for each query, are expected to retrieve all database images containing the same landmarks (if any). The competition will proceed in two phases: The 1st phase will use the same test and index sets as last year, while for phase 2 we will release a completely new dataset that contains 700K images with more than 100K unique landmarks. We hope that this release will accelerate progress in this important research problem.
This challenge is organized in conjunction with the Landmark Recognition Challenge. In particular, note that the test set for both challenges is the same, to encourage participants to compete in both. We also encourage participants to use the training data from the recognition challenge (either from this year’s or last year’s dataset) to develop models which could be useful for the retrieval challenge.",1 files,8 MB,This competition awarded ranking points,This competition counted towards tiers,Research Prediction Competition
Freesound Audio Tagging 2019,"$5,000 ",4 years ago,,"['audio', 'weightedlabelrankingaverageprecision']",880,645,,"One year ago, Freesound and Google’s Machine Perception hosted an audio tagging competition challenging Kagglers to build a general-purpose auto tagging system. This year they’re back and taking the challenge to the next level with multi-label audio tagging, doubled number of audio categories, and a noisier than ever training set. If you like raising your ML game, this challenge is for you.
Here's the background: Some sounds are distinct and instantly recognizable, like a baby’s laugh or the strum of a guitar. Other sounds are difficult to pinpoint. If you close your eyes, could you tell the difference between the sound of a chainsaw and the sound of a blender?
Because of the vastness of sounds we experience, no reliable automatic general-purpose audio tagging systems exist. A significant amount of manual effort goes into tasks like annotating sound collections and providing captions for non-speech events in audiovisual content.
To tackle this problem, Freesound (an initiative by MTG-UPF that maintains a collaborative database with over 400,000 Creative Commons Licensed sounds) and Google Research’s Machine Perception Team (creators of AudioSet, a large-scale dataset of manually annotated audio events with over 500 classes) have teamed up to develop the dataset for this new competition.
To win this competition, Kagglers will develop an algorithm to tag audio data automatically using a diverse vocabulary of 80 categories.
If successful, your systems could be used for several applications, ranging from automatic labelling of sound collections to the development of systems that automatically tag video content or recognize sound events happening in real time.
Ready to raise your game? Join the competition!
Note, this competition is similar in nature to this competition with a new dataset, and multi-class labels.
Organizers
Eduardo Fonseca, MTG-UPF, Barcelona
Manoj Plakal, Google's Sound Understanding, New York
Frederic Font, MTG-UPF, Barcelona
Dan Ellis, Google's Sound Understanding, New York
This is a Kernels-only competition. Refer to Kernels Requirements for details.",6 files,26.15 GB,This competition awarded ranking points,This competition counted towards tiers,Research Code Competition
iNaturalist 2019 at FGVC6,,4 years ago,,['meanbesterroratk'],213,248,"1,457","As part of the FGVC6 workshop at CVPR 2019 we are conducting the iNat Challenge 2019 large scale species classification competition, sponsored by Microsoft. It is estimated that the natural world contains several million species of plants and animals. Without expert knowledge, many of these species are extremely difficult to accurately classify due to their visual similarity. The goal of this competition is to push the state of the art in automatic image classification for real world data that features a large number of fine-grained categories.
Previous versions of the challenge have focused on classifying large numbers of species. This year features a smaller number of highly similar categories captured in a wide variety of situations, from all over the world. In total, the iNat Challenge 2019 dataset contains 1,010 species, with a combined training and validation set of 268,243 images that have been collected and verified by multiple users from iNaturalist.
Teams with top submissions, at the discretion of the workshop organizers, will be invited to present their work at the FGVC6 workshop. Participants who make a submission that beats the sample submission can fill out this form to receive $150 in Google Cloud credits.

Kaggle is excited to partner with research groups to push forward the frontier of machine learning. Research competitions make use of Kaggle's platform and experience, but are largely organized by the research group's data science team. Any questions or concerns regarding the competition data, quality, or topic will be addressed by them.",6 files,87.76 GB,This competition did not award ranking points,This competition did not count towards tiers,Research Prediction Competition
Jigsaw Unintended Bias in Toxicity Classification,"$65,000 ",4 years ago,,"['nlp', 'text', 'custom metric']","3,165","4,377",,"Can you help detect toxic comments ― and minimize unintended model bias? That's your challenge in this competition.
The Conversation AI team, a research initiative founded by Jigsaw and Google (both part of Alphabet), builds technology to protect voices in conversation. A main area of focus is machine learning models that can identify toxicity in online conversations, where toxicity is defined as anything rude, disrespectful or otherwise likely to make someone leave a discussion.
Last year, in the Toxic Comment Classification Challenge, you built multi-headed models to recognize toxicity and several subtypes of toxicity. This year's competition is a related challenge: building toxicity models that operate fairly across a diverse range of conversations.
Here’s the background: When the Conversation AI team first built toxicity models, they found that the models incorrectly learned to associate the names of frequently attacked identities with toxicity. Models predicted a high likelihood of toxicity for comments containing those identities (e.g. ""gay""), even when those comments were not actually toxic (such as ""I am a gay woman""). This happens because training data was pulled from available sources where unfortunately, certain identities are overwhelmingly referred to in offensive ways. Training a model from data with these imbalances risks simply mirroring those biases back to users.
In this competition, you're challenged to build a model that recognizes toxicity and minimizes this type of unintended bias with respect to mentions of identities. You'll be using a dataset labeled for identity mentions and optimizing a metric designed to measure unintended bias. Develop strategies to reduce unintended bias in machine learning models, and you'll help the Conversation AI team, and the entire industry, build models that work well for a wide range of conversations.
Disclaimer: The dataset for this competition contains text that may be considered profane, vulgar, or offensive.
Acknowledgments
The Conversation AI team would like to thank Civil Comments for making this dataset available publicly and the Online Hate Index Research Project at D-Lab, University of California, Berkeley, whose labeling survey/instrument informed the dataset labeling. We'd also like to thank everyone who has contributed to Conversation AI's research, especially those who took part in our last competition, the success of which led to the creation of this challenge.
This is a Kernels-only competition. Refer to Kernels Requirements for details.",8 files,2.38 GB,This competition awarded ranking points,This competition counted towards tiers,Featured Code Competition
iMet Collection 2019 - FGVC6,,4 years ago,,"['art', 'image', 'fscorebetamicro']",521,767,,"The Metropolitan Museum of Art in New York, also known as The Met, has a diverse collection of over 1.5M objects of which over 200K have been digitized with imagery. The online cataloguing information is generated by Subject Matter Experts (SME) and includes a wide range of data. These include, but are not limited to: multiple object classifications, artist, title, period, date, medium, culture, size, provenance, geographic location, and other related museum objects within The Met’s collection. While the SME-generated annotations describe the object from an art history perspective, they can also be indirect in describing finer-grained attributes from the museum-goer’s understanding. Adding fine-grained attributes to aid in the visual understanding of the museum objects will enable the ability to search for visually related objects.
About
This is an FGVCx competition hosted as part of the FGVC6 workshop at CVPR 2019. View the github page for more details.
This is a Kernels-only competition. Refer to Kernels Requirements for details.",116683 files,24.32 GB,This competition awarded ranking points,This competition counted towards tiers,Research Code Competition
Ciphertext Challenge II,,4 years ago,4 years ago,"['internet', 'text', 'categorizationaccuracy']",74,79,173,"Ciphertext Challenge II: The Challengening!
It's baaaaaaack!
In our first ciphertext competition, we hunted the wilds of the '90s-era internet. This time around, we're exploring the dark slow-broadband-y wastelands of 2011, with the Movie Review Dataset. In 2011 most of the internet hadn't even been invented yet*, so wow, you're in for a treat.
Again, simple classic ciphers have been used to encrypt this dataset. Your mission this time: to correctly match each piece of ciphertext with its corresponding piece of plaintext. Daunting! Also, there are some new ciphers in play this time, which will involve some meta-puzzling. Enjoy!
Swag prizes go to the first three teams to crack all four ciphers OR to the top three teams on the LB (in case the ciphers are not all cracked). Additionally, swag prizes will be awarded to the best competition-related kernels, in both visualization and cryptanalysis, based on upvotes.
Go ahead. Get cracking!
* - This is not true.
Acknowledgements
Maas, A., Daly, R., Pham, P., Huang, D., Ng, A. and Potts, C. (2011). Learning Word Vectors for Sentiment Analysis: Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies. [online] Portland, Oregon, USA: Association for Computational Linguistics, pp. 142–150. Available here.",3 files,195.78 MB,This competition did not award ranking points,This competition did not count towards tiers,Playground Prediction Competition
iWildCam 2019 - FGVC6,,4 years ago,,"['image', 'multiclass classification', 'fscoremacro']",336,389,"2,201","Camera Traps (or Wild Cams) enable the automatic collection of large quantities of image data. Biologists all over the world use camera traps to monitor biodiversity and population density of animal species. We have recently been making strides towards automating the species classification challenge in camera traps, but as we try to expand the scope of these models from specific regions where we have collected training data to nearby areas we are faced with an interesting probem: how do you classify a species in a new region that you may not have seen in previous training data?
In order to tackle this problem, we have prepared a challenge where the training data and test data are from different regions, namely The American Southwest and the American Northwest. The species seen in each region overlap, but are not identical, and the challenge is to classify the test species correctly. To this end, we will allow training on our American Southwest data (from CaltechCameraTraps), on iNaturalist 2017/2018 data, and on simulated data generated from Microsoft AirSim. We have provided a taxonomy file mapping our classes into the iNat taxonomy.
This is an FGVCx competition as part of the FGVC6 workshop at CVPR 2019, and is sponsored by Microsoft AI for Earth. There is a github page for the competition here. Please open an issue if you have questions or problems with the dataset.
If you use this dataset in publication, please cite:
@article{beery2019iwildcam,
 title={The iWildCam 2019 Challenge Dataset},
 author={Beery, Sara and Morris, Dan and Perona, Pietro},
 journal={arXiv preprint arXiv:1907.07617},
 year={2019}
}

Kaggle is excited to partner with research groups to push forward the frontier of machine learning. Research competitions make use of Kaggle's platform and experience, but are largely organized by the research group's data science team. Any questions or concerns regarding the competition data, quality, or topic will be addressed by them.",5 files,46.68 GB,This competition did not award ranking points,This competition did not count towards tiers,Playground Prediction Competition
CareerCon 2019 - Help Navigate Robots,,4 years ago,,"['tabular', 'signal processing', 'robotics', 'categorizationaccuracy']","1,443","1,443","17,165","CareerCon 2019 is upon us!
CareerCon is a digital event all about landing your first data science job — and registration is now open! Ahead of the event, we have a fun competition to get you started. See below for a unique challenge and opportunity to share your resume with select CareerCon sponsors.
___________________________________
The Competition
Robots are smart… by design. To fully understand and properly navigate a task, however, they need input about their environment.
In this competition, you’ll help robots recognize the floor surface they’re standing on using data collected from Inertial Measurement Units (IMU sensors).
We’ve collected IMU sensor data while driving a small mobile robot over different floor surfaces on the university premises. The task is to predict which one of the nine floor types (carpet, tiles, concrete) the robot is on using sensor data such as acceleration and velocity. Succeed and you'll help improve the navigation of robots without assistance across many different surfaces, so they won’t fall down on the job.
Special thanks for making this competition possible:
The data for this competition has been collected by Heikki Huttunen and Francesco Lomio from the Department of Signal Processing and Damoon Mohamadi, Kaan Celikbilek, Pedram Ghazi and Reza Ghabcheloo from the Department of Automation and Mechanical Engineering both from Tampere University, Finland. We at Kaggle would like thank them all for kindly donating the data that has made this competition possible!",4 files,98.67 MB,This competition did not award ranking points,This competition did not count towards tiers,Recruitment Prediction Competition
Aerial Cactus Identification,,4 years ago,3 years ago,"['earth and nature', 'image', 'plants', 'auc']","1,221","1,312","6,776","To assess the impact of climate change on Earth's flora and fauna, it is vital to quantify how human activities such as logging, mining, and agriculture are impacting our protected natural areas. Researchers in Mexico have created the VIGIA project, which aims to build a system for autonomous surveillance of protected areas. A first step in such an effort is the ability to recognize the vegetation inside the protected areas. In this competition, you are tasked with creation of an algorithm that can identify a specific type of cactus in aerial imagery.
This is a kernels-only competition, meaning you must submit predictions using Kaggle Kernels. Read the basics here.
Acknowledgments
Kaggle is hosting this competition for the machine learning community to use for fun and practice. The original version of this data can be found here, with details in the following paper:
Efren López-Jiménez, Juan Irving Vasquez-Gomez, Miguel Angel Sanchez-Acevedo, Juan Carlos Herrera-Lozada, Abril Valeria Uriarte-Arcia, Columnar Cactus Recognition in Aerial Images using a Deep Learning Approach. Ecological Informatics. 2019.
Acknowledgements to Consejo Nacional de Ciencia y Tecnología. Project cátedra 1507. Instituto Politècnico Nacional. Universidad de la Cañada. Contributors: Eduardo Armas Garca, Rafael Cano Martnez and Luis Cresencio Mota Carrera. J.I. Vasquez-Gomez, JC. Herrera Lozada. Abril Uriarte, Miguel Sanchez.",4 files,25.4 MB,This competition did not award ranking points,This competition did not count towards tiers,Playground Code Competition
Data Science for Good: CareerVillage.org,"$15,000 ",4 years ago,4 years ago,"['education', 'people']",,,,"Welcome
In this competition you'll notice there isn't a leaderboard, and you are not required to develop a predictive model. This isn't a traditional supervised Kaggle machine learning competition.
CareerVillage.org is a nonprofit that crowdsources career advice for underserved youth. Founded in 2011 in four classrooms in New York City, the platform has now served career advice from 25,000 volunteer professionals to over 3.5M online learners. The platform uses a Q&A style similar to StackOverflow or Quora to provide students with answers to any question about any career.
In this Data Science for Good challenge, CareerVillage.org, in partnership with Google.org, is inviting you to help recommend questions to appropriate volunteers. To support this challenge, CareerVillage.org has supplied five years of data.
Problem Statement
The U.S. has almost 500 students for every guidance counselor. Underserved youth lack the network to find their career role models, making CareerVillage.org the only option for millions of young people in America and around the globe with nowhere else to turn.
To date, 25,000 volunteers have created profiles and opted in to receive emails when a career question is a good fit for them. This is where your skills come in. To help students get the advice they need, the team at CareerVillage.org needs to be able to send the right questions to the right volunteers. The notifications sent to volunteers seem to have the greatest impact on how many questions are answered.
Your objective: develop a method to recommend relevant questions to the professionals who are most likely to answer them.
Criteria for Measuring Solutions
Performance: How well does the solution match professionals to the questions they would be motivated to answer? CareerVillage.org will not be able to live-test every submission, so a strong entry will clearly articulate why it will be effective at motivating answers.
Easy to implement: The CareerVillage.org team wants to put the winning submissions to work, quickly. A good entry will be well documented and easy to test in production.
Extensibility: In the future, CareerVillage.org aims to add more data features and to accommodate new objectives. Winning submissions should allow for this and other augmentations to be added in the future.",15 files,436.59 MB,,,Analytics Competition
Google Cloud & NCAA® ML Competition 2019-Men's,"$25,000 ",4 years ago,,"['sports', 'basketball', 'logloss']",862,947,"1,545","As a result of the continued collaboration between Google Cloud and the NCAA, the sixth annual Kaggle-backed March Madness competition is underway! Another year, another chance to anticipate the upsets, call the probabilities, and put your bracketology skills to the leaderboard test. Kagglers will join the millions of fans who attempt to forecast the outcomes of March Madness during this year's NCAA Division I Men’s and Women’s Basketball Championships. But unlike most fans, you will pick your bracket using a combination of NCAA’s historical data and your computing power, while the ground truth unfolds on national television.
In the first stage of the competition, Kagglers will rely on results of past tournaments to build and test models. We encourage you to post any useful external data as a dataset. In the second stage, competitors will forecast outcomes of all possible matchups in the 2019 NCAA Division I Men’s and Women’s Basketball Championships. You don't need to participate in the first stage to enter the second. The first stage exists to incentivize model building and provide a means to score predictions. The real competition is forecasting the 2019 results.
As the official public cloud provider of the NCAA, Google is proud to provide a competition to help participants strengthen their knowledge of basketball, statistics, data modeling, and cloud technology. As part of its journey to the cloud, the NCAA has migrated 80+ years of historical and play-by-play data, from 90 championships and 24 sports, to Google Cloud Platform (GCP). The NCAA has tapped into decades of historical basketball data using BigQuery, Cloud Spanner, Datalab, Cloud Machine Learning and Cloud Dataflow, to power the analysis of team and player performance. The mission of the NCAA has long been about serving the needs of schools, their teams and students. Google Cloud is proud to support that mission by helping the NCAA use data and machine learning to better engage with its millions of fans, 500,000 student-athletes, and more than 19,000 teams. Game on!
This page is for the NCAA Division I Men's tournament. Check out the NCAA Division I Women's tournament here.",68 files,1.91 GB,This competition awarded ranking points,This competition counted towards tiers,Featured Prediction Competition
Google Cloud & NCAA® ML Competition 2019-Women's,"$25,000 ",4 years ago,,"['sports', 'basketball', 'logloss']",497,524,925,"As a result of the continued collaboration between Google Cloud and the NCAA®, the sixth annual Kaggle-backed March Madness competition is underway! Another year, another chance to anticipate the upsets, call the probabilities, and put your bracketology skills to the leaderboard test. Kagglers will join the millions of fans who attempt to forecast the outcomes of March Madness during this year's NCAA Division I Men’s and Women’s Basketball Championships. But unlike most fans, you will pick your bracket using a combination of NCAA’s historical data and your computing power, while the ground truth unfolds on national television.
In the first stage of the competition, Kagglers will rely on results of past tournaments to build and test models. We encourage you to post any useful external data as a dataset. In the second stage, competitors will forecast outcomes of all possible matchups in the 2019 NCAA Division I Men’s and Women’s Basketball Championships. You don't need to participate in the first stage to enter the second. The first stage exists to incentivize model building and provide a means to score predictions. The real competition is forecasting the 2019 results.
As the official public cloud provider of the NCAA, Google Cloud is proud to provide a competition to help participants strengthen their knowledge of basketball, statistics, data modeling, and cloud technology. As part of its journey to the cloud, the NCAA has migrated 80+ years of historical and play-by-play data, from 90 championships and 24 sports, to Google Cloud Platform (GCP). The NCAA has tapped into decades of historical basketball data using BigQuery, Cloud Spanner, Datalab, Cloud Machine Learning and Cloud Dataflow, to power the analysis of team and player performance. The mission of the NCAA has long been about serving the needs of schools, their teams and students. Google Cloud is proud to support that mission by helping the NCAA use data and machine learning to better engage with its millions of fans, 500,000 student-athletes and more than 19,000 teams. Game on!
This page is for the NCAA Division I Women's tournament. Check out the NCAA Division I Men's tournament here.",24 files,19.63 MB,This competition awarded ranking points,This competition counted towards tiers,Featured Prediction Competition
Santander Customer Transaction Prediction,"$65,000 ",4 years ago,,"['tabular', 'binary classification', 'banking', 'auc']","8,751","9,787","104,121","At Santander our mission is to help people and businesses prosper. We are always looking for ways to help our customers understand their financial health and identify which products and services might help them achieve their monetary goals.
Our data science team is continually challenging our machine learning algorithms, working with the global data science community to make sure we can more accurately identify new ways to solve our most common challenge, binary classification problems such as: is a customer satisfied? Will a customer buy this product? Can a customer pay this loan?
In this challenge, we invite Kagglers to help us identify which customers will make a specific transaction in the future, irrespective of the amount of money transacted. The data provided for this competition has the same structure as the real data we have available to solve this problem.",3 files,606.35 MB,This competition awarded ranking points,This competition counted towards tiers,Featured Prediction Competition
Don't Overfit! II,,4 years ago,4 years ago,"['tabular', 'binary classification', 'auc']","2,315","2,441","35,262","Long ago, in the distant, fragrant mists of time, there was a competition…
It was not just any competition.
It was a competition that challenged mere mortals to model a 20,000x200 matrix of continuous variables using only 250 training samples… without overfitting.
Data scientists ― including Kaggle's very own Will Cukierski ― competed by the hundreds. Legends were made. (Will took 5th place, and eventually ended up working at Kaggle!) People overfit like crazy. It was a Kaggle-y, data science-y madhouse.
So… we're doing it again.
Don't Overfit II: The Overfittening
This is the next logical step in the evolution of weird competitions. Once again we have 20,000 rows of continuous variables, and a mere handful of training samples. Once again, we challenge you not to overfit. Do your best, model without overfitting, and add, perhaps, to your own legend.
In addition to bragging rights, the winner also gets swag. Enjoy!
Acknowledgments
We hereby salute the hard work that went into the original competition, created by Phil Brierly. Thank you!",3 files,38.6 MB,This competition did not award ranking points,This competition did not count towards tiers,Playground Prediction Competition
TMDB Box Office Prediction,,4 years ago,4 years ago,"['tabular', 'movies and tv shows', 'rmsle']","1,395","1,615","18,974","We're going to make you an offer you can't refuse: a Kaggle competition!
In a world… where movies made an estimated $41.7 billion in 2018, the film industry is more popular than ever. But what movies make the most money at the box office? How much does a director matter? Or the budget? For some movies, it's ""You had me at 'Hello.'"" For others, the trailer falls short of expectations and you think ""What we have here is a failure to communicate.""
In this competition, you're presented with metadata on over 7,000 past films from The Movie Database to try and predict their overall worldwide box office revenue. Data points provided include cast, crew, plot keywords, budget, posters, release dates, languages, production companies, and countries. You can collect other publicly available data to use in your model predictions, but in the spirit of this competition, use only data that would have been available before a movie's release.
Join in, ""make our day"", and then ""you've got to ask yourself one question: 'Do I feel lucky?'""",3 files,70.24 MB,This competition did not award ranking points,This competition did not count towards tiers,Playground Prediction Competition
Gendered Pronoun Resolution,"$25,000 ",4 years ago,,"['nlp', 'text', 'multiclassloss']",838,617,,"Can you help end gender bias in pronoun resolution?
Pronoun resolution is part of coreference resolution, the task of pairing an expression to its referring entity. This is an important task for natural language understanding, and the resolution of ambiguous pronouns is a longstanding challenge.
Unfortunately, recent studies have suggested gender bias among state-of-the-art coreference resolvers. Google AI Language aims to improve gender-fairness in modeling by releasing the Gendered Ambiguous Pronouns (GAP) dataset, containing gender-balanced pronouns (50% of its examples containing feminine pronouns, and 50% containing masculine pronouns).
In this two-stage competition, Kagglers are challenged to build pronoun resolution systems that perform equally well regardless of pronoun gender. Stage two's final evaluation will use a new dataset following the same format. To encourage gender-fair modeling, the ratio of masculine to feminine examples in the official test data will not be known ahead of time.
----------
Please cite the original paper if you use GAP in your work:
@inproceedings{webster2018gap,
  title =     {Mind the GAP: A Balanced Corpus of Gendered Ambiguous Pronouns},
  author =    {Webster, Kellie and Recasens, Marta and Axelrod, Vera and Baldridge, Jason},
  booktitle = {Transactions of the ACL},
  year =      {2018},
  pages =     {to appear},
}",4 files,7.84 MB,This competition awarded ranking points,This competition counted towards tiers,Research Prediction Competition
LANL Earthquake Prediction,"$50,000 ",4 years ago,,"['earth science', 'physics', 'signal processing', 'mae']","4,516","5,454","59,891","Forecasting earthquakes is one of the most important problems in Earth science because of their devastating consequences. Current scientific studies related to earthquake forecasting focus on three key points: when the event will occur, where it will occur, and how large it will be.
In this competition, you will address when the earthquake will take place. Specifically, you’ll predict the time remaining before laboratory earthquakes occur from real-time seismic data.
If this challenge is solved and the physics are ultimately shown to scale from the laboratory to the field, researchers will have the potential to improve earthquake hazard assessments that could save lives and billions of dollars in infrastructure.
This challenge is hosted by Los Alamos National Laboratory which enhances national security by ensuring the safety of the U.S. nuclear stockpile, developing technologies to reduce threats from weapons of mass destruction, and solving problems related to energy, environment, infrastructure, health, and global security concerns.
Acknowledgments:
Geophysics Group: The competition builds on initial work from Bertrand Rouet-Leduc, Claudia Hulbert, and Paul Johnson. B. Rouet-Leduc prepared the data for the competition.
Department of Geosciences: Data are from experiments performed by Chas Bolton, Jacques Riviere, Paul Johnson and Prof. Chris Marone.
Department of Physics & Astronomy: This competition stemmed from the DOE Council workshop “Information is in the Noise: Signatures of Evolving Fracture and Fracture Networks” held March 2018 that was organized by Prof. Laura J. Pyrak-Nolte.
Department of Energy
Office of Science, Basic Energy Sciences, Chemical Sciences, Geosciences and Biosciences Division: The Geosciences core research.
Photo by Nik Shuliahin on Unsplash",2626 files,10.42 GB,This competition awarded ranking points,This competition counted towards tiers,Research Prediction Competition
PetFinder.my Adoption Prediction,"$25,000 ",4 years ago,,"['image', 'text', 'quadraticweightedkappa']","2,023","3,136",,"Millions of stray animals suffer on the streets or are euthanized in shelters every day around the world. If homes can be found for them, many precious lives can be saved — and more happy families created.
PetFinder.my has been Malaysia’s leading animal welfare platform since 2008, with a database of more than 150,000 animals. PetFinder collaborates closely with animal lovers, media, corporations, and global organizations to improve animal welfare.
Animal adoption rates are strongly correlated to the metadata associated with their online profiles, such as descriptive text and photo characteristics. As one example, PetFinder is currently experimenting with a simple AI tool called the Cuteness Meter, which ranks how cute a pet is based on qualities present in their photos.
In this competition you will be developing algorithms to predict the adoptability of pets - specifically, how quickly is a pet adopted? If successful, they will be adapted into AI tools that will guide shelters and rescuers around the world on improving their pet profiles' appeal, reducing animal suffering and euthanization.
Top participants may be invited to collaborate on implementing their solutions into AI tools for assessing and improving pet adoption performance, which will benefit global animal welfare.
Important Note
Be aware that this is being run as a Kernels Only Competition, requiring that all submissions be made via a Kernel output.
Photo by Krista Mangulsone on Unsplash",163869 files,2.48 GB,This competition awarded ranking points,This competition counted towards tiers,Featured Code Competition
VSB Power Line Fault Detection,"$25,000 ",4 years ago,,"['tabular', 'binary classification', 'signal processing', 'matthewscorrelationcoefficient']","1,445","1,589","19,481","Medium voltage overhead power lines run for hundreds of miles to supply power to cities. These great distances make it expensive to manually inspect the lines for damage that doesn't immediately lead to a power outage, such as a tree branch hitting the line or a flaw in the insulator. These modes of damage lead to a phenomenon known as partial discharge — an electrical discharge which does not bridge the electrodes between an insulation system completely. Partial discharges slowly damage the power line, so left unrepaired they will eventually lead to a power outage or start a fire.
Your challenge is to detect partial discharge patterns in signals acquired from these power lines with a new meter designed at the ENET Centre at VŠB. Effective classifiers using this data will make it possible to continuously monitor power lines for faults.
ENET Centre researches and develops renewable energy resources with the goal of reducing or eliminating harmful environmental impacts. Their efforts focus on developing technology solutions around transportation and processing of energy raw materials.
By developing a solution to detect partial discharge you’ll help reduce maintenance costs, and prevent power outages.",5 files,12.62 GB,This competition awarded ranking points,This competition counted towards tiers,Featured Prediction Competition
Reducing Commercial Aviation Fatalities,,4 years ago,,['multiclassloss'],178,193,"1,333","Most flight-related fatalities stem from a loss of “airplane state awareness.” That is, ineffective attention management on the part of pilots who may be distracted, sleepy or in other dangerous cognitive states.
Your challenge is to build a model to detect troubling events from aircrew’s physiological data. You'll use data acquired from actual pilots in test situations, and your models should be able to run calculations in real time to monitor the cognitive states of pilots. With your help, pilots could then be alerted when they enter a troubling state, preventing accidents and saving lives.
Reducing aircraft fatalities is just one of the complex problems that Booz Allen Hamilton has been solving for business, government, and military leaders for over 100 years. Through devotion, candor, courage, and character, they produce original solutions where there are no roadmaps. Now you can help them find answers, save lives, and change the world.",3 files,6.32 GB,This competition did not award ranking points,This competition did not count towards tiers,Playground Prediction Competition
20 Newsgroups Ciphertext Challenge,,4 years ago,4 years ago,"['text', 'multiclass classification', 'fscoremacro']",142,145,"1,139","This isn't your classic decoder ring puzzle found in a cereal box. There's a twist.
Welcome to the Ciphertext Challenge! In this competition, we've encrypted parts of a well-known dataset -- the 20 Newsgroups dataset -- with several simple, classic ciphers. This dataset is commonly used as a multi-class and NLP sample set, noted for its small size, varied nature, and the first-hand look it offers into the deep existential horrors of the 90s-era internet. With 20 fairly distinct classes and lots of clues, it allows for a wide variety of successful approaches.
We've made the problem a little harder to solve.
Fabulous Kaggle swag will go to the top competitors - the highest-scoring teams (which might be the first to crack the code!), and the most popular kernel. Note that this is a short competition, so use your submissions wisely.
* = Note: It is possible to apply a number of techniques using ONLY the ciphertext.
Acknowledgements
Kaggle is hosting this competition for the machine learning community to use for fun and practice.
You can view and download the unencrypted dataset from Jason Rennie's homepage. In the words of the host:
The 20 Newsgroups data set is a collection of approximately 20,000 newsgroup documents, partitioned (nearly) evenly across 20 different newsgroups. To the best of my knowledge, it was originally collected by Ken Lang, probably for his Newsweeder: Learning to filter netnews paper, though he does not explicitly mention this collection.
If you use the dataset in a scientific publication, please reference (at a minimum) the above website.
Photo by U.S. Air Force photo/Don Branum",3 files,36.97 MB,This competition did not award ranking points,This competition did not count towards tiers,Playground Prediction Competition
Microsoft Malware Prediction,"$25,000 ",4 years ago,,['auc'],"2,410","2,859","43,406","The malware industry continues to be a well-organized, well-funded market dedicated to evading traditional security measures. Once a computer is infected by malware, criminals can hurt consumers and enterprises in many ways.
With more than one billion enterprise and consumer customers, Microsoft takes this problem very seriously and is deeply invested in improving security.
As one part of their overall strategy for doing so, Microsoft is challenging the data science community to develop techniques to predict if a machine will soon be hit with malware. As with their previous, Malware Challenge (2015), Microsoft is providing Kagglers with an unprecedented malware dataset to encourage open-source progress on effective techniques for predicting malware occurrences.
Can you help protect more than one billion machines from damage BEFORE it happens?
Acknowledgements
This competition is hosted by Microsoft, Windows Defender ATP Research, Northeastern University College of Computer and Information Science, and Georgia Tech Institute for Information Security & Privacy.
Microsoft contacts
Rob McCann (Robert.McCann@microsoft.com)
Christian Seifert (chriseif@microsoft.com)
Susan Higgs (Susan.Higgs@microsoft.com)
Matt Duncan (Matthew.Duncan@microsoft.com)
Northeastern University contact
Mansour Ahmadi (m.ahmadi@northeastern.edu)
Georgia Tech contacts
Brendan Saltaformaggio (brendan@ece.gatech.edu)
Taesoo Kim (taesoo@gatech.edu)",3 files,8.47 GB,This competition awarded ranking points,This competition counted towards tiers,Research Prediction Competition
NFL Punt Analytics Competition,"$80,000 ",4 years ago,4 years ago,"['health', 'sports', 'football']",,,,"Welcome
In this challenge you'll notice there isn't a leaderboard, and you are not required to develop a predictive model. This isn't a traditional supervised Kaggle machine learning competition. Instead, this challenge asks you to use data to propose specific rule modifications for the NFL that aim to reduce the occurrence of concussions during punt plays. For more information on this challenge format, see this forum thread. This challenge is part of NFL 1st & Future, presented by Arrow Electronics – the NFL’s annual Super Bowl competition designed to spur innovation in player health, safety and performance.
The Challenge
For the 2018 season, the NFL revised their kickoff rules in an effort to reduce the risk of injury during those plays. By examining injury reports, player position and velocity data, and game video, they were able to understand the game-play circumstances that may exacerbate the risk of injury to players.
This comprehensive review showed that over the course of all games during the 2015-2017 seasons, the kickoff represented only six percent of plays but 12 percent of concussions. Players had approximately four times the risk of concussion on returned kickoffs compared to running or passing plays. The changes to the kickoff rule aim to address the components that posed the most risk, like the use of a two-man wedge.
Now, the NFL is challenging Kagglers to help them perform the same examination, this time on punt play rules. They have provided data for all punt plays from the 2016 and 2017 NFL seasons that includes player rosters, on-field position data and video data, including the plays in which a player suffered a concussion.
Your challenge is to propose specific rule modifications (e.g. changes to the initial formation, tackling techniques, blocking rules etc.), supported by data, that may reduce the occurrence of concussions during punt plays. More details on the entry criteria are available in Overview tab > Evaluation.
About The NFL
The National Football League is America's most popular sports league, comprised of 32 franchises that compete each year to win the Super Bowl, the world's biggest annual sporting event. Founded in 1920, the NFL developed the model for the successful modern sports league, including national and international distribution, extensive revenue sharing, competitive excellence, and strong franchises across the country.
The NFL is committed to advancing progress in the diagnosis, prevention and treatment of sports-related injuries. The NFL's ongoing health and safety efforts include support for independent medical research and engineering advancements and a commitment to look at anything and everything to protect players and make the game safer, including enhancements to medical protocols and improvements to how our game is taught and played.
As more is learned, the league evaluates and changes rules to evolve the game and try to improve protections for players. Since 2002 alone, the NFL has made 50 rules changes intended to eliminate potentially dangerous tactics and reduce the risk of injuries.
For more information about the NFL's health and safety efforts, please visit www.PlaySmartPlaySafe.com.
Evaluation",30 files,16.24 GB,,,Analytics Competition
Humpback Whale Identification,"$25,000 ",4 years ago,,"['image', 'animals', 'map@{k}']","2,120","2,451","37,466","After centuries of intense whaling, recovering whale populations still have a hard time adapting to warming oceans and struggle to compete every day with the industrial fishing industry for food.
To aid whale conservation efforts, scientists use photo surveillance systems to monitor ocean activity. They use the shape of whales’ tails and unique markings found in footage to identify what species of whale they’re analyzing and meticulously log whale pod dynamics and movements. For the past 40 years, most of this work has been done manually by individual scientists, leaving a huge trove of data untapped and underutilized.
In this competition, you’re challenged to build an algorithm to identify individual whales in images. You’ll analyze Happywhale’s database of over 25,000 images, gathered from research institutions and public contributors. By contributing, you’ll help to open rich fields of understanding for marine mammal population dynamics around the globe.
Note, this competition is similar in nature to this competition with an expanded and updated dataset.
We'd like to thank Happywhale for providing this data and problem. Happywhale is a platform that uses image process algorithms to let anyone to submit their whale photo and have it automatically identified.",33323 files,5.95 GB,This competition awarded ranking points,This competition counted towards tiers,Featured Prediction Competition
Elo Merchant Category Recommendation,"$50,000 ",4 years ago,,"['tabular', 'regression', 'banking', 'rmse']","4,110","4,712","81,771","Imagine being hungry in an unfamiliar part of town and getting restaurant recommendations served up, based on your personal preferences, at just the right moment. The recommendation comes with an attached discount from your credit card provider for a local place around the corner!
Right now, Elo, one of the largest payment brands in Brazil, has built partnerships with merchants in order to offer promotions or discounts to cardholders. But do these promotions work for either the consumer or the merchant? Do customers enjoy their experience? Do merchants see repeat business? Personalization is key.
Elo has built machine learning models to understand the most important aspects and preferences in their customers’ lifecycle, from food to shopping. But so far none of them is specifically tailored for an individual or profile. This is where you come in.
In this competition, Kagglers will develop algorithms to identify and serve the most relevant opportunities to individuals, by uncovering signal in customer loyalty. Your input will improve customers’ lives and help Elo reduce unwanted campaigns, to create the right experience for customers.",8 files,3.1 GB,This competition awarded ranking points,This competition counted towards tiers,Featured Prediction Competition
Don't call me turkey!,,4 years ago,4 years ago,"['tabular', 'binary classification', 'animals', 'auc']",266,277,"1,001","Hungry for a new competition? Give thanks for this opportunity to avoid those awkward family political dinner discussions and endless holiday movie marathons over the Thanksgiving break. Spend time with your Kaggle family instead to find the real turkey!
In this competition you are tasked with finding the turkey sound signature from pre-extracted audio features. A simple binary problem, or is it? What does a turkey really sound like? How many sounds are similar? Will you be able to find the turkey or will you go a-fowl?
This is a short, fun, holiday, playground competition. Please, do not ruin the fun for yourself and for everyone by using a model trained on the answers. Don't be a turkey!",3 files,11.04 MB,This competition did not award ranking points,This competition did not count towards tiers,Playground Prediction Competition
Traveling Santa 2018 - Prime Paths,"$25,000 ",4 years ago,4 years ago,"['optimization', 'custom metric']","1,867","2,042","21,235","Rudolph the red-nosed reindeer
Had some very tired hooves
But he had a job to finish
Could he do it with the shortest moves?
All of the other reindeer
Used to laugh and mock his code
They always said poor Rudolph
Couldn't handle the workload
Then one foggy Christmas Eve
Santa came to say
I see you've taken number theory
Please make this night a bit less dreary?
Then how the reindeer loved him
and each enrolled in an AI degree
Rudolph the red-nosed reindeer
We get to go to bed early!
Rudolph has always believed in working smarter, not harder. And what better way to earn the respect of Comet and Blitzen than showing the initiative to improve Santa's annual route for delivering toys on Christmas Eve?
This year, Rudolph believes he can motivate the overworked Reindeer team by wisely choosing the order in which they visit the houses on Santa's list. The houses in prime cities always leave carrots for the Reindeers alongside the usual cookies and milk. These carrots are just the sustenance the Reindeers need to keep pace. In fact, Rudolph has found that if the Reindeer team doesn't originate from a prime city exactly every 10th step, it takes the 10% longer than it normally would to make their next destination!
Can you help Rudolph solve the Traveling Santa problem subject to his carrot constraint? His team--and Santa--are counting on you!
Attributions:
Reindeer Photo: Norman Tsui
Stocking Photo: Wesley Tingey",2 files,9.22 MB,This competition awarded ranking points,This competition counted towards tiers,Featured Prediction Competition
Histopathologic Cancer Detection,,4 years ago,4 years ago,"['cancer', 'medicine', 'research', 'auc']","1,149","1,347","20,132","In this competition, you must create an algorithm to identify metastatic cancer in small image patches taken from larger digital pathology scans. The data for this competition is a slightly modified version of the PatchCamelyon (PCam) benchmark dataset (the original PCam dataset contains duplicate images due to its probabilistic sampling, however, the version presented on Kaggle does not contain duplicates).
PCam is highly interesting for both its size, simplicity to get started on, and approachability. In the authors' words:
[PCam] packs the clinically-relevant task of metastasis detection into a straight-forward binary image classification task, akin to CIFAR-10 and MNIST. Models can easily be trained on a single GPU in a couple hours, and achieve competitive scores in the Camelyon16 tasks of tumor detection and whole-slide image diagnosis. Furthermore, the balance between task-difficulty and tractability makes it a prime suspect for fundamental machine learning research on topics as active learning, model uncertainty, and explainability.
Acknowledgements
Kaggle is hosting this competition for the machine learning community to use for fun and practice. This dataset was provided by Bas Veeling, with additional input from Babak Ehteshami Bejnordi, Geert Litjens, and Jeroen van der Laak.
You may view and download the official Pcam dataset from GitHub. The data is provided under the CC0 License, following the license of Camelyon16.
If you use PCam in a scientific publication, please reference the following papers:
[1] B. S. Veeling, J. Linmans, J. Winkens, T. Cohen, M. Welling. ""Rotation Equivariant CNNs for Digital Pathology"". arXiv:1806.03962
[2] Ehteshami Bejnordi et al. Diagnostic Assessment of Deep Learning Algorithms for Detection of Lymph Node Metastases in Women With Breast Cancer. JAMA: The Journal of the American Medical Association, 318(22), 2199–2210. doi:jama.2017.14585
Photo by Ousa Chea",277485 files,7.76 GB,This competition did not award ranking points,This competition did not count towards tiers,Playground Prediction Competition
Quora Insincere Questions Classification,"$25,000 ",4 years ago,,"['binary classification', 'text', 'custom metric']","4,037","2,506",,"An existential problem for any major website today is how to handle toxic and divisive content. Quora wants to tackle this problem head-on to keep their platform a place where users can feel safe sharing their knowledge with the world.
Quora is a platform that empowers people to learn from each other. On Quora, people can ask questions and connect with others who contribute unique insights and quality answers. A key challenge is to weed out insincere questions -- those founded upon false premises, or that intend to make a statement rather than look for helpful answers.
In this competition, Kagglers will develop models that identify and flag insincere questions. To date, Quora has employed both machine learning and manual review to address this problem. With your help, they can develop more scalable methods to detect toxic and misleading content.
Here's your chance to combat online trolls at scale. Help Quora uphold their policy of “Be Nice, Be Respectful” and continue to be a place for sharing and growing the world’s knowledge.
Important Note
Be aware that this is being run as a Kernels Only Competition, requiring that all submissions be made via a Kernel output. Please read the Kernels FAQ and the data page very carefully to fully understand how this is designed.",4 files,6.56 GB,This competition awarded ranking points,This competition counted towards tiers,Featured Code Competition
PUBG Finish Placement Prediction (Kernels Only),,4 years ago,4 years ago,"['tabular', 'video games', 'mae']","1,528","1,772","12,747","So, where we droppin' boys and girls?
Battle Royale-style video games have taken the world by storm. 100 players are dropped onto an island empty-handed and must explore, scavenge, and eliminate other players until only one is left standing, all while the play zone continues to shrink.
PlayerUnknown's BattleGrounds (PUBG) has enjoyed massive popularity. With over 50 million copies sold, it's the fifth best selling game of all time, and has millions of active monthly players.
The team at PUBG has made official game data available for the public to explore and scavenge outside of ""The Blue Circle."" This competition is not an official or affiliated PUBG site - Kaggle collected data made possible through the PUBG Developer API.
You are given over 65,000 games' worth of anonymized player data, split into training and testing sets, and asked to predict final placement from final in-game stats and initial player ratings.
What's the best strategy to win in PUBG? Should you sit in one spot and hide your way into victory, or do you need to be the top shot? Let's let the data do the talking!",3 files,965.66 MB,This competition did not award ranking points,This competition did not count towards tiers,Playground Code Competition
Human Protein Atlas Image Classification,"$37,000 ",4 years ago,,"['classification', 'image data', 'macrofscore']","2,160","2,679","54,934","In this competition, Kagglers will develop models capable of classifying mixed patterns of proteins in microscope images. The Human Protein Atlas will use these models to build a tool integrated with their smart-microscopy system to identify a protein's location(s) from a high-throughput image.
Proteins are “the doers” in the human cell, executing many functions that together enable life. Historically, classification of proteins has been limited to single patterns in one or a few cell types, but in order to fully understand the complexity of the human cell, models must classify mixed patterns across a range of different human cells.
Images visualizing proteins in cells are commonly used for biomedical research, and these cells could hold the key for the next breakthrough in medicine. However, thanks to advances in high-throughput microscopy, these images are generated at a far greater pace than what can be manually evaluated. Therefore, the need is greater than ever for automating biomedical image analysis to accelerate the understanding of human cells and disease.
  Nature Methods has indicated interest in considering a paper discussing the outcome and approaches of the challenge. The Human Protein Atlas team would like to invite top performing teams to join as co-authors in the writing of this paper.
Top performing teams will also be eligible to compete for the special prize. Additional information for both the special prize and co-authoring for Nature Methods will become available through the Discussion posts once the main competition is complete.
  Acknowledgements
The Human Protein Atlas is a Sweden-based initiative aimed at mapping all human proteins in cells, tissues and organs. All the data in the knowledge resource is open access to allow anyone to pursue exploration of the human proteome. In a recent publication, the Human Protein Atlas team has demonstrated the promise of both citizen science and artificial intelligence approaches in describing the location of human proteins in images, however current results are yet to approach expert-level annotations (Sullivan et al, Nature Biotechnology, Oct 2018).",171098 files,18.72 GB,This competition awarded ranking points,This competition counted towards tiers,Featured Prediction Competition
PLAsTiCC Astronomical Classification,"$25,000 ",4 years ago,,"['tabular data', 'astronomy', 'weightedmulticlassloss']","1,089","1,320","22,851","Help some of the world's leading astronomers grasp the deepest properties of the universe.
The human eye has been the arbiter for the classification of astronomical sources in the night sky for hundreds of years. But a new facility -- the Large Synoptic Survey Telescope (LSST) -- is about to revolutionize the field, discovering 10 to 100 times more astronomical sources that vary in the night sky than we've ever known. Some of these sources will be completely unprecedented!
The Photometric LSST Astronomical Time-Series Classification Challenge (PLAsTiCC) asks Kagglers to help prepare to classify the data from this new survey. Competitors will classify astronomical sources that vary with time into different classes, scaling from a small training set to a very large test set of the type the LSST will discover.
More background information is available here.
Acknowledgements
PLAsTiCC is funded through LSST Corporation Grant Award # 2017-03 and administered by the University of Toronto. Financial support for LSST comes from the National Science Foundation (NSF) through Cooperative Agreement No. 1258333, the Department of Energy (DOE) Office of Science under Contract No. DE-AC02-76SF00515, and private funding raised by the LSST Corporation. The NSF-funded LSST Project Office for construction was established as an operating center under management of the Association of Universities for Research in Astronomy (AURA). The DOE-funded effort to build the LSST camera is managed by the SLAC National Accelerator Laboratory (SLAC).
The National Science Foundation (NSF) is an independent federal agency created by Congress in 1950 to promote the progress of science. NSF supports basic research and people to create knowledge that transforms the future.




Photo Credit: M. Park/Inigo Films/LSST/AURA/NSF",18 files,40.12 GB,This competition awarded ranking points,This competition counted towards tiers,Featured Prediction Competition
Human Protein Atlas Image Classification,"$37,000 ",4 years ago,,"['classification', 'image', 'fscoremacro']","2,160","2,679","54,934","In this competition, Kagglers will develop models capable of classifying mixed patterns of proteins in microscope images. The Human Protein Atlas will use these models to build a tool integrated with their smart-microscopy system to identify a protein's location(s) from a high-throughput image.
Proteins are “the doers” in the human cell, executing many functions that together enable life. Historically, classification of proteins has been limited to single patterns in one or a few cell types, but in order to fully understand the complexity of the human cell, models must classify mixed patterns across a range of different human cells.
Images visualizing proteins in cells are commonly used for biomedical research, and these cells could hold the key for the next breakthrough in medicine. However, thanks to advances in high-throughput microscopy, these images are generated at a far greater pace than what can be manually evaluated. Therefore, the need is greater than ever for automating biomedical image analysis to accelerate the understanding of human cells and disease.
  Nature Methods has indicated interest in considering a paper discussing the outcome and approaches of the challenge. The Human Protein Atlas team would like to invite top performing teams to join as co-authors in the writing of this paper.
Top performing teams will also be eligible to compete for the special prize. Additional information for both the special prize and co-authoring for Nature Methods will become available through the Discussion posts once the main competition is complete.
  Acknowledgements
The Human Protein Atlas is a Sweden-based initiative aimed at mapping all human proteins in cells, tissues and organs. All the data in the knowledge resource is open access to allow anyone to pursue exploration of the human proteome. In a recent publication, the Human Protein Atlas team has demonstrated the promise of both citizen science and artificial intelligence approaches in describing the location of human proteins in images, however current results are yet to approach expert-level annotations (Sullivan et al, Nature Biotechnology, Oct 2018).",171098 files,18.72 GB,This competition awarded ranking points,This competition counted towards tiers,Featured Prediction Competition
PLAsTiCC Astronomical Classification,"$25,000 ",4 years ago,,"['tabular', 'astronomy', 'weightedmulticlassloss']","1,089","1,320","22,851","Help some of the world's leading astronomers grasp the deepest properties of the universe.
The human eye has been the arbiter for the classification of astronomical sources in the night sky for hundreds of years. But a new facility -- the Large Synoptic Survey Telescope (LSST) -- is about to revolutionize the field, discovering 10 to 100 times more astronomical sources that vary in the night sky than we've ever known. Some of these sources will be completely unprecedented!
The Photometric LSST Astronomical Time-Series Classification Challenge (PLAsTiCC) asks Kagglers to help prepare to classify the data from this new survey. Competitors will classify astronomical sources that vary with time into different classes, scaling from a small training set to a very large test set of the type the LSST will discover.
More background information is available here.
Acknowledgements
PLAsTiCC is funded through LSST Corporation Grant Award # 2017-03 and administered by the University of Toronto. Financial support for LSST comes from the National Science Foundation (NSF) through Cooperative Agreement No. 1258333, the Department of Energy (DOE) Office of Science under Contract No. DE-AC02-76SF00515, and private funding raised by the LSST Corporation. The NSF-funded LSST Project Office for construction was established as an operating center under management of the Association of Universities for Research in Astronomy (AURA). The DOE-funded effort to build the LSST camera is managed by the SLAC National Accelerator Laboratory (SLAC).
The National Science Foundation (NSF) is an independent federal agency created by Congress in 1950 to promote the progress of science. NSF supports basic research and people to create knowledge that transforms the future.




Photo Credit: M. Park/Inigo Films/LSST/AURA/NSF",18 files,40.12 GB,This competition awarded ranking points,This competition counted towards tiers,Featured Prediction Competition
"Quick, Draw! Doodle Recognition Challenge","$25,000 ",4 years ago,,"['image', 'map@{k}']","1,309","1,563","21,314","""Quick, Draw!"" was released as an experimental game to educate the public in a playful way about how AI works. The game prompts users to draw an image depicting a certain category, such as ”banana,” “table,” etc. The game generated more than 1B drawings, of which a subset was publicly released as the basis for this competition’s training set. That subset contains 50M drawings encompassing 340 label categories.
Sounds fun, right? Here's the challenge: since the training data comes from the game itself, drawings can be incomplete or may not match the label. You’ll need to build a recognizer that can effectively learn from this noisy data and perform well on a manually-labeled test set from a different distribution.
Your task is to build a better classifier for the existing Quick, Draw! dataset. By advancing models on this dataset, Kagglers can improve pattern recognition solutions more broadly. This will have an immediate impact on handwriting recognition and its robust applications in areas including OCR (Optical Character Recognition), ASR (Automatic Speech Recognition) & NLP (Natural Language Processing).",683 files,244.4 GB,This competition awarded ranking points,This competition counted towards tiers,Featured Prediction Competition
Two Sigma: Using News to Predict Stock Movements,"$100,000 ",4 years ago,,"['finance', 'news', 'currencies and foreign exchange', 'custom metric']","2,927","1,138",,"August 2019 Update: this competition is closed and is no longer accepting submissions. The data has been removed from this competition and is not available for use. Thanks for participating!
Can we use the content of news analytics to predict stock price performance? The ubiquity of data today enables investors at any scale to make better investment decisions. The challenge is ingesting and interpreting the data to determine which data is useful, finding the signal in this sea of information. Two Sigma is passionate about this challenge and is excited to share it with the Kaggle community.
As a scientifically driven investment manager, Two Sigma has been applying technology and data science to financial forecasts for over 17 years. Their pioneering advances in big data, AI, and machine learning have pushed the investment industry forward. Now, they're eager to engage with Kagglers in this continuing pursuit of innovation.
By analyzing news data to predict stock prices, Kagglers have a unique opportunity to advance the state of research in understanding the predictive power of the news. This power, if harnessed, could help predict financial outcomes and generate significant economic impact all over the world.
Data for this competition comes from the following sources:
Market data provided by Intrinio.
News data provided by Thomson Reuters. Copyright Thomson Reuters, 2017. All Rights Reserved. Use, duplication, or sale of this service, or data contained herein, except as described in the Competition Rules, is strictly prohibited.

The THOMSON REUTERS Kinesis Logo and THOMSON REUTERS are trademarks of Thomson Reuters and its affiliated companies in the United States and other countries and used herein under license.",1 files,34 B,This competition awarded ranking points,This competition counted towards tiers,Featured Code Competition
Google Analytics Customer Revenue Prediction,"$45,000 ",4 years ago,,"['tabular', 'regression', 'rmse']","3,611","4,171",,"The 80/20 rule has proven true for many businesses–only a small percentage of customers produce most of the revenue. As such, marketing teams are challenged to make appropriate investments in promotional strategies.
RStudio, the developer of free and open tools for R and enterprise-ready products for teams to scale and share work, has partnered with Google Cloud and Kaggle to demonstrate the business impact that thorough data analysis can have.
In this competition, you’re challenged to analyze a Google Merchandise Store (also known as GStore, where Google swag is sold) customer dataset to predict revenue per customer. Hopefully, the outcome will be more actionable operational changes and a better use of marketing budgets for those companies who choose to use data analysis on top of GA data.",6 files,35.9 GB,This competition awarded ranking points,This competition counted towards tiers,Featured Prediction Competition
Inclusive Images Challenge,"$25,000 ",4 years ago,,"['image', 'multiclass classification', 'fscorebetamicro']",468,475,,"Making products that work for people all over the globe is an important value at Google AI. In the field of classification, this means developing models that work well for regions all over the world.
Today, the dataset a model is trained on greatly dictates the performance of that model. A system trained on a dataset that doesn’t represent a broad range of localities could perform worse on images drawn from geographic regions underrepresented in the training data. Google and the industry at large are working to create more diverse & representative datasets. But it is also important for the field to make progress in understanding how to build models when the data available may not cover all audiences a model is meant to reach.
Google AI is challenging Kagglers to develop models that are robust to blind spots that might exist in a data set, and to create image recognition systems that can perform well on test images drawn from different geographic distributions than the ones they were trained on.
By finding ways to teach image classifiers to generalize to new geographic and cultural contexts, we hope the community will make even more progress in inclusive machine learning that benefits everyone, everywhere.
Note: This competition is run in two stages. Refer to the FAQ for an explanation of how this works & the Timeline for specific dates.
This competition is a part of the NIPS 2018 competition track. Winners will be invited to attend and present their solutions at the workshop.





Shankar et al. ""No Classification without Representation: Assessing Geodiversity Issues in Open Data Sets for the Developing World"" NIPS 2017 Workshop on Machine Learning for the Developing World",131902 files,15.88 GB,This competition awarded ranking points,This competition counted towards tiers,Research Prediction Competition
RSNA Pneumonia Detection Challenge,"$30,000 ",4 years ago,,"['image', 'medicine', 'custom metric']","1,499","2,001",,"In this competition, you’re challenged to build an algorithm to detect a visual signal for pneumonia in medical images. Specifically, your algorithm needs to automatically locate lung opacities on chest radiographs.
Here’s the backstory and why solving this problem matters.
Pneumonia accounts for over 15% of all deaths of children under 5 years old internationally. In 2015, 920,000 children under the age of 5 died from the disease. In the United States, pneumonia accounts for over 500,000 visits to emergency departments [1] and over 50,000 deaths in 2015 [2], keeping the ailment on the list of top 10 causes of death in the country.
While common, accurately diagnosing pneumonia is a tall order. It requires review of a chest radiograph (CXR) by highly trained specialists and confirmation through clinical history, vital signs and laboratory exams. Pneumonia usually manifests as an area or areas of increased opacity [3] on CXR. However, the diagnosis of pneumonia on CXR is complicated because of a number of other conditions in the lungs such as fluid overload (pulmonary edema), bleeding, volume loss (atelectasis or collapse), lung cancer, or post-radiation or surgical changes. Outside of the lungs, fluid in the pleural space (pleural effusion) also appears as increased opacity on CXR. When available, comparison of CXRs of the patient taken at different time points and correlation with clinical symptoms and history are helpful in making the diagnosis.
CXRs are the most commonly performed diagnostic imaging study. A number of factors such as positioning of the patient and depth of inspiration can alter the appearance of the CXR [4], complicating interpretation further. In addition, clinicians are faced with reading high volumes of images every shift.
To improve the efficiency and reach of diagnostic services, the Radiological Society of North America (RSNA®) has reached out to Kaggle’s machine learning community and collaborated with the US National Institutes of Health, The Society of Thoracic Radiology, and MD.ai to develop a rich dataset for this challenge.
The RSNA is an international society of radiologists, medical physicists and other medical professionals with more than 54,000 members from 146 countries across the globe. They see the potential for ML to automate initial detection (imaging screening) of potential pneumonia cases in order to prioritize and expedite their review.
Challenge participants may be invited to present their AI models and methodologies during an award ceremony at the RSNA Annual Meeting which will be held in Chicago, Illinois, USA, from November 25-30, 2018.
Acknowledgements
Thank you to the National Institutes of Health Clinical Center for publicly providing the Chest X-Ray dataset [5].
NIH News release: NIH Clinical Center provides one of the largest publicly available chest x-ray datasets to scientific community
Original source files and documents
Also, a big thank you to the competition organizers!
References
Rui P, Kang K. National Ambulatory Medical Care Survey: 2015 Emergency Department Summary Tables. Table 27. Available from: www.cdc.gov/nchs/data/nhamcs/webtables/2015edwebtables.pdf
Deaths: Final Data for 2015. Supplemental Tables. Tables I-21, I-22. Available from: www.cdc.gov/nchs/data/nvsr/nvsr66/nvsr6606tables.pdf
Franquet T. Imaging of community-acquired pneumonia. J Thorac Imaging 2018 (epub ahead of print). PMID 30036297
Kelly B. The Chest Radiograph. Ulster Med J 2012;81(3):143-148
Wang X, Peng Y, Lu L, Lu Z, Bagheri M, Summers RM. ChestX-ray8: Hospital-scale Chest X-ray Database and Benchmarks on Weakly-Supervised Classification and Localization of Common Thorax Diseases. IEEE CVPR 2017, http://openaccess.thecvf.com/content_cvpr_2017/papers/Wang_ChestX-ray8_Hospital-Scale_Chest_CVPR_2017_paper.pdf",29687 files,3.96 GB,This competition awarded ranking points,This competition counted towards tiers,Featured Prediction Competition
Airbus Ship Detection Challenge,"$60,000 ",4 years ago,,"['image', 'intersectionoverunionobjectsegmentationbeta']",878,"1,104","12,475","Airbus is excited to challenge Kagglers to build a model that detects all ships in satellite images as quickly as possible. Can you find them even in imagery with clouds or haze?
Here’s the backstory: Shipping traffic is growing fast. More ships increase the chances of infractions at sea like environmentally devastating ship accidents, piracy, illegal fishing, drug trafficking, and illegal cargo movement. This has compelled many organizations, from environmental protection agencies to insurance companies and national government authorities, to have a closer watch over the open seas.
Airbus offers comprehensive maritime monitoring services by building a meaningful solution for wide coverage, fine details, intensive monitoring, premium reactivity and interpretation response. Combining its proprietary-data with highly-trained analysts, they help to support the maritime industry to increase knowledge, anticipate threats, trigger alerts, and improve efficiency at sea.
A lot of work has been done over the last 10 years to automatically extract objects from satellite images with significative results but no effective operational effects. Now Airbus is turning to Kagglers to increase the accuracy and speed of automatic ship detection.
Algorithm Speed Prize: After the Kaggle challenge is complete, competitors may submit their model via a private Kaggle kernel for a speed evaluation based upon the inference time on over 40.000 images chips (typical size of a full satellite image) to win a special algorithm speed prize.
  If you're interested to explore more Airbus data, you are welcomed to check out the OneAtlas Sandbox. And for more insights on our Maritime Surveillance capabilities, have a look at Airbus Intelligence page.",208164 files,31.41 GB,This competition awarded ranking points,This competition counted towards tiers,Featured Prediction Competition
New York City Taxi Fare Prediction,,4 years ago,4 years ago,"['tabular', 'regression', 'rmse']","1,483","1,566","20,088","In this playground competition, hosted in partnership with Google Cloud and Coursera, you are tasked with predicting the fare amount (inclusive of tolls) for a taxi ride in New York City given the pickup and dropoff locations. While you can get a basic estimate based on just the distance between the two points, this will result in an RMSE of $5-$8, depending on the model used (see the starter code for an example of this approach in Kernels). Your challenge is to do better than this using Machine Learning techniques!
To learn how to handle large datasets with ease and solve this problem using TensorFlow, consider taking the Machine Learning with TensorFlow on Google Cloud Platform specialization on Coursera -- the taxi fare problem is one of several real-world problems that are used as case studies in the series of courses. To make this easier, head to Coursera.org/NEXTextended to claim this specialization for free for the first month!",4 files,5.7 GB,This competition did not award ranking points,This competition did not count towards tiers,Playground Prediction Competition
TGS Salt Identification Challenge,"$100,000 ",4 years ago,,"['image', 'geology', 'custom metric']","3,219","3,726","76,185","Several areas of Earth with large accumulations of oil and gas also have huge deposits of salt below the surface.
But unfortunately, knowing where large salt deposits are precisely is very difficult. Professional seismic imaging still requires expert human interpretation of salt bodies. This leads to very subjective, highly variable renderings. More alarmingly, it leads to potentially dangerous situations for oil and gas company drillers.
To create the most accurate seismic images and 3D renderings, TGS (the world’s leading geoscience data company) is hoping Kaggle’s machine learning community will be able to build an algorithm that automatically and accurately identifies if a subsurface target is salt or not.",7 files,483.07 MB,This competition awarded ranking points,This competition counted towards tiers,Featured Prediction Competition
Costa Rican Household Poverty Level Prediction,,4 years ago,4 years ago,"['tabular', 'multiclass classification', 'fscoremacro']",616,673,"6,495","The Inter-American Development Bank is asking the Kaggle community for help with income qualification for some of the world's poorest families. Are you up for the challenge?
Here's the backstory: Many social programs have a hard time making sure the right people are given enough aid. It’s especially tricky when a program focuses on the poorest segment of the population. The world’s poorest typically can’t provide the necessary income and expense records to prove that they qualify.
In Latin America, one popular method uses an algorithm to verify income qualification. It’s called the Proxy Means Test (or PMT). With PMT, agencies use a model that considers a family’s observable household attributes like the material of their walls and ceiling, or the assets found in the home to classify them and predict their level of need.
While this is an improvement, accuracy remains a problem as the region’s population grows and poverty declines.
To improve on PMT, the IDB (the largest source of development financing for Latin America and the Caribbean) has turned to the Kaggle community. They believe that new methods beyond traditional econometrics, based on a dataset of Costa Rican household characteristics, might help improve PMT’s performance.
Beyond Costa Rica, many countries face this same problem of inaccurately assessing social need. If Kagglers can generate an improvement, the new algorithm could be implemented in other countries around the world.
This is a Kernels-Only Competition, so you must submit your code through Kernels, rather than uploading .csv predictions. You can create private Kernels and even share/edit your work with teammates by adding them as collaborators.",8 files,12.52 MB,This competition did not award ranking points,This competition did not count towards tiers,Playground Code Competition
Google AI Open Images - Visual Relationship Track,"$20,000 ",4 years ago,,['custom metric'],231,265,"1,633","Introduction
Google AI (Google’s AI research arm, tasked with advancing AI for everyone) is challenging you to build an algorithm that detects objects automatically using an absolutely massive training dataset ― one with more varied and complex bounding-box annotations and object classes than ever before.
Here's the background. Computers are getting better and better at vision. But in a few critical ways, they still can't match a human’s intuitive perception.
For example, what do you see when you look at this photo?
Most of us would answer, “a sandy beach, the ocean, a few people walking, some trees, grass, and buildings…a woman walking her dog right there! Oh yeah, and there is a man holding a plastic cup.”
Can a computer provide as precise an image description? Google AI wants to further push the capabilities of computer vision. We hope that providing very large training set will stimulate research into more sophisticated object and relationship detection models that will exceed current state-of-the-art performance.
The results of this Challenge will be presented at a workshop at the European Conference on Computer Vision 2018.
Visual Relationship Detection Track
Identifying different objects (man and cup) is an important problem on its own, but identifying the relationship between them (holding) is critical for many real world use cases.
In this Visual Relationship Detection track Challenge you’re asked to build an algorithm that detects pairs of objects in particular relations: things like ""woman playing guitar,"" ""beer on table,"" or ""dog inside car.""
The Challenge dataset includes both object bounding boxes and visual relationship annotations. The training set contains annotations for 329 distinct relationship triplets, occurring a total of 374,768 times.
In this track of the Challenge, you are asked to build the best performing algorithm for automatically detecting relationships triplets. Please refer to the Open Images Challenge page for additional details on the dataset.
This competition is one of two tracks in the Open Images Challenge. Find the Object Detection track of this competition using the entire training set here.
Example of ‘man playing guitar’
Radiofiera - Villa Cordellina Lombardi, Montecchio Maggiore (VI) - agosto 2010 by Andrea Sartorati
Example of ‘chair at table’
Epic Fireworks - Loads A Room by Epic Fireworks",3 files,10.43 GB,This competition awarded 0.5X ranking points,This competition counted towards tiers,Featured Prediction Competition
Google AI Open Images - Object Detection Track,"$30,000 ",4 years ago,,['custom metric'],452,549,"3,614","Introduction
Google AI (Google’s AI research arm, tasked with advancing AI for everyone) is challenging you to build an algorithm that detects objects automatically using an absolutely massive training dataset ― one with more varied and complex bounding-box annotations and object classes than ever before.
Here's the background. Computers are getting better and better at vision. But in a few critical ways, they still can't match a human’s intuitive perception.
For example, what do you see when you look at this photo?
Most of us would answer, “a sandy beach, the ocean, a few people walking, some trees, grass, and buildings…a woman walking her dog right there! Oh yeah, and there is a man holding a plastic cup.”
Can a computer provide as precise an image description? Google AI wants to further push the capabilities of computer vision. We hope that providing very large training set will stimulate research into more sophisticated object and relationship detection models that will exceed current state-of-the-art performance.
The results of this Challenge will be presented at a workshop at the European Conference on Computer Vision 2018.
Object Detection Track
Object detection is a central task in computer vision, with applications ranging across search, robotics, self-driving cars, and many others. As deep network solutions become deeper and more complex, they are often limited by the amount of training data available.
With this in mind, to spur advances in analyzing and understanding images, Google AI has publicly released the Open Images dataset. Open Images follows the tradition of PASCAL VOC, ImageNet and COCO, now at an unprecedented scale.
The Open Images Challenge is based on Open Images dataset. The training set of the Challenge contains:
12M bounding-box annotations for 500 object classes on 1.7M training images
Images of complex scenes with several objects–an average of 7 boxes per image
Highly varied images that contain brand new objects like “fedora” and “snowman”
Class hierarchy that reflects the relationships between classes of Open Images.
In this track of the Challenge, you are asked to build the best performing algorithm for automatically detecting objects.
Please refer to the Open Images Challenge page for additional details on the dataset. In addition to this Object Detection track, the Challenge also includes a Visual Relationship Detection track to detect pairs of objects in particular relations, e.g. ""woman playing guitar,"" ""beer on table,"" ""dog inside car"", ""man holding coffee"", etc. The Visual Relationship Detection track is available here.
Example annotations. Left: Mark Paul Gosselaar plays the guitar by Rhys A. Right: the house by anita kluska. Both images used under CC BY 2.0 license.",100000 files,10.46 GB,This competition awarded 0.5X ranking points,This competition counted towards tiers,Featured Prediction Competition
Store Item Demand Forecasting Challenge,,4 years ago,4 years ago,"['tabular', 'smape']",459,484,"6,135","This competition is provided as a way to explore different time series techniques on a relatively simple and clean dataset.
You are given 5 years of store-item sales data, and asked to predict 3 months of sales for 50 different items at 10 different stores.
What's the best way to deal with seasonality? Should stores be modeled separately, or can you pool them together? Does deep learning work better than ARIMA? Can either beat xgboost?
This is a great competition to explore different models and improve your skills in forecasting.",3 files,18.7 MB,This competition did not award ranking points,This competition did not count towards tiers,Playground Code Competition
Flavours of Physics: Finding τ → μμμ (Kernels Only),,4 years ago,4 years ago,['custom metric'],63,64,375,"The European Organization for Nuclear Research is the world’s largest high energy physics laboratory.
LHCb is an experiment set up to explore what happened after the Big Bang that allowed matter to survive and build the Universe we inhabit today.
The Yandex School of Data Analysis (YSDA) is a free Master’s-level program in Computer Science and Data Analysis, which is offered by Yandex since 2007. The aim of the School is to train specialists in data analysis and information retrieval to be able to solve cutting edge industry problems as well as fundamental research challenges. YSDA is associated member of LHCb since December 2014.
Yandex Data Factory are the Machine Learning and data analytics experts that use data science to improve business’ operations, revenues and profitability. By building upon the real-time personalisation and predictive analytics technology of parent company, Yandex, the fourth largest search engine in the world, Yandex Data Factory helps clients improve their business awareness through the exploitation of their own data.
Yandex Data Factory’s proven data science and technology continually analyses, tests, refines and reapplies hundreds of hypotheses to the customers’ datasets to determine the best next course of action. It offers tailored, scalable, SaaS-driven Machine Learning services to a wide variety of data-reliant verticals, such as retail, financial services, travel and telecoms, who wish to use their data for purposes such as improving personalisation, segmentation, churn prevention or fraud detection.
Yandex Data Factory was founded in 2014 by Yandex and is headquartered in Amsterdam, operating throughout Europe.
Intel (NASDAQ: INTC) is a world leader in computing innovation. The company designs and builds the essential technologies that serve as the foundation for the world’s computing devices. As a leader in corporate responsibility and sustainability, Intel also manufactures the world’s first commercially available “conflict-free” microprocessors. Additional information about Intel is available at http://newsroom.intel.com and http://blogs.intel.com.
The University of Zurich is one of the leading research universities in Europe and offers the widest range of degree programs in Switzerland. It was founded in 1833 and currently has seven faculties: Philosophy, Human Medicine, Economic Sciences, Law, Mathematics and Natural Sciences, Theology and Veterinary Medicine. 
Warwick is one of the UK's leading universities, with an acknowledged reputation for excellence in research and teaching, for innovation, and for links with business and industry.
Institute of Nuclear Physics, Polish Academy of Sciences. Founded in 1955 Institute of Nuclear Physics has become leading Particle Physics research institution and ranked as class A+ by Polish Ministry of Higher Education.
Consistently ranked as one of Russia’s top universities, the Higher School of Economics is a leader in Russian education and one of the preeminent economics and social sciences universities in eastern Europe and Eurasia.  ",5 files,436.14 MB,This competition did not award ranking points,This competition did not count towards tiers,Playground Code Competition
What's Cooking? (Kernels Only),,4 years ago,4 years ago,"['text', 'food', 'multiclass classification', 'categorizationaccuracy']",520,552,"4,949","Picture yourself strolling through your local, open-air market... What do you see? What do you smell? What will you make for dinner tonight?
If you're in Northern California, you'll be walking past the inevitable bushels of leafy greens, spiked with dark purple kale and the bright pinks and yellows of chard. Across the world in South Korea, mounds of bright red kimchi greet you, while the smell of the sea draws your attention to squids squirming nearby. India’s market is perhaps the most colorful, awash in the rich hues and aromas of dozens of spices: turmeric, star anise, poppy seeds, and garam masala as far as the eye can see.
Some of our strongest geographic and cultural associations are tied to a region's local foods. This playground competitions asks you to predict the category of a dish's cuisine given a list of its ingredients. 
Acknowledgements
We want to thank Yummly for providing this unique dataset. Kaggle is hosting this playground competition for fun and practice.",3 files,15.29 MB,This competition did not award ranking points,This competition did not count towards tiers,Playground Code Competition
Movie Review Sentiment Analysis (Kernels Only),,4 years ago,4 years ago,"['text', 'multiclass classification', 'categorizationaccuracy']",409,430,"3,806","""There's a thin line between likably old-fashioned and fuddy-duddy, and The Count of Monte Cristo ... never quite settles on either side.""
The Rotten Tomatoes movie review dataset is a corpus of movie reviews used for sentiment analysis, originally collected by Pang and Lee [1]. In their work on sentiment treebanks, Socher et al. [2] used Amazon's Mechanical Turk to create fine-grained labels for all parsed phrases in the corpus. This competition presents a chance to benchmark your sentiment-analysis ideas on the Rotten Tomatoes dataset. You are asked to label phrases on a scale of five values: negative, somewhat negative, neutral, somewhat positive, positive. Obstacles like sentence negation, sarcasm, terseness, language ambiguity, and many others make this task very challenging.
Kaggle is hosting this competition for the machine learning community to use for fun and practice. This competition was inspired by the work of Socher et al [2]. We encourage participants to explore the accompanying (and dare we say, fantastic) website that accompanies the paper:
http://nlp.stanford.edu/sentiment/
There you will find have source code, a live demo, and even an online interface to help train the model.
[1] Pang and L. Lee. 2005. Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales. In ACL, pages 115–124.
[2] Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank, Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Chris Manning, Andrew Ng and Chris Potts. Conference on Empirical Methods in Natural Language Processing (EMNLP 2013).",3 files,2.44 MB,This competition did not award ranking points,This competition did not count towards tiers,Playground Code Competition
Forest Cover Type (Kernels Only),,4 years ago,4 years ago,"['tabular', 'forestry', 'categorizationaccuracy']",358,377,"3,499","Random forests? Cover trees? Not so fast, computer nerds. We're talking about the real thing.
In this competition you are asked to predict the forest cover type (the predominant kind of tree cover) from cartographic variables. The actual forest cover type for a given 30 x 30 meter cell was determined from US Forest Service (USFS) Region 2 Resource Information System data. Independent variables were then derived from data obtained from the US Geological Survey and USFS. The data is in raw form and contains binary columns of data for qualitative independent variables such as wilderness areas and soil type.
This study area includes four wilderness areas located in the Roosevelt National Forest of northern Colorado. These areas represent forests with minimal human-caused disturbances, so that existing forest cover types are more a result of ecological processes rather than forest management practices.
This competition originally ran in 2015. We are relaunching it as a kernels-only version here.
Acknowledgements
Kaggle is hosting this competition for the machine learning community to use for fun and practice. This dataset was provided by Jock A. Blackard and Colorado State University. We also thank the UCI machine learning repository for hosting the dataset. If you use the problem in publication, please cite:
Bache, K. & Lichman, M. (2013). UCI Machine Learning Repository. Irvine, CA: University of California, School of Information and Computer Science",4 files,15.38 MB,This competition did not award ranking points,This competition did not count towards tiers,Playground Code Competition
Santander Value Prediction Challenge,"$60,000 ",4 years ago,,"['finance', 'banking', 'rmsle']","4,463","4,865","54,750","According to Epsilon research, 80% of customers are more likely to do business with you if you provide personalized service. Banking is no exception.
The digitalization of everyday lives means that customers expect services to be delivered in a personalized and timely manner… and often before they´ve even realized they need the service. In their 3rd Kaggle competition, Santander Group aims to go a step beyond recognizing that there is a need to provide a customer a financial service and intends to determine the amount or value of the customer's transaction. This means anticipating customer needs in a more concrete, but also simple and personal way. With so many choices for financial services, this need is greater now than ever before.
In this competition, Santander Group is asking Kagglers to help them identify the value of transactions for each potential customer. This is a first step that Santander needs to nail in order to personalize their services at scale.",3 files,1.08 GB,This competition awarded ranking points,This competition counted towards tiers,Featured Prediction Competition
The 2nd YouTube-8M Video Understanding Challenge,"$25,000 ",5 years ago,,"['video data', 'custom metric']",312,380,"2,554","The world is generating and consuming an enormous amount of video content. Currently on YouTube, people watch over 1 billion hours of video every single day.
To spur advances in analyzing and understanding video, Google AI has publicly released a large-scale video dataset that consists of millions of YouTube video features and associated labels from a diverse vocabulary of 3,700+ visual entities called the YouTube-8M Dataset. Last year, we successfully hosted Google Cloud & YouTube-8M Video Understanding Challenge, with 742 participating teams with 946 individual competitors from 60 countries. This competition is the second Kaggle competition based on YouTube 8M dataset, and is focused on learning video representation under budget constraints.
For a lot of video tasks where there are a large number of classes, like recommending new videos or automatic video classification, compact models need to meet memory and computational requirements. This is true even if working in cloud computational environments. Also, compact models make it possible to have limited-memory or catalog indexes on devices in order to do personalized and privacy-preserving computation on user’s personal mobile phones.
In this competition, you’re challenged to produce a compact video classification model. Your model size must not exceed 1 GB (this is strictly enforced, through model upload). We encourage participants to train a model that most efficiently uses this budget, rather than ensembles of lots of models.
This competition is being hosted by Google AI (previously known as Google Research) as a part of the European Conference on Computer Vision (ECCV) 2018 selected workshop session. Please refer to the YouTube 8M Large-Scale Video Understanding Workshop Page for details about the workshop.",11 files,1.1 GB,This competition awarded ranking points,This competition counted towards tiers,Featured Prediction Competition
Home Credit Default Risk,"$70,000 ",5 years ago,,"['tabular', 'banking', 'auc']","7,176","8,373","131,888","Many people struggle to get loans due to insufficient or non-existent credit histories. And, unfortunately, this population is often taken advantage of by untrustworthy lenders.
Home Credit strives to broaden financial inclusion for the unbanked population by providing a positive and safe borrowing experience. In order to make sure this underserved population has a positive loan experience, Home Credit makes use of a variety of alternative data--including telco and transactional information--to predict their clients' repayment abilities.
While Home Credit is currently using various statistical and machine learning methods to make these predictions, they're challenging Kagglers to help them unlock the full potential of their data. Doing so will ensure that clients capable of repayment are not rejected and that loans are given with a principal, maturity, and repayment calendar that will empower their clients to be successful.",10 files,2.68 GB,This competition awarded ranking points,This competition counted towards tiers,Featured Prediction Competition
TrackML Particle Tracking Challenge,"$25,000 ",5 years ago,,"['tabular', 'physics', 'custom metric']",651,739,"5,776","To explore what our universe is made of, scientists at CERN are colliding protons, essentially recreating mini big bangs, and meticulously observing these collisions with intricate silicon detectors.
While orchestrating the collisions and observations is already a massive scientific accomplishment, analyzing the enormous amounts of data produced from the experiments is becoming an overwhelming challenge.
Event rates have already reached hundreds of millions of collisions per second, meaning physicists must sift through tens of petabytes of data per year. And, as the resolution of detectors improve, ever better software is needed for real-time pre-processing and filtering of the most promising events, producing even more data.
To help address this problem, a team of Machine Learning experts and physics scientists working at CERN (the world largest high energy physics laboratory), has partnered with Kaggle and prestigious sponsors to answer the question: can machine learning assist high energy physics in discovering and characterizing new particles?
Specifically, in this competition, you’re challenged to build an algorithm that quickly reconstructs particle tracks from 3D points left in the silicon detectors. This challenge consists of two phases:
The Accuracy phase has run on Kaggle from May to 13th August 2018 (Winners to be announced by end September). Here we’ll be focusing on the highest score, irrespective of the evaluation time. This phase is an official IEEE WCCI competition (Rio de Janeiro, Jul 2018).
The Throughput phase will run on Codalab starting in September 2018. Participants will submit their software which is evaluated by the platform. Incentive is on the throughput (or speed) of the evaluation while reaching a good score. This phase is an official NIPS competition (Montreal, Dec 2018).
All the necessary information for the Accuracy phase is available here on Kaggle site. The overall TrackML challenge web site is there.",10 files,81.39 GB,This competition awarded ranking points,This competition counted towards tiers,Featured Prediction Competition
Avito Demand Prediction Challenge,"$25,000 ",5 years ago,,"['tabular', 'image', 'text', 'rmse']","1,868","2,313","43,509","When selling used goods online, a combination of tiny, nuanced details in a product description can make a big difference in drumming up interest. Details like:

And, even with an optimized product listing, demand for a product may simply not exist–frustrating sellers who may have over-invested in marketing.
Avito, Russia’s largest classified advertisements website, is deeply familiar with this problem. Sellers on their platform sometimes feel frustrated with both too little demand (indicating something is wrong with the product or the product listing) or too much demand (indicating a hot item with a good description was underpriced).
In their fourth Kaggle competition, Avito is challenging you to predict demand for an online advertisement based on its full description (title, description, images, etc.), its context (geographically where it was posted, similar ads already posted) and historical demand for similar ads in similar contexts. With this information, Avito can inform sellers on how to best optimize their listing and provide some indication of how much interest they should realistically expect to receive.",14 files,146.76 GB,This competition awarded ranking points,This competition counted towards tiers,Featured Prediction Competition
CVPR 2018 WAD Video Segmentation Challenge,"$2,500 ",5 years ago,,['custom metric'],141,188,"1,210","When you're driving, how important is it to be able to quickly tell the difference between a person vs. a stop sign? It's a hugely important, but typically very simple, distinction that you would make reflexively. Autonomous vehicles are not able to do this quite as effortlessly.
This challenge, hosted by the 2018 CVPR workshop on autonomous driving (WAD), asks you to help give autonomously driven vehicles the same edge. Using an unprecedented dataset, you're asked to segment movable objects, such as cars and pedestrians, at instance level within image frames.
By participating in this competition, you'll be helping to further our understand of the current status of computer vision algorithms in solving environmental perception problems for autonomous driving.
This challenge is a truly unique opportunity to work on a tremendously high value and high profile problem. The dataset presented here contains over 10 times more fine-labeled images than the largest public dataset of its type.
Acknowledgements
This competition is hosted by the 2018 CVPR workshop on autonomous driving (WAD), with dataset and evaluation metric contributed by Baidu Inc.",8 files,102.59 GB,This competition did not award ranking points,This competition did not count towards tiers,Research Prediction Competition
iMaterialist Challenge (Fashion) at FGVC5,"$2,500 ",5 years ago,,['fscorebetamicro'],212,301,"2,253","As shoppers move online, it would be a dream come true to have products in photos classified automatically. But, automatic product recognition is tough because for the same product, a picture can be taken in different lighting, angles, backgrounds, and levels of occlusion. Meanwhile different fine-grained categories may look very similar, for example, royal blue vs turquoise in color. Many of today’s general-purpose recognition machines simply cannot perceive such subtle differences between photos, yet these differences could be important for shopping decisions.
Tackling issues like this is why the Conference on Computer Vision and Pattern Recognition (CVPR) has put together a workshop specifically for data scientists focused on fine-grained visual categorization called the FGVC5 workshop. As part of this workshop, CVPR is partnering with Google, Wish, and Malong Technologies to challenge the data science community to help push the state of the art in automatic image classification.
In this competition, FGVC workshop organizers with Wish and Malong Technologies challenge you to develop algorithms that will help with an important step towards automatic product detection – to accurately assign attribute labels for fashion images. Individuals/Teams with top submissions will be invited to present their work live at the FGVC5 workshop.
Kaggle is excited to partner with research groups to push forward the frontier of machine learning. Research competitions make use of Kaggle's platform and experience, but are largely organized by the research group's data science team. Any questions or concerns regarding the competition data, quality, or topic will be addressed by them.",4 files,31.93 MB,This competition did not award ranking points,This competition did not count towards tiers,Research Prediction Competition
Freesound General-Purpose Audio Tagging Challenge,,5 years ago,,['map@{k}'],556,638,"5,671","Some sounds are distinct and instantly recognizable, like a baby’s laugh or the strum of a guitar.
Other sounds aren’t clear and are difficult to pinpoint. If you close your eyes, can you tell which of the sounds below is a chainsaw versus a blender?
Moreover, we often experience a mix of sounds that create an ambience – like the clamoring of construction, a hum of traffic from outside the door, blended with loud laughter from the room, and the ticking of the clock on your wall. The sound clip below is of a busy food court in the UK.
Partly because of the vastness of sounds we experience, no reliable automatic general-purpose audio tagging systems exist. Currently, a lot of manual effort is required for tasks like annotating sound collections and providing captions for non-speech events in audiovisual content.
To tackle this problem, Freesound (an initiative by MTG-UPF that maintains a collaborative database with over 370,000 Creative Commons Licensed sounds) and Google Research’s Machine Perception Team (creators of AudioSet, a large-scale dataset of manually annotated audio events with over 500 classes) have teamed up to develop the dataset for this competition.
You’re challenged to build a general-purpose automatic audio tagging system using a dataset of audio files covering a wide range of real-world environments. Sounds in the dataset include things like musical instruments, human sounds, domestic sounds, and animals from Freesound’s library, annotated using a vocabulary of more than 40 labels from Google’s AudioSet ontology. To succeed in this competition your systems will need to be able to recognize an increased number of sound events of very diverse nature, and to leverage subsets of training data featuring annotations of varying reliability (see Data section for more information).
Organizers
Eduardo Fonseca, MTG-UPF, Barcelona
Manoj Plakal, Google's Sound Understanding, New York
Frederic Font, MTG-UPF, Barcelona
Dan Ellis, Google's Sound Understanding, New York",18877 files,10.37 GB,This competition did not award ranking points,This competition did not count towards tiers,Research Prediction Competition
iMaterialist Challenge (Furniture) at FGVC5,"$2,500 ",5 years ago,,['meanbesterroratk'],426,569,"5,214","As shoppers move online, it’d be a dream come true to have products in photos classified automatically. But, automatic product recognition is challenging because for the same product, a picture can be taken in different lighting, angles, backgrounds, and levels of occlusion. Meanwhile different fine-grained categories may look very similar, for example, ball chair vs egg chair for furniture, or dutch oven vs french oven for cookware. Many of today’s general-purpose recognition machines simply can’t perceive such subtle differences between photos, yet these differences could be important for shopping decisions.
Tackling issues like this is why the Conference on Computer Vision and Pattern Recognition (CVPR) has put together a workshop specifically for data scientists focused on fine-grained visual categorization called the FGVC5 workshop. As part of this workshop, CVPR is partnering with Google, Malong Technologies and Wish to challenge the data science community to help push the state of the art in automatic image classification.
In this competition, FGVC5 workshop organizers and Malong Technologies challenge you to develop algorithms that will help with an important step towards automatic product recognition – to accurately assign category labels for furniture and home goods images. Individuals/Teams with top submissions will be invited to present their work live at the FGVC5 workshop.
Kaggle is excited to partner with research groups to push forward the frontier of machine learning. Research competitions make use of Kaggle's platform and experience, but are largely organized by the research group's data science team. Any questions or concerns regarding the competition data, quality, or topic will be addressed by them.
 ",4 files,48.59 MB,This competition did not award ranking points,This competition did not count towards tiers,Research Prediction Competition
TalkingData AdTracking Fraud Detection Challenge,"$25,000 ",5 years ago,,['auc'],"3,943","4,610","68,452","Fraud risk is everywhere, but for companies that advertise online, click fraud can happen at an overwhelming volume, resulting in misleading click data and wasted money. Ad channels can drive up costs by simply clicking on the ad at a large scale. With over 1 billion smart mobile devices in active use every month, China is the largest
mobile market in the world and therefore suffers from huge volumes of fradulent traffic.
TalkingData, China’s largest independent big data service platform, covers over 70% of active mobile devices nationwide. They handle 3 billion clicks per day, of which 90% are potentially fraudulent. Their current approach to prevent click fraud for app developers is to measure the journey of a user’s click across their portfolio, and flag IP addresses who produce lots of clicks, but never end up installing apps. With this information, they've built an IP blacklist and device blacklist.
While successful, they want to always be one step ahead of fraudsters and have turned to the Kaggle community for help in further developing their solution. In their 2nd competition with Kaggle, you’re challenged to build an algorithm that predicts whether a user will download an app after clicking a mobile app ad. To support your modeling, they have provided a generous dataset covering approximately 200 million clicks over 4 days!",5 files,11.27 GB,This competition awarded ranking points,This competition counted towards tiers,Featured Prediction Competition
DonorsChoose.org Application Screening,,5 years ago,,"['binary classification', 'crowdfunding', 'auc']",580,617,"6,401","Founded in 2000 by a high school teacher in the Bronx, DonorsChoose.org empowers public school teachers from across the country to request much-needed materials and experiences for their students. At any given time, there are thousands of classroom requests that can be brought to life with a gift of any amount.
DonorsChoose.org receives hundreds of thousands of project proposals each year for classroom projects in need of funding. Right now, a large number of volunteers is needed to manually screen each submission before it's approved to be posted on the DonorsChoose.org website.
Next year, DonorsChoose.org expects to receive close to 500,000 project proposals. As a result, there are three main problems they need to solve:
How to scale current manual processes and resources to screen 500,000 projects so that they can be posted as quickly and as efficiently as possible
How to increase the consistency of project vetting across different volunteers to improve the experience for teachers
How to focus volunteer time on the applications that need the most assistance
The goal of the competition is to predict whether or not a DonorsChoose.org project proposal submitted by a teacher will be approved, using the text of project descriptions as well as additional metadata about the project, teacher, and school. DonorsChoose.org can then use this information to identify projects most likely to need further review before approval.
With an algorithm to pre-screen applications, DonorsChoose.org can auto-approve some applications quickly so that volunteers can spend their time on more nuanced and detailed project vetting processes, including doing more to help teachers develop projects that qualify for specific funding opportunities.
Your machine learning algorithm can help more teachers get funded more quickly, and with less cost to DonorsChoose.org, allowing them to channel even more funding directly to classrooms across the country.
Getting Started with Kernels
Get familiar with the competition data and the machine learning objective quickly using Kernels. Google's engineering education team has put together a starter tutorial implementing benchmark linear classification model.
Acknowledgments
Machine Learning Crash Course was created by Google's engineering education team in partnership with numerous Machine Learning subject matter experts across Google.",,,This competition did not award ranking points,This competition did not count towards tiers,Playground Prediction Competition
Nomad2018 Predicting Transparent Conductors,"€5,000",5 years ago,,"['chemistry', 'mcrmsle']",878,944,"13,192","Innovative materials design is needed to tackle some of the most important health, environmental, energy, social, and economic challenges of this century. In particular, improving the properties of materials that are intrinsically connected to the generation and utilization of energy is crucial if we are to mitigate environmental damage due to a growing global demand. Transparent conductors are an important class of compounds that are both electrically conductive and have a low absorption in the visible range, which are typically competing properties. A combination of both of these characteristics is key for the operation of a variety of technological devices such as photovoltaic cells, light-emitting diodes for flat-panel displays, transistors, sensors, touch screens, and lasers. However, only a small number of compounds are currently known to display both transparency and conductivity suitable enough to be used as transparent conducting materials.
Aluminum (Al), gallium (Ga), indium (In) sesquioxides are some of the most promising transparent conductors because of a combination of both large bandgap energies, which leads to optical transparency over the visible range, and high conductivities. These materials are also chemically stable and relatively inexpensive to produce. Alloying of these binary compounds in ternary or quaternary mixtures could enable the design of a new material at a specific composition with improved properties over what is current possible. These alloys are described by the formula
(AlxGayInz)2NO3N
; where x, y, and z can vary but are limited by the constraint x+y+z = 1. The total number of atoms in the",5 files,6.24 MB,This competition did not award ranking points,This competition did not count towards tiers,Research Prediction Competition
Santa Gift Matching Challenge,"$25,000 ",5 years ago,,['custom metric'],428,464,"5,278","‘Tis the night before Christmas
year: two thousand seventeen.
Santa’s grown grouchy,
borderline mean.
What used to be simple for Old St. Nick,
is now too puzzling, it’s making him sick!
See, Santa always knew, deep down in his gut,
what toy each kid wanted–no ifs, ands, or buts.
But fierce population growth, more twins, and toy innovation,
has left too complex a problem, in dire need of optimization.
“Don’t worry, Mr. Santa”, said an Elf named McMaggle,
“I have a solution! Have you heard of Kaggle?”
As she explained Kaggle in-depth, Santa’s doubt began turning,
he became a believer in the magic of...machine learning.
So, Santa’s team needs YOU more than ever this year,
to solve this painful problem and save Christmas cheer.
The Challenge
In this playground competition, you’re challenged to build a toy matching algorithm that maximizes happiness by pairing kids with toys they want. In the dataset, each kid has 10 preferences for their gift (from 1000) and Santa has 1000 preferred kids for every gift available. What makes this extra difficult is that 0.4% of the kids are twins, and by their parents’ request, require the same gift.",4 files,168.85 MB,This competition awarded ranking points,This competition counted towards tiers,Featured Prediction Competition
Recruit Restaurant Visitor Forecasting,"$25,000 ",5 years ago,,['rmsle'],"2,148","2,426","39,869","Running a thriving local restaurant isn't always as charming as first impressions appear. There are often all sorts of unexpected troubles popping up that could hurt business.
One common predicament is that restaurants need to know how many customers to expect each day to effectively purchase ingredients and schedule staff members. This forecast isn't easy to make because many unpredictable factors affect restaurant attendance, like weather and local competition. It's even harder for newer restaurants with little historical data.
Recruit Holdings has unique access to key datasets that could make automated future customer prediction possible. Specifically, Recruit Holdings owns Hot Pepper Gourmet (a restaurant review service), AirREGI (a restaurant point of sales service), and Restaurant Board (reservation log management software).
In this competition, you're challenged to use reservation and visitation data to predict the total number of visitors to a restaurant for future dates. This information will help restaurants be much more efficient and allow them to focus on creating an enjoyable dining experience for their customers.",8 files,27.3 MB,This competition awarded ranking points,This competition counted towards tiers,Featured Prediction Competition
Plant Seedlings Classification,,5 years ago,5 years ago,"['image', 'multiclass classification', 'plants', 'fscoremicro']",833,871,"7,385","Can you differentiate a weed from a crop seedling?
The ability to do so effectively can mean better crop yields and better stewardship of the environment.
The Aarhus University Signal Processing group, in collaboration with University of Southern Denmark, has recently released a dataset containing images of approximately 960 unique plants belonging to 12 species at several growth stages.
We're hosting this dataset as a Kaggle competition in order to give it wider exposure, to give the community an opportunity to experiment with different image recognition techniques, as well to provide a place to cross-pollenate ideas.
Acknowledgments
We extend our appreciation to the Aarhus University Department of Engineering Signal Processing Group for hosting the original data.
Citation
A Public Image Database for Benchmark of Plant Seedling Classification Algorithms",5545 files,1.81 GB,This competition did not award ranking points,This competition did not count towards tiers,Playground Prediction Competition
Mercari Price Suggestion Challenge,"$100,000 ",5 years ago,,['rmsle'],"2,380","2,742","36,709","It can be hard to know how much something’s really worth. Small details can mean big differences in pricing. For example, one of these sweaters cost $335 and the other cost $9.99. Can you guess which one’s which?
Product pricing gets even harder at scale, considering just how many products are sold online. Clothing has strong seasonal pricing trends and is heavily influenced by brand names, while electronics have fluctuating prices based on product specs.
Mercari, Japan’s biggest community-powered shopping app, knows this problem deeply. They’d like to offer pricing suggestions to sellers, but this is tough because their sellers are enabled to put just about anything, or any bundle of things, on Mercari's marketplace.
In this competition, Mercari’s challenging you to build an algorithm that automatically suggests the right product prices. You’ll be provided user-inputted text descriptions of their products, including details like product category name, brand name, and item condition.
Note that, because of the public nature of this data, this competition is a “Kernels Only” competition. In the second stage of the challenge, files will only be available through Kernels and you will not be able to modify your approach in response to new data. Read more details in the data tab and Kernels FAQ page.",5 files,430.52 MB,This competition awarded ranking points,This competition counted towards tiers,Featured Code Competition
TensorFlow Speech Recognition Challenge,"$25,000 ",5 years ago,,['categorizationaccuracy'],"1,313","1,591","24,245","We might be on the verge of too many screens. It seems like everyday, new versions of common objects are “re-invented” with built-in wifi and bright touchscreens. A promising antidote to our screen addiction are voice interfaces.
But, for independent makers and entrepreneurs, it’s hard to build a simple speech detector using free, open data and code. Many voice recognition datasets require preprocessing before a neural network model can be built on them. To help with this, TensorFlow recently released the Speech Commands Datasets. It includes 65,000 one-second long utterances of 30 short words, by thousands of different people.
In this competition, you're challenged to use the Speech Commands Dataset to build an algorithm that understands simple spoken commands. By improving the recognition accuracy of open-sourced voice interface tools, we can improve product effectiveness and their accessibility.",4 files,3.76 GB,This competition awarded ranking points,This competition counted towards tiers,Featured Prediction Competition
Spooky Author Identification,"$25,000 ",5 years ago,5 years ago,"['multiclass classification', 'literature', 'linguistics', 'multiclassloss']","1,241","1,353","10,627","As I scurried across the candlelit chamber, manuscripts in hand, I thought I'd made it. Nothing would be able to hurt me anymore. Little did I know there was one last fright lurking around the corner.
DING! My phone pinged me with a disturbing notification. It was Will, the scariest of Kaggle moderators, sharing news of another data leak.
""ph’nglui mglw’nafh Cthulhu R’lyeh wgah’nagl fhtagn!"" I cried as I clumsily dropped my crate of unbound, spooky books. Pages scattered across the chamber floor. How will I ever figure out how to put them back together according to the authors who wrote them? Or are they lost, forevermore? Wait, I thought... I know, machine learning!
In this year's Halloween playground competition, you're challenged to predict the author of excerpts from horror stories by Edgar Allan Poe, Mary Shelley, and HP Lovecraft. We're encouraging you (with cash prizes!) to share your insights in the competition's discussion forum and code in Kernels. We've designated prizes to reward authors of kernels and discussion threads that are particularly valuable to the community. Click the ""Prizes"" tab on this overview page to learn more.
Getting Started
New to Kernels or working with natural language data? We've put together some starter kernels in Python and R to help you hit the ground running.",3 files,1.9 MB,This competition did not award ranking points,This competition did not count towards tiers,Playground Prediction Competition
Statoil/C-CORE Iceberg Classifier Challenge,"$50,000 ",5 years ago,,"['image', 'binary classification', 'weather and climate', 'logloss']","3,330","3,639","40,702","Drifting icebergs present threats to navigation and activities in areas such as offshore of the East Coast of Canada.
Currently, many institutions and companies use aerial reconnaissance and shore-based support to monitor environmental conditions and assess risks from icebergs. However, in remote areas with particularly harsh weather, these methods are not feasible, and the only viable monitoring option is via satellite.
Statoil, an international energy company operating worldwide, has worked closely with companies like C-CORE. C-CORE have been using satellite data for over 30 years and have built a computer vision based surveillance system. To keep operations safe and efficient, Statoil is interested in getting a fresh new perspective on how to use machine learning to more accurately detect and discriminate against threatening icebergs as early as possible.
In this competition, you’re challenged to build an algorithm that automatically identifies if a remotely sensed target is a ship or iceberg. Improvements made will help drive the costs down for maintaining safe working conditions.",3 files,302.1 MB,This competition awarded ranking points,This competition counted towards tiers,Featured Prediction Competition
Corporación Favorita Grocery Sales Forecasting,"$30,000 ",5 years ago,,"['tabular', 'regression', 'food', 'custom metric']","1,671","1,868","31,241","Brick-and-mortar grocery stores are always in a delicate dance with purchasing and sales forecasting. Predict a little over, and grocers are stuck with overstocked, perishable goods. Guess a little under, and popular items quickly sell out, leaving money on the table and customers fuming.
The problem becomes more complex as retailers add new locations with unique needs, new products, ever transitioning seasonal tastes, and unpredictable product marketing. Corporación Favorita, a large Ecuadorian-based grocery retailer, knows this all too well. They operate hundreds of supermarkets, with over 200,000 different products on their shelves.
Corporación Favorita has challenged the Kaggle community to build a model that more accurately forecasts product sales. They currently rely on subjective forecasting methods with very little data to back them up and very little automation to execute plans. They’re excited to see how machine learning could better ensure they please customers by having just enough of the right products at the right time.",8 files,479.88 MB,This competition awarded ranking points,This competition counted towards tiers,Featured Prediction Competition
Porto Seguro’s Safe Driver Prediction,"$25,000 ",5 years ago,,"['tabular', 'binary classification', 'normalizedgini']","5,156","5,784","93,568","Nothing ruins the thrill of buying a brand new car more quickly than seeing your new insurance bill. The sting’s even more painful when you know you’re a good driver. It doesn’t seem fair that you have to pay so much if you’ve been cautious on the road for years.
Porto Seguro, one of Brazil’s largest auto and homeowner insurance companies, completely agrees. Inaccuracies in car insurance company’s claim predictions raise the cost of insurance for good drivers and reduce the price for bad ones.
In this competition, you’re challenged to build a model that predicts the probability that a driver will initiate an auto insurance claim in the next year. While Porto Seguro has used machine learning for the past 20 years, they’re looking to Kaggle’s machine learning community to explore new, more powerful methods. A more accurate prediction will allow them to further tailor their prices, and hopefully make auto insurance coverage more accessible to more drivers.",3 files,300.58 MB,This competition awarded ranking points,This competition counted towards tiers,Featured Prediction Competition
Dog Breed Identification,,5 years ago,5 years ago,"['image', 'multiclass classification', 'animals', 'multiclassloss']","1,280","1,405","8,708","Who's a good dog? Who likes ear scratches? Well, it seems those fancy deep neural networks don't have all the answers. However, maybe they can answer that ubiquitous question we all ask when meeting a four-legged stranger: what kind of good pup is that?
In this playground competition, you are provided a strictly canine subset of ImageNet in order to practice fine-grained image categorization. How well you can tell your Norfolk Terriers from your Norwich Terriers? With 120 breeds of dogs and a limited number training images per class, you might find the problem more, err, ruff than you anticipated.
Acknowledgments
We extend our gratitude to the creators of the Stanford Dogs Dataset for making this competition possible: Aditya Khosla, Nityananda Jayadevaprakash, Bangpeng Yao, and Fei-Fei Li.",20581 files,750.43 MB,This competition did not award ranking points,This competition did not count towards tiers,Playground Prediction Competition
WSDM - KKBox's Music Recommendation Challenge,"$5,000 ",5 years ago,5 years ago,['auc'],"1,081","1,229","15,250","The 11th ACM International Conference on Web Search and Data Mining (WSDM 2018) is challenging you to build a better music recommendation system using a donated dataset from KKBOX. WSDM (pronounced ""wisdom"") is one of the the premier conferences on web inspired research involving search and data mining. They're committed to publishing original, high quality papers and presentations, with an emphasis on practical but principled novel models.
Not many years ago, it was inconceivable that the same person would listen to the Beatles, Vivaldi, and Lady Gaga on their morning commute. But, the glory days of Radio DJs have passed, and musical gatekeepers have been replaced with personalizing algorithms and unlimited streaming services.
While the public’s now listening to all kinds of music, algorithms still struggle in key areas. Without enough historical data, how would an algorithm know if listeners will like a new song or a new artist? And, how would it know what songs to recommend brand new users?
WSDM has challenged the Kaggle ML community to help solve these problems and build a better music recommendation system. The dataset is from KKBOX, Asia’s leading music streaming service, holding the world’s most comprehensive Asia-Pop music library with over 30 million tracks. They currently use a collaborative filtering based algorithm with matrix factorization and word embedding in their recommendation system but believe new techniques could lead to better results.
Winners will present their findings at the conference February 6-8, 2018 in Los Angeles, CA. For more information on the conference, click here, and don't forget to check out the other KKBox/WSDM competition: KKBox Music Churn Prediction Challenge",6 files,361.58 MB,This competition did not award ranking points,This competition did not count towards tiers,Research Prediction Competition
WSDM - KKBox's Churn Prediction Challenge,"$5,000 ",5 years ago,5 years ago,"['binary classification', 'logloss']",574,844,"6,251","The 11th ACM International Conference on Web Search and Data Mining (WSDM 2018) is challenging you to build an algorithm that predicts whether a subscription user will churn using a donated dataset from KKBOX. WSDM (pronounced ""wisdom"") is one of the the premier conferences on web inspired research involving search and data mining. They're committed to publishing original, high quality papers and presentations, with an emphasis on practical but principled novel models.
For a subscription business, accurately predicting churn is critical to long-term success. Even slight variations in churn can drastically affect profits.
KKBOX is Asia’s leading music streaming service, holding the world’s most comprehensive Asia-Pop music library with over 30 million tracks. They offer a generous, unlimited version of their service to millions of people, supported by advertising and paid subscriptions. This delicate model is dependent on accurately predicting churn of their paid users.
In this competition you’re tasked to build an algorithm that predicts whether a user will churn after their subscription expires. Currently, the company uses survival analysis techniques to determine the residual membership life time for each subscriber. By adopting different methods, KKBOX anticipates they’ll discover new insights to why users leave so they can be proactive in keeping users dancing.
Winners will present their findings at the WSDM conference February 6-8, 2018 in Los Angeles, CA. For more information on the conference, click here.",10 files,8.95 GB,This competition did not award ranking points,This competition did not count towards tiers,Research Prediction Competition
Cdiscount’s Image Classification Challenge,"$35,000 ",5 years ago,,"['multiclass classification', 'categorizationaccuracy']",626,751,"5,848","Rules Update: The CDiscount team has updated their rules to allow for use of this dataset for research and academic purposes only. To access the data, go to rules and accept the terms to download the data.
Cdiscount.com generated nearly 3 billion euros last year, making it France’s largest non-food e-commerce company. While the company already sells everything from TVs to trampolines, the list of products is still rapidly growing. By the end of this year, Cdiscount.com will have over 30 million products up for sale. This is up from 10 million products only 2 years ago. Ensuring that so many products are well classified is a challenging task.
Currently, Cdiscount.com applies machine learning algorithms to the text description of the products in order to automatically predict their category. As these methods now seem close to their maximum potential, Cdiscount.com believes that the next quantitative improvement will be driven by the application of data science techniques to images.
In this challenge you will be building a model that automatically classifies the products based on their images. As a quick tour of Cdiscount.com's website can confirm, one product can have one or several images. The data set Cdiscount.com is making available is unique and characterized by superlative numbers in several ways:
Almost 9 million products: half of the current catalogue
More than 15 million images at 180x180 resolution
More than 5000 categories: yes this is quite an extreme multi-class classification!",5 files,78.12 GB,This competition awarded ranking points,This competition counted towards tiers,Featured Prediction Competition
Text Normalization Challenge - Russian Language,"$25,000 ",5 years ago,,"['text', 'languages', 'linguistics', 'categorizationaccuracy']",162,175,872,"As many of us can attest, learning another language is tough. Picking up on nuances like slang, dates and times, and local expressions, can often be a distinguishing factor between proficiency and fluency. This challenge is even more difficult for a machine.
Many speech and language applications, including text-to-speech synthesis (TTS) and automatic speech recognition (ASR), require text to be converted from written expressions into appropriate ""spoken"" forms. This is a process known as text normalization, and helps convert 12:47 to ""twelve forty-seven"" and $3.16 into ""three dollars, sixteen cents."" 
However, one of the biggest challenges when developing a TTS or ASR system for a new language is to develop and test the grammar for all these rules, a task that requires quite a bit of linguistic sophistication and native speaker intuition.
Проверено        
12 февраля 2013    двенадцатого февраля две тысячи тринадцатого года
,    sil
Архивировано    
из    
первоисточника    
15 февраля 2013    февраля две тысячи тринадцатого года
.    sil
In this competition, you are challenged to automate the process of developing text normalization grammars via machine learning. This track will focus on Russian, while a separate will focus on English here: English Text Normalization Challenge
About the sponsor
Google's Text Normalization Research Group conducts research and creates tools for the detection, normalization and denormalization of non-standard words such as abbreviations, numbers or currency expressions; and semiotic classes -- text tokens and token sequences that represent particular entities that are semantically constrained, such as measure phrases, addresses or dates. Applications of this work include text-to-speech synthesis, automatic speech recognition, and information extraction/retrieval.",5 files,125.87 MB,This competition awarded ranking points,This competition counted towards tiers,Research Prediction Competition
Text Normalization Challenge - English Language,"$25,000 ",5 years ago,,"['text', 'languages', 'linguistics', 'categorizationaccuracy']",260,279,"1,833","As many of us can attest, learning another language is tough. Picking up on nuances like slang, dates and times, and local expressions, can often be a distinguishing factor between proficiency and fluency. This challenge is even more difficult for a machine.
Many speech and language applications, including text-to-speech synthesis (TTS) and automatic speech recognition (ASR), require text to be converted from written expressions into appropriate ""spoken"" forms. This is a process known as text normalization, and helps convert 12:47 to ""twelve forty-seven"" and $3.16 into ""three dollars, sixteen cents."" 
However, one of the biggest challenges when developing a TTS or ASR system for a new language is to develop and test the grammar for all these rules, a task that requires quite a bit of linguistic sophistication and native speaker intuition.
A    
baby    
giraffe    
is    
6ft    six feet
tall    
and    
weighs    
150lb    one hundred fifty pounds
.    sil
In this competition, you are challenged to automate the process of developing text normalization grammars via machine learning. This track will focus on English, while a separate will focus on Russian here: Russian Text Normalization Challenge
About the sponsor
Google's Text Normalization Research Group conducts research and creates tools for the detection, normalization and denormalization of non-standard words such as abbreviations, numbers or currency expressions; and semiotic classes -- text tokens and token sequences that represent particular entities that are semantically constrained, such as measure phrases, addresses or dates. Applications of this work include text-to-speech synthesis, automatic speech recognition, and information extraction/retrieval.",5 files,95.51 MB,This competition awarded ranking points,This competition counted towards tiers,Research Prediction Competition
Carvana Image Masking Challenge,"$25,000 ",5 years ago,,"['image', 'automobiles and vehicles', 'dice']",734,874,"6,878","As with any big purchase, full information and transparency are key. While most everyone describes buying a used car as frustrating, it’s just as annoying to sell one, especially online. Shoppers want to know everything about the car but they must rely on often blurry pictures and little information, keeping used car sales a largely inefficient, local industry.
Carvana, a successful online used car startup, has seen opportunity to build long term trust with consumers and streamline the online buying process.
An interesting part of their innovation is a custom rotating photo studio that automatically captures and processes 16 standard images of each vehicle in their inventory. While Carvana takes high quality photos, bright reflections and cars with similar colors as the background cause automation errors, which requires a skilled photo editor to change.


In this competition, you’re challenged to develop an algorithm that automatically removes the photo studio background. This will allow Carvana to superimpose cars on a variety of backgrounds. You’ll be analyzing a dataset of photos, covering different vehicles with a wide variety of year, make, and model combinations.",9 files,26.23 GB,This competition awarded ranking points,This competition counted towards tiers,Featured Prediction Competition
New York City Taxi Trip Duration,"$30,000 ",5 years ago,5 years ago,"['tabular', 'regression', 'rmsle']","1,254","1,358","11,193","In this competition, Kaggle is challenging you to build a model that predicts the total ride duration of taxi trips in New York City. Your primary dataset is one released by the NYC Taxi and Limousine Commission, which includes pickup time, geo-coordinates, number of passengers, and several other variables.
Longtime Kagglers will recognize that this competition objective is similar to the ECML/PKDD trip time challenge we hosted in 2015. But, this challenge comes with a twist. Instead of awarding prizes to the top finishers on the leaderboard, this playground competition was created to reward collaboration and collective learning.
We are encouraging you (with cash prizes!) to publish additional training data that other participants can use for their predictions. We also have designated bi-weekly and final prizes to reward authors of kernels that are particularly insightful or valuable to the community.",3 files,89.91 MB,This competition did not award ranking points,This competition did not count towards tiers,Playground Prediction Competition
Web Traffic Time Series Forecasting,"$25,000 ",5 years ago,,"['tabular', 'internet', 'smape']","1,095",681,,"This competition focuses on the problem of forecasting the future values of multiple time series, as it has always been one of the most challenging problems in the field. More specifically, we aim the competition at testing state-of-the-art methods designed by the participants, on the problem of forecasting future web traffic for approximately 145,000 Wikipedia articles.
Sequential or temporal observations emerge in many key real-world problems, ranging from biological data, financial markets, weather forecasting, to audio and video processing. The field of time series encapsulates many different problems, ranging from analysis and inference to classification and forecast. What can you do to help predict future views?
This competition will run as two stages and involves prediction of actual future events. There will be a training stage during which the leaderboard is based on historical data, followed by a stage where participants are scored on real future events.
You have complete freedom in how to produce your forecasts: e.g. use of univariate vs multi-variate models, use of metadata (article identifier), hierarchical time series modeling (for different types of traffic), data augmentation (e.g. using Google Trends data to extend the dataset), anomaly and outlier detection and cleaning, different strategies for missing value imputation, and many more types of approaches.
We thank Google Inc. and Voleon for sponsorship of this competition, and Oren Anava and Vitaly Kuznetsov for organizing it.

Kaggle is excited to partner with research groups to push forward the frontier of machine learning. Research competitions make use of Kaggle's platform and experience, but are largely organized by the research group's data science team. Any questions or concerns regarding the competition data, quality, or topic will be addressed by them.",6 files,611.85 MB,This competition awarded ranking points,This competition counted towards tiers,Research Prediction Competition
NIPS 2017: Defense Against Adversarial Attack,,5 years ago,5 years ago,"['image', 'adversarial learning', 'custom metric']",107,187,109,"This research competition doesn't follow Kaggle's normal submission process. See the Submission Format tab for more details.
Most existing machine learning classifiers are highly vulnerable to adversarial examples. An adversarial example is a sample of input data which has been modified very slightly in a way that is intended to cause a machine learning classifier to misclassify it. In many cases, these modifications can be so subtle that a human observer does not even notice the modification at all, yet the classifier still makes a mistake.
Adversarial examples pose security concerns because they could be used to perform an attack on machine learning systems, even if the adversary has no access to the underlying model.
To accelerate research on adversarial examples, Google Brain is organizing Competition on Adversarial Attacks and Defenses within the NIPS 2017 competition track.
The competition on Adversarial Attacks and Defenses consist of three sub-competitions:
Non-targeted Adversarial Attack. The goal of the non-targeted attack is to slightly modify source image in a way that image will be classified incorrectly by generally unknown machine learning classifier.
Targeted Adversarial Attack. The goal of the targeted attack is to slightly modify source image in a way that image will be classified as specified target class by generally unknown machine learning classifier.
Defense Against Adversarial Attack. The goal of the defense is to build machine learning classifier which is robust to adversarial example, i.e. can classify adversarial images correctly.
In each of the sub-competitions you're invited to make and submit a program which solves the corresponding task. In the end of the competition we will run all attacks against all defenses to evaluate how each of the attacks performs against each of the defenses.
Kaggle is excited to partner with research groups to push forward the frontier of machine learning. Research competitions make use of Kaggle's platform and experience, but are largely organized by the research group's data science team. Any questions or concerns regarding the competition data, rules, quality, or topic will be addressed by them.",4 files,201.94 MB,This competition awarded ranking points,This competition counted towards tiers,Research Prediction Competition
NIPS 2017: Targeted Adversarial Attack,,5 years ago,5 years ago,"['image', 'adversarial learning', 'custom metric']",65,107,65,"This research competition doesn't follow Kaggle's normal submission process. See the Submission Format tab for more details.
Most existing machine learning classifiers are highly vulnerable to adversarial examples. An adversarial example is a sample of input data which has been modified very slightly in a way that is intended to cause a machine learning classifier to misclassify it. In many cases, these modifications can be so subtle that a human observer does not even notice the modification at all, yet the classifier still makes a mistake.
Adversarial examples pose security concerns because they could be used to perform an attack on machine learning systems, even if the adversary has no access to the underlying model.
To accelerate research on adversarial examples, Google Brain is organizing Competition on Adversarial Attacks and Defenses within the NIPS 2017 competition track.
The competition on Adversarial Attacks and Defenses consist of three sub-competitions:
Non-targeted Adversarial Attack. The goal of the non-targeted attack is to slightly modify source image in a way that image will be classified incorrectly by generally unknown machine learning classifier.
Targeted Adversarial Attack. The goal of the targeted attack is to slightly modify source image in a way that image will be classified as specified target class by generally unknown machine learning classifier.
Defense Against Adversarial Attack. The goal of the defense is to build machine learning classifier which is robust to adversarial example, i.e. can classify adversarial images correctly.
In each of the sub-competitions you're invited to make and submit a program which solves the corresponding task. In the end of the competition we will run all attacks against all defenses to evaluate how each of the attacks performs against each of the defenses.
Kaggle is excited to partner with research groups to push forward the frontier of machine learning. Research competitions make use of Kaggle's platform and experience, but are largely organized by the research group's data science team. Any questions or concerns regarding the competition data, rules, quality, or topic will be addressed by them.",4 files,391.35 MB,This competition awarded ranking points,This competition counted towards tiers,Research Prediction Competition
NIPS 2017: Non-targeted Adversarial Attack,,5 years ago,5 years ago,"['image', 'adversarial learning', 'custom metric']",91,153,91,"This research competition doesn't follow Kaggle's normal submission process. See the Submission Format tab for more details.
Most existing machine learning classifiers are highly vulnerable to adversarial examples. An adversarial example is a sample of input data which has been modified very slightly in a way that is intended to cause a machine learning classifier to misclassify it. In many cases, these modifications can be so subtle that a human observer does not even notice the modification at all, yet the classifier still makes a mistake.
Adversarial examples pose security concerns because they could be used to perform an attack on machine learning systems, even if the adversary has no access to the underlying model.
To accelerate research on adversarial examples, Google Brain is organizing Competition on Adversarial Attacks and Defenses within the NIPS 2017 competition track.
The competition on Adversarial Attacks and Defenses consist of three sub-competitions:
Non-targeted Adversarial Attack. The goal of the non-targeted attack is to slightly modify source image in a way that image will be classified incorrectly by generally unknown machine learning classifier.
Targeted Adversarial Attack. The goal of the targeted attack is to slightly modify source image in a way that image will be classified as specified target class by generally unknown machine learning classifier.
Defense Against Adversarial Attack. The goal of the defense is to build machine learning classifier which is robust to adversarial example, i.e. can classify adversarial images correctly.
In each of the sub-competitions you're invited to make and submit a program which solves the corresponding task. In the end of the competition we will run all attacks against all defenses to evaluate how each of the attacks performs against each of the defenses.
Kaggle is excited to partner with research groups to push forward the frontier of machine learning. Research competitions make use of Kaggle's platform and experience, but are largely organized by the research group's data science team. Any questions or concerns regarding the competition data, rules, quality, or topic will be addressed by them.",5 files,543.49 MB,This competition awarded ranking points,This competition counted towards tiers,Research Prediction Competition
Personalized Medicine: Redefining Cancer Treatment,"$15,000 ",5 years ago,,"['text', 'multiclass classification', 'genetics', 'multiclassloss']","1,386","3,032",,"A lot has been said during the past several years about how precision medicine and, more concretely, how genetic testing is going to disrupt the way diseases like cancer are treated.
But this is only partially happening due to the huge amount of manual work still required. Memorial Sloan Kettering Cancer Center (MSKCC) launched this competition, accepted by the NIPS 2017 Competition Track,  because we need your help to take personalized medicine to its full potential.
Once sequenced, a cancer tumor can have thousands of genetic mutations. But the challenge is distinguishing the mutations that contribute to tumor growth (drivers) from the neutral mutations (passengers). 
Currently this interpretation of genetic mutations is being done manually. This is a very time-consuming task where a clinical pathologist has to manually review and classify every single genetic mutation based on evidence from text-based clinical literature.
For this competition MSKCC is making available an expert-annotated knowledge base where world-class researchers and oncologists have manually annotated thousands of mutations.
We need your help to develop a Machine Learning algorithm that, using this knowledge base as a baseline, automatically classifies genetic variations.

Kaggle is excited to partner with research groups to push forward the frontier of machine learning. Research competitions make use of Kaggle's platform and experience, but are largely organized by the research group's data science team. Any questions or concerns regarding the competition data, quality, or topic will be addressed by them.",9 files,177.1 MB,This competition did not award ranking points,This competition did not count towards tiers,Research Prediction Competition
Passenger Screening Algorithm Challenge,"$1,500,000 ",5 years ago,,"['image', 'logloss']",518,396,,"While long lines and frantically shuffling luggage into plastic bins isn’t a fun experience, airport security is a critical and necessary requirement for safe travel.
No one understands the need for both thorough security screenings and short wait times more than U.S. Transportation Security Administration (TSA). They’re responsible for all U.S. airport security, screening more than two million passengers daily.
As part of their Apex Screening at Speed Program, DHS has identified high false alarm rates as creating significant bottlenecks at the airport checkpoints. Whenever TSA’s sensors and algorithms predict a potential threat, TSA staff needs to engage in a secondary, manual screening process that slows everything down. And as the number of travelers increase every year and new threats develop, their prediction algorithms need to continually improve to meet the increased demand.
Currently, TSA purchases updated algorithms exclusively from the manufacturers of the scanning equipment used. These algorithms are proprietary, expensive, and often released in long cycles. In this competition, TSA is stepping outside their established procurement process and is challenging the broader data science community to help improve the accuracy of their threat prediction algorithms. Using a dataset of images collected on the latest generation of scanners, participants are challenged to identify the presence of simulated threats under a variety of object types, clothing types, and body types. Even a modest decrease in false alarms will help TSA significantly improve the passenger experience while maintaining high levels of security.
This is a two-stage competition. Please read our two-stage FAQs to understand more about what this means.
All persons contained in the dataset are volunteers who have agreed to have their images used for this competition. The images may contain sensitive content. We kindly request that you conduct yourself with professionalism, respect, and maturity when working with this data.",10 files,133.98 GB,This competition awarded ranking points,This competition counted towards tiers,Featured Prediction Competition
iMaterialist Challenge at FGVC 2017,,6 years ago,5 years ago,"['clothing and accessories', 'image', 'meanbesterroratk']",28,34,260,"As shoppers move online, it’d be a dream come true to have product attributes in photos detected automatically. But, automatic product recognition is tough because for the same product, a picture can be taken in different lighting, angles, backgrounds, and levels of occlusion. Meanwhile different fine grained attribute labels may look very similar, for example, royal blue vs turquoise in color. Many of today’s general-purpose recognition machines simply can’t perceive such subtle differences between photos.
Tackling issues like this is why the Conference on Computer Vision and Pattern Recognition (CVPR) has put together a workshop specifically for data scientists focused on fine-grained visual categorization called the FGVC4 workshop. As part of this workshop, CVPR is partnering with Google to challenge the data science community to help push the state of the art in automatic image classification.
In this competition, FGVC workshop organizers and Google challenge you to develop algorithms that will help with the an important step towards automatic product detection–accurately assigning attribute labels for product images. Individuals/Teams with top submissions will be invited to present their work live at the FGVC4 workshop.
Kaggle is excited to partner with research groups to push forward the frontier of machine learning. Research competitions make use of Kaggle's platform and experience, but are largely organized by the research group's data science team. Any questions or concerns regarding the competition data, quality, or topic will be addressed by them.",11 files,225.43 MB,This competition did not award ranking points,This competition did not count towards tiers,Research Prediction Competition
iNaturalist Challenge at FGVC 2017,,6 years ago,5 years ago,"['image', 'animals', 'plants', 'meanbesterroratk']",50,60,618,"With so much diversity, accurately classifying animals and plants is a tough challenge. Check out the photos below. Alpaca or Llama? Donkey or mule? Roses or kale?
It’s estimated that our planet contains several million species of plants and animals–many that look really similar to each other. Because of this, a lot of species in the natural world are too hard to classify without an expert.
As part of the FGVC4 workshop at CVPR 2017 we are conducting the iNat Challenge 2017 large scale species classification competition, sponsored by Google. It is estimated that the natural world contains several million species of plants and animals. Without expert knowledge, many of these species are extremely difficult to accurately classify due to their visual similarity. The goal of this competition is to push the state of the art in automatic image classification for real world data that features fine-grained categories, big class imbalances, and large numbers of classes.

The iNat Challenge 2017 dataset contains 5,089 species, with a combined training and validation set of 675,000 images that have been collected and verified by multiple users from inaturalist.org. The dataset features many visually similar species, captured in a wide variety of situations, from all over the world.  Example images, along with their unique GBIF ID numbers (where available), can be viewed here.
Teams with top submissions, at the discretion of the workshop organizers, will be invited to present their work at the FGVC4 workshop. 

Kaggle is excited to partner with research groups to push forward the frontier of machine learning. Research competitions make use of Kaggle's platform and experience, but are largely organized by the research group's data science team. Any questions or concerns regarding the competition data, quality, or topic will be addressed by them.",3 files,38.95 MB,This competition did not award ranking points,This competition did not count towards tiers,Research Prediction Competition
Mercedes-Benz Greener Manufacturing,"$25,000 ",6 years ago,,"['tabular', 'regression', 'automobiles and vehicles', 'r2score']","3,823","4,032","75,157","Since the first automobile, the Benz Patent Motor Car in 1886, Mercedes-Benz has stood for important automotive innovations. These include, for example, the passenger safety cell with crumple zone, the airbag and intelligent assistance systems. Mercedes-Benz applies for nearly 2000 patents per year, making the brand the European leader among premium car makers. Daimler’s Mercedes-Benz cars are leaders in the premium car industry. With a huge selection of features and options, customers can choose the customized Mercedes-Benz of their dreams. .
To ensure the safety and reliability of each and every unique car configuration before they hit the road, Daimler’s engineers have developed a robust testing system. But, optimizing the speed of their testing system for so many possible feature combinations is complex and time-consuming without a powerful algorithmic approach. As one of the world’s biggest manufacturers of premium cars, safety and efficiency are paramount on Daimler’s production lines.
In this competition, Daimler is challenging Kagglers to tackle the curse of dimensionality and reduce the time that cars spend on the test bench. Competitors will work with a dataset representing different permutations of Mercedes-Benz car features to predict the time it takes to pass testing. Winning algorithms will contribute to speedier testing, resulting in lower carbon dioxide emissions without reducing Daimler’s standards.",3 files,351.45 kB,This competition awarded ranking points,This competition counted towards tiers,Featured Prediction Competition
Zillow Prize: Zillow’s Home Value Prediction (Zestimate),"$1,200,000 ",6 years ago,,"['real estate', 'housing', 'custom metric']","3,770","4,241","68,053","Zillow’s Zestimate home valuation has shaken up the U.S. real estate industry since first released 11 years ago.
A home is often the largest and most expensive purchase a person makes in his or her lifetime. Ensuring homeowners have a trusted way to monitor this asset is incredibly important. The Zestimate was created to give consumers as much information as possible about homes and the housing market, marking the first time consumers had access to this type of home value information at no cost.
“Zestimates” are estimated home values based on 7.5 million statistical and machine learning models that analyze hundreds of data points on each property. And, by continually improving the median margin of error (from 14% at the onset to 5% today), Zillow has since become established as one of the largest, most trusted marketplaces for real estate information in the U.S. and a leading example of impactful machine learning.
Zillow Prize, a competition with a one million dollar grand prize, is challenging the data science community to help push the accuracy of the Zestimate even further. Winning algorithms stand to impact the home values of 110M homes across the U.S.
In this million-dollar competition, participants will develop an algorithm that makes predictions about the future sale prices of homes. The contest is structured into two rounds, the qualifying round which opens May 24, 2017 and the private round for the 100 top qualifying teams that opens on Feb 1st, 2018. In the qualifying round, you’ll be building a model to improve the Zestimate residual error. In the final round, you’ll build a home valuation algorithm from the ground up, using external data sources to help engineer new features that give your model an edge over the competition.
Because real estate transaction data is public information, there will be a three-month sales tracking period after each competition round closes where your predictions will be evaluated against the actual sale prices of the homes. The final leaderboard won’t be revealed until the close of the sales tracking period.",6 files,1.37 GB,This competition awarded ranking points,This competition counted towards tiers,Featured Prediction Competition
Instacart Market Basket Analysis,"$25,000 ",6 years ago,,"['food', 'fscoremicro']","2,621","2,621","39,863","Whether you shop from meticulously planned grocery lists or let whimsy guide your grazing, our unique food rituals define who we are. Instacart, a grocery ordering and delivery app, aims to make it easy to fill your refrigerator and pantry with your personal favorites and staples when you need them. After selecting products through the Instacart app, personal shoppers review your order and do the in-store shopping and delivery for you.
Instacart’s data science team plays a big part in providing this delightful shopping experience. Currently they use transactional data to develop models that predict which products a user will buy again, try for the first time, or add to their cart next during a session. Recently, Instacart open sourced this data - see their blog post on 3 Million Instacart Orders, Open Sourced.
In this competition, Instacart is challenging the Kaggle community to use this anonymized data on customer orders over time to predict which previously purchased products will be in a user’s next order. They’re not only looking for the best model, Instacart’s also looking for machine learning engineers to grow their team.
Winners of this competition will receive both a cash prize and a fast track through the recruiting process. For more information about exciting opportunities at Instacart, check out their careers page here or e-mail their recruiting team directly at ml.jobs@instacart.com.",7 files,205.77 MB,This competition awarded ranking points,This competition counted towards tiers,Featured Prediction Competition
Invasive Species Monitoring,,6 years ago,5 years ago,"['image', 'plants', 'auc']",511,526,"4,239","Tangles of kudzu overwhelm trees in Georgia while cane toads threaten habitats in over a dozen countries worldwide. These are just two invasive species of many which can have damaging effects on the environment, the economy, and even human health. Despite widespread impact, efforts to track the location and spread of invasive species are so costly that they’re difficult to undertake at scale.
Currently, ecosystem and plant distribution monitoring depends on expert knowledge. Trained scientists visit designated areas and take note of the species inhabiting them. Using such a highly qualified workforce is expensive, time inefficient, and insufficient since humans cannot cover large areas when sampling.
Because scientists cannot sample a large quantity of areas, some machine learning algorithms are used in order to predict the presence or absence of invasive species in areas that have not been sampled. The accuracy of this approach is far from optimal, but still contributes to approaches to solving ecological problems.
In this playground competition, Kagglers are challenged to develop algorithms to more accurately identify whether images of forests and foliage contain invasive hydrangea or not. Techniques from computer vision alongside other current technologies like aerial imaging can make invasive species monitoring cheaper, faster, and more reliable.
Acknowledgments
Data providers: Christian Requena Mesa, Thore Engel, Amrita Menon, Emma Bradley.",4 files,3.35 GB,This competition did not award ranking points,This competition did not count towards tiers,Playground Prediction Competition
Sberbank Russian Housing Market,"$25,000 ",6 years ago,,"['tabular', 'regression', 'banking', 'housing', 'rmsle']","3,264","3,658","67,581","Housing costs demand a significant investment from both consumers and developers. And when it comes to planning a budget—whether personal or corporate—the last thing anyone needs is uncertainty about one of their biggets expenses. Sberbank, Russia’s oldest and largest bank, helps their customers by making predictions about realty prices so renters, developers, and lenders are more confident when they sign a lease or purchase a building.
Although the housing market is relatively stable in Russia, the country’s volatile economy makes forecasting prices as a function of apartment characteristics a unique challenge. Complex interactions between housing features such as number of bedrooms and location are enough to make pricing predictions complicated. Adding an unstable economy to the mix means Sberbank and their customers need more than simple regression models in their arsenal.
In this competition, Sberbank is challenging Kagglers to develop algorithms which use a broad spectrum of features to predict realty prices. Competitors will rely on a rich dataset that includes housing data and macroeconomic patterns. An accurate forecasting model will allow Sberbank to provide more certainty to their customers in an uncertain economy.",5 files,22.71 MB,This competition awarded ranking points,This competition counted towards tiers,Featured Prediction Competition
Planet: Understanding the Amazon from Space,"$60,000 ",6 years ago,,"['image', 'forestry', 'fscorebetamicro']",936,"1,181","20,666","Every minute, the world loses an area of forest the size of 48 football fields. And deforestation in the Amazon Basin accounts for the largest share, contributing to reduced biodiversity, habitat loss, climate change, and other devastating effects. But better data about the location of deforestation and human encroachment on forests can help governments and local stakeholders respond more quickly and effectively.
Planet, designer and builder of the world’s largest constellation of Earth-imaging satellites, will soon be collecting daily imagery of the entire land surface of the earth at 3-5 meter resolution. While considerable research has been devoted to tracking changes in forests, it typically depends on coarse-resolution imagery from Landsat (30 meter pixels) or MODIS (250 meter pixels). This limits its effectiveness in areas where small-scale deforestation or forest degradation dominate.
Furthermore, these existing methods generally cannot differentiate between human causes of forest loss and natural causes. Higher resolution imagery has already been shown to be exceptionally good at this, but robust methods have not yet been developed for Planet imagery.
In this competition, Planet and its Brazilian partner SCCON are challenging Kagglers to label satellite image chips with atmospheric conditions and various classes of land cover/land use. Resulting algorithms will help the global community better understand where, how, and why deforestation happens all over the world - and ultimately how to respond.
To dig into/explore more Planet data, sign up for a free account.
And if you're interested in building applications on Planet data, check out our Application Developer Program.
Getting Started
Review the data page, which includes detailed information about the labels and the labeling process.
Download a subsample of the data to get familiar with how it looks.
Explore the subsample on Kernels. We’ve created a notebook for you to get started.",5 files,7.61 MB,This competition awarded ranking points,This competition counted towards tiers,Featured Prediction Competition
NOAA Fisheries Steller Sea Lion Population Count,"$25,000 ",6 years ago,,"['image', 'animals', 'water bodies', 'mcrmse']",385,463,"6,015","Steller sea lions in the western Aleutian Islands have declined 94 percent in the last 30 years. The endangered western population, found in the North Pacific, are the focus of conservation efforts which require annual population counts. Specially trained scientists at NOAA Fisheries Alaska Fisheries Science Center conduct these surveys using airplanes and unoccupied aircraft systems to collect aerial images. Having accurate population estimates enables us to better understand factors that may be contributing to lack of recovery of Stellers in this area.
Currently, it takes biologists up to four months to count sea lions from the thousands of images NOAA Fisheries collects each year. Once individual counts are conducted, the tallies must be reconciled to confirm their reliability. The results of these counts are time-sensitive.
In this competition, Kagglers are invited to develop algorithms which accurately count the number of sea lions in aerial photographs. Automating the annual population count will free up critical resources allowing NOAA Fisheries to focus on ensuring we hear the sea lion’s roar for many years to come. Plus, advancements in computer vision applied to aerial population counts may also greatly benefit other endangered species.
Resources
Learn more about research being done to better understand what's going on with the endangered Steller sea lion populations by joining scientists on a research vessel to the western Aleutian Islands in the video below.",5 files,103.01 GB,This competition awarded ranking points,This competition counted towards tiers,Featured Prediction Competition
Quora Question Pairs,"$25,000 ",6 years ago,,"['tabular', 'internet', 'text', 'linguistics', 'logloss']","3,295","3,850","53,687","Where else but Quora can a physicist help a chef with a math problem and get cooking tips in return? Quora is a place to gain and share knowledge—about anything. It’s a platform to ask questions and connect with people who contribute unique insights and quality answers. This empowers people to learn from each other and to better understand the world.
Over 100 million people visit Quora every month, so it's no surprise that many people ask similarly worded questions. Multiple questions with the same intent can cause seekers to spend more time finding the best answer to their question, and make writers feel they need to answer multiple versions of the same question. Quora values canonical questions because they provide a better experience to active seekers and writers, and offer more value to both of these groups in the long term.
Currently, Quora uses a Random Forest model to identify duplicate questions. In this competition, Kagglers are challenged to tackle this natural language processing problem by applying advanced techniques to classify whether question pairs are duplicates or not. Doing so will make it easier to find high quality answers to questions resulting in an improved experience for Quora writers, seekers, and readers.",4 files,523.24 MB,This competition awarded ranking points,This competition counted towards tiers,Featured Prediction Competition
Intel & MobileODT Cervical Cancer Screening,"$100,000 ",6 years ago,,"['image', 'multiclass classification', 'healthcare', 'multiclassloss']",848,"1,365",,"Cervical cancer is so easy to prevent if caught in its pre-cancerous stage that every woman should have access to effective, life-saving treatment no matter where they live. Today, women worldwide in low-resource settings are benefiting from programs where cancer is identified and treated in a single visit. However, due in part to lacking expertise in the field, one of the greatest challenges of these cervical cancer screen and treat programs is determining the appropriate method of treatment which can vary depending on patients’ physiological differences.
Especially in rural parts of the world, many women at high risk for cervical cancer are receiving treatment that will not work for them due to the position of their cervix. This is a tragedy: health providers are able to identify high risk patients, but may not have the skills to reliably discern which treatment which will prevent cancer in these women. Even worse, applying the wrong treatment has a high cost. A treatment which works effectively for one woman may obscure future cancerous growth in another woman, greatly increasing health risks.
Currently, MobileODT offers a Quality Assurance workflow to support remote supervision which helps healthcare providers make better treatment decisions in rural settings. However, their workflow would be greatly improved given the ability to make real-time determinations about patients’ treatment eligibility based on cervix type.
In this competition, Intel is partnering with MobileODT to challenge Kagglers to develop an algorithm which accurately identifies a woman’s cervix type based on images. Doing so will prevent ineffectual treatments and allow healthcare providers to give proper referral for cases that require more advanced treatment.
Competition Partner
MobileODT has developed and sells the Enhanced Visual Assessment (EVA) System, a digital toolkit for health care workers of every level to provide expert services to patients, anchored at the point-of-care by an FDA-approved, intelligent, mobile-phone based medical device. Combining the algorithmic power of biomedical optics with the computational capabilities and connectivity of mobile phones, MobileODT's connected, intelligent medical systems can be used everywhere, under nearly any conditions. MobileODT's first product, the FDA approved EVA System for colposcopy, is in use by health providers in 31 hospital systems across the US, and in 22 countries, to better screen and treat women for cervical cancer and to conduct forensic colposcopy.",8734 files,46.53 GB,This competition awarded ranking points,This competition counted towards tiers,Featured Prediction Competition
Google Cloud & YouTube-8M Video Understanding Challenge,"$100,000 ",6 years ago,6 years ago,"['internet', 'image', 'custom metric']",655,821,"6,911","Video captures a cross-section of our society. And major advances in analyzing and understanding video have the potential to touch all aspects of life from learning and communication to entertainment and play. In this competition, Google is inviting the Kaggle community to join efforts to accelerate research in large-scale video understanding, while giving participants access to the Google Cloud Machine Learning Engine.
Today, one of the greatest obstacles to rapid improvements in video understanding research has been the lack of large-scale, labeled datasets open to the public. For example, the availability of large, labeled datasets such as ImageNet has enabled continued breakthroughs in machine learning and machine perception. To that end, Google’s recent release of the YouTube-8M (YT-8M) dataset represents a significant step in this direction. Making this resource open to everyone from students and industry professionals is expected to kickstart innovation in areas such as representation learning and video modeling architectures.
In this competition, you are challenged to develop classification algorithms which accurately assign video-level labels using the new and improved YT-8M V2 dataset. The dataset was created from over 7 million YouTube videos (450,000 hours of video) and includes video labels from a vocabulary of 4716 classes (3.4 labels/video on average). It also comes with pre-extracted audio & visual features from every second of video (3.2B feature vectors in total). By taking part, Kagglers will not only play a pivotal role in setting state-of-the-art benchmarks, but also improve search and organization of video archives.
Getting Started
Review the data page for special instructions on how to access the competition's data. It will be hosted on Google Cloud. Participants have the option to download the data to work locally or work within the Google Cloud ML beta Platform.
Review the tutorial on Getting Started with Google Cloud, and try the starter code.
Sign up for a Google Cloud ML Platform free trial account. The free trial account includes $300 in credits!
We've also provided a subsample of the data to explore on Kernels. Take a look at this Python notebook and create your own.
Don't forget to review the prize eligibility details, which includes requirements for code open-sourcing and a paper submission.
Because Cloud ML is currently a beta product, Google welcomes the opportunity to hear your feedback about using the tool. Please share your questions and thoughts on the competition's forums. Additional resources specific to the YT-8M dataset and Google Cloud ML can be found here.
Acknowledgements
Google Cloud Machine Learning, Competition Sponsor
Google Cloud Machine Learning is a managed service that enables you to easily build machine learning models, that work on any type of data, of any size. Create your model with the powerful TensorFlow framework that powers many Google products, from GooglePhotos to Google Cloud Speech. Build models of any size with our managed scalable infrastructure. Your trained model is immediately available for use with our global prediction platform that can support thousands of users and TBs of data. The service is integrated with Google Cloud Dataflow for pre-processing, allowing you to access data from Google Cloud Storage, Google BigQuery, and others.",2 files,1.8 MB,This competition awarded ranking points,This competition counted towards tiers,Featured Prediction Competition
Two Sigma Connect: Rental Listing Inquiries,,6 years ago,6 years ago,"['tabular', 'text', 'multiclass classification', 'housing', 'multiclassloss']","2,480","2,480","46,856","Finding the perfect place to call your new home should be more than browsing through endless listings. RentHop makes apartment search smarter by using data to sort rental listings by quality. But while looking for the perfect apartment is difficult enough, structuring and making sense of all available real estate data programmatically is even harder. Two Sigma and RentHop, a portfolio company of Two Sigma Ventures, invite Kagglers to unleash their creative engines to uncover business value in this unique recruiting competition.

Two Sigma invites you to apply your talents in this recruiting competition featuring rental listing data from RentHop. Kagglers will predict the number of inquiries a new listing receives based on the listing’s creation date and other features. Doing so will help RentHop better handle fraud control, identify potential listing quality issues, and allow owners and agents to better understand renters’ needs and preferences.

Two Sigma has been at the forefront of applying technology and data science to financial forecasts. While their pioneering advances in big data, AI, and machine learning in the financial world have been pushing the industry forward, as with all other scientific progress, they are driven to make continual progress. This challenge is an opportunity for competitors to gain a sneak peek into Two Sigma's data science work outside of finance.
Acknowledgments
This competition is co-hosted by Two Sigma and RentHop (a portfolio company of Two Sigma Ventures, which is a division of Two Sigma Investments) to encourage creativity in using real world data to solve everyday problems.",5 files,117.22 MB,This competition awarded ranking points,This competition counted towards tiers,Recruitment Prediction Competition
March Machine Learning Mania 2017,,6 years ago,,"['sports', 'basketball', 'logloss']",441,491,771,"Another year, another chance to predict the upsets, call the probabilities, and put your bracketology skills to the leaderboard test. In our fourth annual March Machine Learning Mania competition, Kagglers will once again join the millions of fans who attempt to predict the outcomes of this year's US men's college basketball tournament. But unlike most fans, you will pick the winners and losers using a combination of rich historical data and computing power, while the ground truth unfolds on national television.
In the first stage of the competition, Kagglers will rely on results of past tournaments to build and test models. We encourage you to post any useful external data as a dataset. In the second stage, competitors will forecast outcomes of all possible match-ups in the 2017 tournament. You don't need to participate in the first stage to enter the second. The first stage exists to incentivize model building and provide a means to score predictions. The real competition is forecasting the 2017 results.",9 files,12.11 MB,This competition awarded 0.5X ranking points,This competition did not count towards tiers,Playground Prediction Competition
Data Science Bowl 2017,"$1,000,000 ",6 years ago,,"['image', 'binary classification', 'healthcare', 'logloss']","1,972","1,676",,"In the United States, lung cancer strikes 225,000 people every year, and accounts for $12 billion in health care costs. Early detection is critical to give patients the best chance at recovery and survival.
One year ago, the office of the U.S. Vice President spearheaded a bold new initiative, the Cancer Moonshot, to make a decade's worth of progress in cancer prevention, diagnosis, and treatment in just 5 years.
In 2017, the Data Science Bowl will be a critical milestone in support of the Cancer Moonshot by convening the data science and medical communities to develop lung cancer detection algorithms.
Using a data set of thousands of high-resolution lung scans provided by the National Cancer Institute, participants will develop algorithms that accurately determine when lesions in the lungs are cancerous. This will dramatically reduce the false positive rate that plagues the current detection technology, get patients earlier access to life-saving interventions, and give radiologists more time to spend with their patients.
This year, the Data Science Bowl will award $1 million in prizes to those who observe the right patterns, ask the right questions, and in turn, create unprecedented impact around cancer screening care and prevention. The funds for the prize purse will be provided by the Laura and John Arnold Foundation.
Visit DataScienceBowl.com to:
• Sign up to receive news about the competition
• Learn about the history of the Data Science Bowl and past competitions
• Read our latest insights on emerging analytics techniques
Acknowledgments
The Data Science Bowl is presented by
Competition Sponsors
Laura and John Arnold Foundation
Cancer Imaging Program of the National Cancer Institute
American College of Radiology
Amazon Web Services
NVIDIA
Data Support Providers
National Lung Screening Trial
The Cancer Imaging Archive
Diagnostic Image Analysis Group, Radboud University
Lahey Hospital & Medical Center
Copenhagen University Hospital
Supporting Organizations 
Bayes Impact
Black Data Processng Associates
Code the Change
Data Community DC
DataKind
Galvanize
Great Minds in STEM
Hortonworks
INFORMS
Lesbians Who Tech
NSBE
Society of Asian Scientists & Engineers
Society of Women Engineers
University of Texas Austin, Business Analytics Program,
McCombs School of Business
US Dept. of Health and Human Services
US Food and Drug Administration
Women in Technology
Women of Cyberjutsu",1 files,108 B,This competition awarded ranking points,This competition counted towards tiers,Featured Prediction Competition
Santa's Uncertain Bags,,6 years ago,,"['tabular', 'custom metric']",692,743,"18,383","All was well in Santa's workshop. The gifts were made, the route was planned, the naughty and nice list complete. Santa thought this would finally be the year he didn't need Kaggle's help with his combinatorial conundrums. At last, the Claus family could take the elves and reindeer on that well deserved vacation to the South Pole.
Then, with just days until the big night, Santa received an email from a panicked database admin elf. Attached was a server log with the six least jolly words a jolly old St. Nick could read:
ALTER TABLE Gifts
DROP COLUMN Weight
One of the North Pole elf interns had mistakenly deleted the weights for all of the inventory in the workshop! Santa didn't have a backup (remember, this is a guy who makes a list and checks it twice) and, without knowing each present's weight, he didn't know how he would safely pack his many gift bags. Gifts were already on their way to the sleigh packing facility and there wasn't time to re-weigh all the presents. It was once again necessary to summon the holiday talents of Kaggle's elite.
Can you help Santa fill his multiple bags with sets of uncertain gifts? Save the season by turning Santa's uncertain probabilities into presents for good little boys and girls.",2 files,84.85 kB,This competition awarded ranking points,This competition counted towards tiers,Playground Prediction Competition
Dstl Satellite Imagery Feature Detection,"$100,000 ",6 years ago,,"['image', 'multiclass classification', 'custom metric']",419,519,"5,542","The proliferation of satellite imagery has given us a radically improved understanding of our planet. It has enabled us to better achieve everything from mobilizing resources during disasters to monitoring effects of global warming. What is often taken for granted is that advancements such as these have relied on labeling features of significance like building footprints and roadways fully by hand or through imperfect semi-automated methods.
As these large, complex datasets continue to increase exponentially in number, the Defence Science and Technology Laboratory (Dstl) is seeking novel solutions to alleviate the burden on their image analysts. In this competition, Kagglers are challenged to accurately classify features in overhead imagery. Automating feature labeling will not only help Dstl make smart decisions more quickly around the defense and security of the UK, but also bring innovation to computer vision methodologies applied to satellite imagery.",6 files,21.69 GB,This competition awarded ranking points,This competition counted towards tiers,Featured Prediction Competition
Two Sigma Financial Modeling Challenge,"$100,000 ",6 years ago,,"['finance', 'custom metric']","2,063","2,317","24,573","How can we use the world’s tools and intelligence to forecast economic outcomes that can never be entirely predictable? This question is at the core of countless economic activities around the world – including at Two Sigma Investments, who has been applying technology and systematic strategies to financial trading since 2001.
For over 15 years, Two Sigma has been at the forefront of applying technology and data science to financial forecasts. While their pioneering advances in big data, AI, and machine learning in the financial world have been pushing the industry forward, as with all other scientific progress, they are driven to make continual progress. Through this exclusive partnership, Two Sigma is excited to explore what untapped value Kaggle's diverse data science community can discover in the financial markets.
Economic opportunity depends on the ability to deliver singularly accurate forecasts in a world of uncertainty. By accurately predicting financial movements, Kagglers will learn about scientifically-driven approaches to unlocking significant predictive capability. Two Sigma is excited to find predictive value and gain a better understanding of the skills offered by the global data science crowd.
What is a Code Competition?
Welcome to Kaggle's very first Code Competition! In contrast to our traditional competitions, where competitors submit only prediction outputs, participants in Code Competitions will submit their code via Kaggle Kernels. All kernels are private by default in Code Competitions. You can build your models in Kernels by running them on a training set and, once you're ready to submit your code, your model's performance will be evaluated against the test set and your score and public leaderboard position revealed. As with our traditional competitions, we still maintain a private leaderboard test set, which your code is also evaluated against for final scoring, but is not revealed until the competition closes.
Since Code Competitions are brand new, we ask for your patience if you encounter bugs or frustrating platform quirks. Please report any issues you find in the forums and we'll do our best to respond.
Who owns my code?
You do. Even though you are submitting code, the intellectual property exchange here works similarly to a standard prediction competition, whereby prize winners have the option to grant a non-exclusive license in exchange for a prize. There is a new addition to the terms for Code Competitions: Kaggle and the competition host reserve a right to review submissions ""for purposes related to evaluation and scoring in this Competition, including but not limited to the assessment of potential cheating behavior."" Please refer to the official competition rules for full details.
Getting Started
Review the data page for details about the data and the evaluation metric. You may download the train set for local training.
Take a look at the tutorial covering the new code submission process under the submission instructions tab. You'll find step-by-step instructions, some helpful pointers, plus details on environment constraints.
Get feedback on your benchmark code and share exploratory analyses with the community by making any of your kernels public.
Improve your score!
Note: there is no cost of entry for participation.",1 files,0 B,This competition awarded ranking points,This competition counted towards tiers,Featured Code Competition
The Nature Conservancy Fisheries Monitoring,"$150,000 ",6 years ago,,"['image', 'multiclass classification', 'multiclassloss']","2,293","2,059",,"Nearly half of the world depends on seafood for their main source of protein. In the Western and Central Pacific, where 60% of the world’s tuna is caught, illegal, unreported, and unregulated fishing practices are threatening marine ecosystems, global seafood supplies and local livelihoods. The Nature Conservancy is working with local, regional and global partners to preserve this fishery for the future.
Currently, the Conservancy is looking to the future by using cameras to dramatically scale the monitoring of fishing activities to fill critical science and compliance monitoring data gaps. Although these electronic monitoring systems work well and are ready for wider deployment, the amount of raw data produced is cumbersome and expensive to process manually.
The Conservancy is inviting the Kaggle community to develop algorithms to automatically detect and classify species of tunas, sharks and more that fishing boats catch, which will accelerate the video review process. Faster review and more reliable data will enable countries to reallocate human capital to management and enforcement activities which will have a positive impact on conservation and our planet.
Machine learning has the ability to transform what we know about our oceans and how we manage them. You can be part of the solution.
Resources
You can learn more about this competition and The Nature Conservancy in the video below.",5 files,2.27 GB,This competition awarded ranking points,This competition counted towards tiers,Featured Prediction Competition
"Ghouls, Goblins, and Ghosts... Boo!",,6 years ago,6 years ago,"['tabular', 'multiclass classification', 'categorizationaccuracy']",763,802,"7,522","Get out your dowsing rods, electromagnetic sensors, … and gradient boosting machines. Kaggle is haunted and we need your help. After a month of making scientific observations and taking careful measurements, we’ve determined that 900 ghouls, ghosts, and goblins are infesting our halls and frightening our data scientists. When trying garlic, asking politely, and using reverse psychology didn't work, it became clear that machine learning is the only answer to banishing our unwanted guests.
So now the hour has come to put the data we’ve collected in your hands. We’ve managed to identify 371 of the ghastly creatures, but need your help to vanquish the rest. And only an accurate classification algorithm can thwart them. Use bone length measurements, severity of rot, extent of soullessness, and other characteristics to distinguish (and extinguish) the intruders. Are you ghost-busters up for the challenge?",3 files,38.44 kB,This competition did not award ranking points,This competition did not count towards tiers,Playground Prediction Competition
Transfer Learning on Stack Exchange Tags,,6 years ago,6 years ago,"['tabular', 'text', 'multiclass classification', 'fscoremicro']",380,600,"3,514","What does physics have in common with biology, cooking, cryptography, diy, robotics, and travel? If you answered ""all pursuits are governed by the immutable laws of physics"" we'll begrudgingly give you partial credit. If you answered ""all were chosen randomly by a scheming Kaggle employee for a twisted transfer learning competition"", congratulations, we accept your answer and mark the question as solved.
In this competition, we provide the titles, text, and tags of Stack Exchange questions from six different sites. We then ask for tag predictions on unseen physics questions. Solving this problem via a standard machine approach might involve training an algorithm on a corpus of related text. Here, you are challenged to train on material from outside the field. Can an algorithm learn appropriate physics tags from ""extreme-tourism Antarctica""? Let's find out.
Kaggle is hosting this competition for the data science community to use for fun and education. This dataset originates from the Stack Exchange data dump.",8 files,50.88 MB,This competition did not award ranking points,This competition did not count towards tiers,Playground Prediction Competition
Santander Product Recommendation,"$60,000 ",6 years ago,,"['tabular', 'multiclass classification', 'banking', 'map@{k}']","1,779","2,051","28,732","Ready to make a downpayment on your first house? Or looking to leverage the equity in the home you have? To support needs for a range of financial decisions, Santander Bank offers a lending hand to their customers through personalized product recommendations.
Under their current system, a small number of Santander’s customers receive many recommendations while many others rarely see any resulting in an uneven customer experience. In their second competition, Santander is challenging Kagglers to predict which products their existing customers will use in the next month based on their past behavior and that of similar customers.
With a more effective recommendation system in place, Santander can better meet the individual needs of all customers and ensure their satisfaction no matter where they are in life.
Disclaimer: This data set does not include any real Santander Spain's customer, and thus it is not representative of Spain's customer base. ",3 files,240.06 MB,This competition awarded ranking points,This competition counted towards tiers,Featured Prediction Competition
Allstate Claims Severity,,6 years ago,,"['tabular', 'regression', 'mae']","3,045","3,045","52,353","When you’ve been devastated by a serious car accident, your focus is on the things that matter the most: family, friends, and other loved ones. Pushing paper with your insurance agent is the last place you want your time or mental energy spent. This is why Allstate, a personal insurer in the United States, is continually seeking fresh ideas to improve their claims service for the over 16 million households they protect.
Allstate is currently developing automated methods of predicting the cost, and hence severity, of claims. In this recruitment challenge, Kagglers are invited to show off their creativity and flex their technical chops by creating an algorithm which accurately predicts claims severity. Aspiring competitors will demonstrate insight into better ways to predict claims severity for the chance to be part of Allstate’s efforts to ensure a worry-free customer experience.
New to Kaggle? This competition is a recruiting competition, your chance to get a foot in the door with the hiring team at Allstate.",6 files,142.87 MB,This competition awarded ranking points,This competition counted towards tiers,Recruitment Prediction Competition
Outbrain Click Prediction,"$25,000 ",6 years ago,,"['tabular', 'internet', 'map@{k}']",978,"1,301","6,642","The internet is a stimulating treasure trove of possibility. Every day we stumble on news stories relevant to our communities or experience the serendipity of finding an article covering our next travel destination. Outbrain, the web’s leading content discovery platform, delivers these moments while we surf our favorite sites.
Currently, Outbrain pairs relevant content with curious readers in about 250 billion personalized recommendations every month across many thousands of sites. In this competition, Kagglers are challenged to predict which pieces of content its global base of users are likely to click on. Improving Outbrain’s recommendation algorithm will mean more users uncover stories that satisfy their individual tastes.",11 files,38.96 GB,This competition awarded ranking points,This competition counted towards tiers,Featured Prediction Competition
Dogs vs. Cats Redux: Kernels Edition,,6 years ago,6 years ago,"['image', 'binary classification', 'animals', 'logloss']","1,314","1,334","9,449","In 2013, we hosted one of our favorite for-fun competitions:  Dogs vs. Cats. Much has since changed in the machine learning landscape, particularly in deep learning and image analysis. Back then, a tensor flow was the diffusion of the creamer in a bored mathematician's cup of coffee. Now, even the cucumber farmers are neural netting their way to a bounty.
Much has changed at Kaggle as well. Our online coding environment Kernels didn't exist in 2013, and so it was that we approached sharing by scratching primitive glpyhs on cave walls with sticks and sharp objects. No more. Now, Kernels have taken over as the way to share code on Kaggle. IPython is out and Jupyter Notebook is in. We even have TensorFlow. What more could a data scientist ask for? But seriously, what more? Pull requests welcome.
We are excited to bring back the infamous Dogs vs. Cats classification problem as a playground competition with kernels enabled. Although modern techniques may make light of this once-difficult problem, it is through practice of new techniques on old datasets that we will make light of machine learning's future challenges.",3 files,854.51 MB,This competition did not award ranking points,This competition did not count towards tiers,Playground Prediction Competition
Melbourne University AES/MathWorks/NIH Seizure Prediction,"$20,000 ",6 years ago,,"['healthcare', 'diseases', 'auc']",477,645,"10,047","Epilepsy afflicts nearly 1% of the world's population, and is characterized by the occurrence of spontaneous seizures. For many patients, anticonvulsant medications can be given at sufficiently high doses to prevent seizures, but patients frequently suffer side effects. For 20-40% of patients with epilepsy, medications are not effective. Even after surgical removal of epilepsy, many patients continue to experience spontaneous seizures. Despite the fact that seizures occur infrequently, patients with epilepsy experience persistent anxiety due to the possibility of a seizure occurring.
Seizure forecasting systems have the potential to help patients with epilepsy lead more normal lives. In order for electrical brain activity (EEG) based seizure forecasting systems to work effectively, computational algorithms must reliably identify periods of increased probability of seizure occurrence. If these seizure-permissive brain states can be identified, devices designed to warn patients of impeding seizures would be possible. Patients could avoid potentially dangerous activities like driving or swimming, and medications could be administered only when needed to prevent impending seizures, reducing overall side effects.
The Competition
Transitioning from the Kaggle contests held on seizure detection and seizure prediction in 2014 that primarily involved long-term electrical brain activity recordings from dogs, the current contest focuses on seizure prediction using long-term electrical brain activity recordings from humans obtained from the world-first clinical trial of the implantable NeuroVista Seizure Advisory System.
Human brain activity was recorded in the form of intracranial EEG (iEEG), which involves electrodes positioned on the surface of the cerebral cortex and the recording of electrical signals with an ambulatory monitoring system. These are long duration recordings, spanning multiple months up to multiple years and recording large numbers of seizures in some humans. The challenge is to distinguish between ten minute long data clips covering an hour prior to a seizure, and ten minute iEEG clips of interictal activity. 
Acknowledgments 
This competition is sponsored by MathWorks, the National Institutes of Health (NINDS), the American Epilepsy Society and the University of Melbourne, and organised in partnership with the Alliance for Epilepsy Research, the University of Pennsylvania and the Mayo Clinic.
      References",,,This competition awarded ranking points,This competition counted towards tiers,Research Prediction Competition
Painter by Numbers,,7 years ago,6 years ago,"['image', 'auc']",41,50,177,"With an original Picasso carrying a 106 million dollar price tag, identifying an authentic work of art from a forgery is a high-stakes industry. While algorithms have gotten good at telling us if a still life is of a basket of apples or a sunflower bouquet, they aren't yet able to tell us with certainty if both paintings are by van Gogh.  
In this playground competition, we're challenging Kagglers to examine pairs of paintings and determine if they are by the same artist. This is an excellent opportunity to improve your computer vision skills and engage with a unique dataset of art. From the movement of brushstrokes to the use of light and dark, successful algorithms will likely incorporate many aspects of a painter's unique style. 
Resources
neural algorithm
How Do We See Art: An Eye-Tracker Study
Acknowledgments
Many of the images in this dataset were obtained from wikiart.org. Additional paintings were provided by artists whose contributions will be acknowledged at the close of the competition.
This playground competition and its datasets were prepared by Small Yellow Duck (Kiri Nichol). This includes the design of the pairwise-evaluation scheme.",17 files,90.51 GB,This competition did not award ranking points,This competition did not count towards tiers,Playground Prediction Competition
Draper Satellite Image Chronology,"$75,000 ",7 years ago,,"['image', 'maspearmanr']",399,422,"2,676","Imagine a world where we can use satellite images to help find better access to clean water, prevent poaching of wildlife, predict storms more efficiently, optimize traffic patterns more readily, and inform human behaviors to mitigate the spread of disease.
Thanks to a marked increase of satellites in orbit, we will be able to capture images – and the information contained within – of nearly every place on Earth, every day by 2017. However, our ability to analyze datasets of these images has not advanced as quickly. Changes from day to day in images of the same location are subtle, can be hard to detect, and are difficult to understand in terms of their significance.
In this competition, Draper provides a unique dataset of images taken at the same locations over 5 days. Kagglers are challenged to predict the chronological order of the photos taken at each location. Accurately doing so could uncover approaches that have a global impact on commerce, science, and humanitarian works.",3441 files,36.07 GB,This competition awarded ranking points,This competition counted towards tiers,Featured Prediction Competition
Expedia Hotel Recommendations,"$25,000 ",7 years ago,,"['tabular', 'recommender systems', 'hotels and accommodations', 'map@{k}']","1,971","2,176","22,632","Planning your dream vacation, or even a weekend escape, can be an overwhelming affair. With hundreds, even thousands, of hotels to choose from at every destination, it's difficult to know which will suit your personal preferences. Should you go with an old standby with those pillow mints you like, or risk a new hotel with a trendy pool bar? 
Expedia wants to take the proverbial rabbit hole out of hotel search by providing personalized hotel recommendations to their users. This is no small task for a site with hundreds of millions of visitors every month!
Currently, Expedia uses search parameters to adjust their hotel recommendations, but there aren't enough customer specific data to personalize them for each user. In this competition, Expedia is challenging Kagglers to contextualize customer data and predict the likelihood a user will stay at 100 different hotel groups.
The data in this competition is a random selection from Expedia and is not representative of the overall statistics. ",4 files,4.52 GB,This competition awarded ranking points,This competition counted towards tiers,Featured Prediction Competition
Kobe Bryant Shot Selection,,7 years ago,6 years ago,"['tabular', 'binary classification', 'basketball', 'logloss']","1,117","1,200","10,389","Kobe Bryant marked his retirement from the NBA by scoring 60 points in his final game as a Los Angeles Laker on Wednesday, April 12, 2016. Drafted into the NBA at the age of 17, Kobe earned the sport’s highest accolades throughout his long career.
Using 20 years of data on Kobe's swishes and misses, can you predict which shots will find the bottom of the net? This competition is well suited for practicing classification basics, feature engineering, and time series analysis. Practice got Kobe an eight-figure contract and 5 championship rings. What will it get you?
Acknowledgements
Kaggle is hosting this competition for the data science community to use for fun and education. For more data on Kobe and other NBA greats, visit stats.nba.com.",2 files,708.3 kB,This competition did not award ranking points,This competition did not count towards tiers,Playground Prediction Competition
State Farm Distracted Driver Detection,"$65,000 ",7 years ago,,"['image', 'automobiles and vehicles', 'multiclassloss']","1,438","1,679","25,571","We've all been there: a light turns green and the car in front of you doesn't budge. Or, a previously unremarkable vehicle suddenly slows and starts swerving from side-to-side.
When you pass the offending driver, what do you expect to see? You certainly aren't surprised when you spot a driver who is texting, seemingly enraptured by social media, or in a lively hand-held conversation on their phone.
According to the CDC motor vehicle safety division, one in five car accidents is caused by a distracted driver. Sadly, this translates to 425,000 people injured and 3,000 people killed by distracted driving every year.
State Farm hopes to improve these alarming statistics, and better insure their customers, by testing whether dashboard cameras can automatically detect drivers engaging in distracted behaviors. Given a dataset of 2D dashboard camera images, State Farm is challenging Kagglers to classify each driver's behavior. Are they driving attentively, wearing their seatbelt, or taking a selfie with their friends in the backseat?",102152 files,4.31 GB,This competition awarded ranking points,This competition counted towards tiers,Featured Prediction Competition
Shelter Animal Outcomes,,7 years ago,6 years ago,"['tabular', 'multiclass classification', 'animals', 'multiclassloss']","1,599","1,755","15,596","Every year, approximately 7.6 million companion animals end up in US shelters. Many animals are given up as unwanted by their owners, while others are picked up after getting lost or taken out of cruelty situations. Many of these animals find forever families to take them home, but just as many are not so lucky. 2.7 million dogs and cats are euthanized in the US every year.
Using a dataset of intake information including breed, color, sex, and age from the Austin Animal Center, we're asking Kagglers to predict the outcome for each animal.
We also believe this dataset can help us understand trends in animal outcomes. These insights could help shelters focus their energy on specific animals who need a little extra help finding a new home. We encourage you to publish your insights on Scripts so they are publicly accessible.
Acknowledgements
Kaggle is hosting this competition for the machine learning community to use for data science practice and social good. The dataset is brought to you by Austin Animal Center. Shelter animal statistics were taken from the ASPCA.
Glamour shots of Kaggle's shelter pets are pictured above. From left to right: Shelby, Bailey, Hazel, Daisy, and Yeti.",3 files,744.6 kB,This competition did not award ranking points,This competition did not count towards tiers,Playground Prediction Competition
Santander Customer Satisfaction,"$60,000 ",7 years ago,,"['tabular', 'binary classification', 'banking', 'auc']","5,115","5,688","93,333","From frontline support teams to C-suites, customer satisfaction is a key measure of success. Unhappy customers don't stick around. What's more, unhappy customers rarely voice their dissatisfaction before leaving.
Santander Bank is asking Kagglers to help them identify dissatisfied customers early in their relationship. Doing so would allow Santander to take proactive steps to improve a customer's happiness before it's too late.
In this competition, you'll work with hundreds of anonymized features to predict if a customer is satisfied or dissatisfied with their banking experience.",3 files,119.04 MB,This competition awarded ranking points,This competition counted towards tiers,Featured Prediction Competition
March Machine Learning Mania 2016,"$25,000 ",7 years ago,7 years ago,"['tabular', 'sports', 'basketball', 'logloss']",596,653,"1,045","Update: although the tournament is over, we're continuing our analysis under the predictions dataset page.
Back for its third year, March Machine Learning Mania challenges data scientists to predict winners and losers of the men's 2016 NCAA basketball tournament. You're provided data covering three decades of historical NCAA games and freely encouraged to use other sources of data to gain a winning edge.
In stage one of this two-stage competition, participants will build and test their models against the previous four tournaments. In the second stage, participants will predict the outcome of the 2016 tournament. You don’t need to participate in the first stage to enter the second. The first stage exists to incentivize model building and provide a means to score predictions. The real competition is forecasting the 2016 results.
Acknowledgments
SAP is the presenting sponsor of March Machine Learning Mania 2016. Please see About the Sponsor to read more.",1 files,7.24 MB,This competition awarded 0.5X ranking points,This competition did not count towards tiers,Featured Prediction Competition
BNP Paribas Cardif Claims Management,"$30,000 ",7 years ago,,"['tabular', 'binary classification', 'banking', 'logloss']","2,920","3,283","54,337","As a global specialist in personal insurance, BNP Paribas Cardif serves 90 million clients in 36 countries across Europe, Asia and Latin America.
In a world shaped by the emergence of new uses and lifestyles, everything is going faster and faster. When facing unexpected events, customers expect their insurer to support them as soon as possible. However, claims management may require different levels of check before a claim can be approved and a payment can be made. With the new practices and behaviors generated by the digital economy, this process needs adaptation thanks to data science to meet the new needs and expectations of customers.
In this challenge, BNP Paribas Cardif is providing an anonymized database with two categories of claims:
Kagglers are challenged to predict the category of a claim based on features available early in the process, helping BNP Paribas Cardif accelerate its claims process and therefore provide a better service to its customers.",3 files,103.75 MB,This competition awarded ranking points,This competition counted towards tiers,Featured Prediction Competition
Home Depot Product Search Relevance,"$40,000 ",7 years ago,,"['tabular', 'rmse']","2,123","2,551","35,492","Shoppers rely on Home Depot’s product authority to find and buy the latest products and to get timely solutions to their home improvement needs. From installing a new ceiling fan to remodeling an entire kitchen, with the click of a mouse or tap of the screen, customers expect the correct results to their queries – quickly. Speed, accuracy and delivering a frictionless customer experience are essential.
In this competition, Home Depot is asking Kagglers to help them improve their customers' shopping experience by developing a model that can accurately predict the relevance of search results.
Search relevancy is an implicit measure Home Depot uses to gauge how quickly they can get customers to the right products. Currently, human raters evaluate the impact of potential changes to their search algorithms, which is a slow and subjective process. By removing or minimizing human input in search relevance evaluation, Home Depot hopes to increase the number of iterations their team can perform on the current search algorithms.",6 files,72.93 MB,This competition awarded ranking points,This competition counted towards tiers,Featured Prediction Competition
Yelp Restaurant Photo Classification,,7 years ago,7 years ago,"['internet', 'image', 'food', 'fscoremicro']",355,355,"4,509","Does your favorite Ethiopian restaurant take reservations? Will a first date at that authentic looking bistro break your wallet? Is the diner down the street a good call for breakfast? Restaurant labels help Yelp users quickly answer questions like these, narrowing down their results to only restaurants that fit their nuanced needs.
In this competition, Yelp is challenging Kagglers to build a model that automatically tags restaurants with multiple labels using a dataset of user-submitted photos. Currently, restaurant labels are manually selected by Yelp users when they submit a review. Selecting the labels is optional, leaving some restaurants un- or only partially-categorized. 
In an age of food selfies and photo-centric social storytelling, it may be no surprise to hear that Yelp's users upload an enormous amount of photos every day alongside their written reviews. Can you turn their pictures into (less than a thousand) words?
Yelp isn’t only looking for your best model; we’re looking for data mining engineers that can help us use our data in novel ways while pushing code to production. The prize for this competition is a fast track through the recruiting process and an opportunity to show our data mining teams just what you’ve got! For more information about exciting opportunities at Yelp, check out the Jobs at Yelp competition page and Yelp's own careers page.",6 files,14.14 GB,This competition awarded ranking points,This competition counted towards tiers,Recruitment Prediction Competition
Second Annual Data Science Bowl,"$200,000 ",7 years ago,,"['image', 'healthcare', 'crps']",192,293,619,"We all have a heart. Although we often take it for granted, it's our heart that gives us the moments in life to imagine, create, and discover. Yet cardiovascular disease threatens to take away these moments. Each day, 1,500 people in the U.S. alone are diagnosed with heart failure—but together, we can help. We can use data science to transform how we diagnose heart disease. By putting data science to work in the cardiology field, we can empower doctors to help more people live longer lives and spend more time with those that they love.
Declining cardiac function is a key indicator of heart disease. Doctors determine cardiac function by measuring end-systolic and end-diastolic volumes (i.e., the size of one chamber of the heart at the beginning and middle of each heartbeat), which are then used to derive the ejection fraction (EF). EF is the percentage of blood ejected from the left ventricle with each heartbeat. Both the volumes and the ejection fraction are predictive of heart disease. While a number of technologies can measure volumes or EF, Magnetic Resonance Imaging (MRI) is considered the gold standard test to accurately assess the heart's squeezing ability.
The challenge with using MRI to measure cardiac volumes and derive ejection fraction, however, is that the process is manual and slow. A skilled cardiologist must analyze MRI scans to determine EF. The process can take up to 20 minutes to complete—time the cardiologist could be spending with his or her patients. Making this measurement process more efficient will enhance doctors' ability to diagnose heart conditions early, and carries broad implications for advancing the science of heart disease treatment.
The 2015 Data Science Bowl challenges you to create an algorithm to automatically measure end-systolic and end-diastolic volumes in cardiac MRIs. You will examine MRI images from more than 1,000 patients. This data set was compiled by the National Institutes of Health and Children's National Medical Center and is an order of magnitude larger than any cardiac MRI data set released previously. With it comes the opportunity for the data science community to take action to transform how we diagnose heart disease.
This is not an easy task, but together we can push the limits of what's possible. We can give people the opportunity to spend more time with the ones they love, for longer than ever before.
Acknowledgments
The Data Science Bowl is presented by:
The National Heart, Lung, and Blood Institute (NHLBI) provided the MRI images for this competition. Special thanks to NHLBI Intramural Investigators Dr. Michael Hansen and Dr. Andrew Arai.
Additional support for the Data Science Bowl was provided by NVIDIA:",441047 files,78.31 GB,This competition awarded ranking points,This competition counted towards tiers,Featured Prediction Competition
Cervical Cancer Screening,"$100,000 ",7 years ago,,['auc'],40,63,"1,160","This is a Masters competition. You must be a Kaggle Master to participate.
Cervical cancer is the third most common cancer in women worldwide, affecting over 500,000 women and resulting in approximately 275,000 deaths every year. After reading these statistics, you may be surprised to he
ar that cervical cancer is potentially preventable and curable.
Cervical cancer can be prevented through early administration of the HPV vaccine and regular pap smear screenings, which indicate the presence of precancerous cells. It is also sometimes curable by the removal of the early-stage cancerous tissue that is identified through pap smears. Screening and early treatment can lead to potential cures in about 95% of women at risk for cervical cancer.
Most women in the US have access to cervical cancer screening, yet 4,000 women die every year from cervical cancer in the US and it is estimated that 30% of US women do not receive regular pap screenings. We know little about who these women are and why they are not getting screened. Prior research suggests that lower screening rates are associated with low income, low education, lack of interaction with the healthcare system, and lack of health insurance. But research also shows that even in women with access to healthcare fail to get this preventive test, indicating that barriers like lack of education and not being comfortable with the procedure are influencing their behavior (Patient Survey). 
There are many patient advocacy programs on the importance of pap smears in cervical cancer prevention. However, these widespread programs may not be reaching or effectively speaking to the most vulnerable populations. If one could better identify these women, education campaigns could target them with content that speaks directly to their unique risk factors. Identifying predictors of not receiving pap smears will provide important information to stakeholders in cervical cancer prevention who run awareness programs.
With this Masters competition, Genentech is asking you to join their mission to help prevent cervical cancer. Given a dataset of de-identified health records, your challenge is to predict which women will not be screened for cervical cancer on the recommended schedule. Identifying at-risk populations will make education and other intervention efforts more effective, ideally ultimately reducing the number of women who die from this disease.
About Genentech
Founded more than 35 years ago, Genentech is a leading biotechnology company that discovers, develops, manufactures and commercializes medicines to treat patients with serious or life-threatening medical conditions. The company, a member of the Roche Group, has headquarters in South San Francisco, California. For additional information about the company, please visit http://www.gene.com.
Acknowledgements
The dataset for this competition is provided by Symphony Health Solutions. 
@Jotform.Show(35)",,,This competition did not award ranking points,This competition counted towards tiers,Featured Prediction Competition
The Allen AI Science Challenge,"$80,000 ",7 years ago,,"['tabular', 'text', 'multiclass classification', 'artificial intelligence', 'categorizationaccuracy']",170,302,679,"The Allen Institute for Artificial Intelligence (AI2) is working to improve humanity through fundamental advances in artificial intelligence. One critical but challenging problem in AI is to demonstrate the ability to consistently understand and correctly answer general questions about the world. 
The Aristo project at AI2 is focused on building such a system. One way Aristo ""learns"" is by extracting facts from various sources and processing them into a structured knowledge base. When taking an exam, questions are parsed and processed along with any accompanying diagrams to determine a strategy for answering. Aristo then uses entailment, statistical analysis, and inference methods to select a final answer.
While Aristo's abilities have improved significantly in the last two years, it still doesn't have perfect, reliable methods of gathering knowledge, understanding questions, or reasoning through answers.
Using a dataset of multiple choice question and answers from a standardized 8th grade science exam, AI2 is challenging you to create a model that gets to the head of the class.",1 files,52.61 kB,This competition awarded ranking points,This competition counted towards tiers,Featured Prediction Competition
Rossmann Store Sales,"$35,000 ",7 years ago,,"['tabular', 'time series analysis', 'rootmeansquarepercentageerror']","3,298","3,735","70,114","Rossmann operates over 3,000 drug stores in 7 European countries. Currently, Rossmann store managers are tasked with predicting their daily sales for up to six weeks in advance. Store sales are influenced by many factors, including promotions, competition, school and state holidays, seasonality, and locality. With thousands of individual managers predicting sales based on their unique circumstances, the accuracy of results can be quite varied.",4 files,39.85 MB,This competition awarded ranking points,This competition counted towards tiers,Featured Prediction Competition
How Much Did It Rain? II,$500 ,7 years ago,,"['tabular', 'regression', 'mae']",587,691,"8,105","After incorporating feedback from the Kaggle community, as well as scientific and educational partners, the Artificial Intelligence Committee of the American Meteorological Society is excited to be running a second iteration of the How Much Did It Rain? competition.
How Much Did It Rain? II is focused on solving the same core rain measurement prediction problem, but approaches it with a new and improved dataset and evaluation metric. This competition will go even further towards building a useful educational tool for universities, as well as making a meaningful contribution to continued meteorological research.
Competition Description
Rainfall is highly variable across space and time, making it notoriously tricky to measure. Rain gauges can be an effective measurement tool for a specific location, but it is impossible to have them everywhere. In order to have widespread coverage, data from weather radars is used to estimate rainfall nationwide. Unfortunately, these predictions never exactly match the measurements taken using rain gauges.
Recently, in an effort to improve their rainfall predictors, the U.S. National Weather Service upgraded their radar network to be polarimetric. These polarimetric radars are able to provide higher quality data than conventional Doppler radars because they transmit radio wave pulses with both horizontal and vertical orientations. 
Dual pulses make it easier to infer the size and type of precipitation because rain drops become flatter as they increase in size, whereas ice crystals tend to be elongated vertically.
In this competition, you are given snapshots of polarimetric radar values and asked to predict the hourly rain gauge total. A word of caution: many of the gauge values in the training dataset are implausible (gauges may get clogged, for example). More details are on the data page.
Acknowledgements
This competition is sponsored by the Artificial Intelligence Committee of the American Meteorological Society. Climate Corporation is providing the prize pool.",4 files,397.78 MB,This competition awarded ranking points,This competition counted towards tiers,Research Prediction Competition
What's Cooking?,,7 years ago,7 years ago,"['text', 'food', 'multiclass classification', 'categorizationaccuracy']","1,387","1,539","14,294","Picture yourself strolling through your local, open-air market... What do you see? What do you smell? What will you make for dinner tonight?
If you're in Northern California, you'll be walking past the inevitable bushels of leafy greens, spiked with dark purple kale and the bright pinks and yellows of chard. Across the world in South Korea, mounds of bright red kimchi greet you, while the smell of the sea draws your attention to squids squirming nearby. India’s market is perhaps the most colorful, awash in the rich hues and aromas of dozens of spices: turmeric, star anise, poppy seeds, and garam masala as far as the eye can see.
Some of our strongest geographic and cultural associations are tied to a region's local foods. This playground competitions asks you to predict the category of a dish's cuisine given a list of its ingredients. 
Acknowledgements
We want to thank Yummly for providing this unique dataset. Kaggle is hosting this playground competition for fun and practice.",3 files,2.31 MB,This competition did not award ranking points,This competition did not count towards tiers,Playground Prediction Competition
Right Whale Recognition,"$10,000 ",7 years ago,,"['image', 'animals', 'water bodies', 'multiclassloss']",364,470,"4,788","With fewer than 500 North Atlantic right whales left in the world's oceans, knowing the health and status of each whale is integral to the efforts of researchers working to protect the species from extinction.
Currently, only a handful of very experienced researchers can identify individual whales on sight while out on the water. For the majority of researchers, identifying individual whales takes time, making it difficult to effectively target whales for biological samples, acoustic recordings, and necessary health assessments.
To track and monitor the population, right whales are photographed during aerial surveys and then manually matched to an online photo-identification catalog. Customized software has been developed to aid in this process (DIGITS), but this still relies on a manual inspection of the potential comparisons, and there is a lag time for those images to be incorporated into the database. The current identification process is extremely time consuming and requires special training. This constrains marine biologists, who work under tight deadlines with limited budgets.
This competition challenges you to automate the right whale recognition process using a dataset of aerial photographs of individual whales. Automating the identification of right whales would allow researchers to better focus on their conservation efforts. Recognizing a whale in real-time would also give researchers on the water access to potentially life-saving historical health and entanglement records as they struggle to free a whale that has been accidentally caught up in fishing gear.
Acknowledgements
MathWorks is sponsoring the competition prize pool. If your team is participating in this competition MathWorks is also providing complimentary software. Click here for more details on how to request your copy.
Thanks to Christin Khan and Leah Crowe from NOAA for hand labeling the images to create this one of a kind dataset and to the right whale research team at New England Aquarium for maintaining the photo-identification catalog. Without their continued efforts, none of this would be possible. ",8 files,9.79 GB,This competition awarded ranking points,This competition counted towards tiers,Research Prediction Competition
Western Australia Rental Prices,"$100,000 ",7 years ago,,['rmsle'],59,80,"4,350","This is a Masters competition. You must be a Kaggle Master to participate.
Property rental prices are a key economic indicator, often signaling significant changes in things like unemployment rate or income. Accurately predicting rental prices would help organizations offering public and commercial services with the ability to better plan for and price these services.
Weekly rental values for properties vary due to a broad mix of factors. Some measures are objective, like proximity to hospitals, schools, transport, and coastline. Others are more subjective, like the aesthetic value of your backyard garden.
The rental market in Western Australia is unusually diverse and difficult to predict due to the region's varied landscape and small, widely spread population.
Currently, automated valuation models are used for over 90% of residential property estimates in Western Australia. Using data on location, property, zoning, past sales, and more, the goal of this competition is to improve on existing models by accurately estimating the weekly market rental value for residential properties across Western Australia.
@JotForm.Show(34)",,,This competition did not award ranking points,This competition counted towards tiers,Featured Prediction Competition
ECML/PKDD 15: Taxi Trip Time Prediction (II),$250 ,8 years ago,,"['tabular', 'rmsle']",345,418,"3,297","This is the second of two data science challenges that share the same dataset. The Taxi Service Trajectory competition predicts the final destination of taxi trips. 
To improve the efficiency of electronic taxi dispatching systems it is important to be able to predict how long a driver will have his taxi occupied. If a dispatcher knew approximately when a taxi driver would be ending their current ride, they would be better able to identify which driver to assign to each pickup request. 
In this challenge, we ask you to build a predictive framework that is able to infer the trip time of taxi rides in Porto, Portugal based on their (initial) partial trajectories. The output of such a framework must be the travel time of a particular taxi trip.
This competition is affiliated with the organization of ECML/PKDD 2015.",5 files,533.7 MB,This competition awarded 0.5X ranking points,This competition counted towards tiers,Research Prediction Competition
West Nile Virus Prediction,"$40,000 ",8 years ago,,"['tabular', 'binary classification', 'auc']","1,304","1,445","29,882","West Nile virus is most commonly spread to humans through infected mosquitos. Around 20% of people who become infected with the virus develop symptoms ranging from a persistent fever, to serious neurological illnesses that can result in death.
In 2002, the first human cases of West Nile virus were reported in Chicago. By 2004 the City of Chicago and the Chicago Department of Public Health (CDPH) had established a comprehensive surveillance and control program that is still in effect today.
Every week from late spring through the fall, mosquitos in traps across the city are tested for the virus. The results of these tests influence when and where the city will spray airborne pesticides to control adult mosquito populations.
Given weather, location, testing, and spraying data, this competition asks you to predict when and where different species of mosquitos will test positive for West Nile virus. A more accurate method of predicting outbreaks of West Nile virus in mosquitos will help the City of Chicago and CPHD more efficiently and effectively allocate resources towards preventing transmission of this potentially deadly virus. 
We've jump-started your analysis with some visualizations and starter code in R and Python on Kaggle Scripts. No data download or local environment setup needed!
Acknowledgements 
This competition is sponsored by the Robert Wood Johnson Foundation. Data is provided by the Chicago Department of Public Health.",9 files,15.84 MB,This competition awarded ranking points,This competition counted towards tiers,Featured Prediction Competition
ECML/PKDD 15: Taxi Trajectory Prediction (I),$250 ,8 years ago,,"['tabular', 'ahd@{type}']",381,459,"3,031","The taxi industry is evolving rapidly. New competitors and technologies are changing the way traditional taxi services do business. While this evolution has created new efficiencies, it has also created new problems. 
One major shift is the widespread adoption of electronic dispatch systems that have replaced the VHF-radio dispatch systems of times past. These mobile data terminals are installed in each vehicle and typically provide information on GPS localization and taximeter state. Electronic dispatch systems make it easy to see where a taxi has been, but not necessarily where it is going. In most cases, taxi drivers operating with an electronic dispatch system do not indicate the final destination of their current ride.
Another recent change is the switch from broadcast-based (one to many) radio messages for service dispatching to unicast-based (one to one) messages. With unicast-messages, the dispatcher needs to correctly identify which taxi they should dispatch to a pick up location. Since taxis using electronic dispatch systems do not usually enter their drop off location, it is extremely difficult for dispatchers to know which taxi to contact. 
To improve the efficiency of electronic taxi dispatching systems it is important to be able to predict the final destination of a taxi while it is in service. Particularly during periods of high demand, there is often a taxi whose current ride will end near or exactly at a requested pick up location from a new rider. If a dispatcher knew approximately where their taxi drivers would be ending their current rides, they would be able to identify which taxi to assign to each pickup request.
The spatial trajectory of an occupied taxi could provide some hints as to where it is going. Similarly, given the taxi id, it might be possible to predict its final destination based on the regularity of pre-hired services. In a significant number of taxi rides (approximately 25%), the taxi has been called through the taxi call-center, and the passenger’s telephone id can be used to narrow the destination prediction based on historical ride data connected to their telephone id.
In this challenge, we ask you to build a predictive framework that is able to infer the final destination of taxi rides in Porto, Portugal based on their (initial) partial trajectories. The output of such a framework must be the final trip's destination (WGS84 coordinates).
This is the first of two data science challenges that share the same dataset. The Taxi Service Trip Time competition predicts the total time of taxi rides.
This competition is affiliated with the organization of ECML/PKDD 2015.",5 files,533.7 MB,This competition awarded 0.5X ranking points,This competition counted towards tiers,Research Prediction Competition
15.071x - The Analytics Edge (Spring 2015),,8 years ago,8 years ago,['auc'],"2,920","2,920","41,624","IMPORTANT NOTE: This competition is only open to students of the MITx free, online course 15.071x - The Analytics Edge.
What makes online news articles popular?
Newspapers and online news aggregators like Google News need to understand which news articles will be the most popular, so that they can prioritize the order in which stories appear. In this competition, you will predict the popularity of a set of New York Times blog articles from the time period September 2014-December 2014.
The following screenshot shows an example of the New York Times technology blog ""Bits"" homepage:
Many blog articles are published each day, and the New York Times has to decide which articles should be featured. In this competition, we challenge you to develop an analytics model that will help the New York Times understand the features of a blog post that make it popular.
To download the data and learn how this competition works, please be sure to read the ""Data"" page, as well as the ""Evaluation"" page, which can both be found in the panel on the left.
Acknowledgements
This competition is brought to you by MITx and edX.",,,This competition did not award ranking points,This competition did not count towards tiers,Research Prediction Competition
Finding Elo,,8 years ago,8 years ago,"['tabular', 'board games', 'mae']",157,170,"1,873","Elite chess players are rated, ranked, analyzed, and compared in many ways. Classical methods of ranking chess players have focused on game histories, paying particular attention to the relative strength of the players involved. This includes the popular FIDE Elo score, which was the focus of one of Kaggle's first ever competitions - Elo vs. the Rest of the World.
Recent work on chess analysis has focused on intrinsic performance ratings, where one assesses skill based on the quality of decisions rather than the outcomes of games. For an example of this kind of approach, see this draft by Kenneth Regan. Two advantages of an intrinsic approach are an increased sample size (there are many more moves than games) and the ability to approach new challenges, such as determining whether a player is cheating by performing moves above their skill level.
This competition challenges Kagglers to determine players' FIDE Elo ratings at the time a game is played, based solely on the moves in one game. Do a player's moves reflect their absolute skill? Does the opponent matter? How closely does one game reflect intrinsic ability? How well can an algorithm do? Does computational horsepower increase accuracy? Let's find out!
You do not need to be a chess expert -- or even know how to play chess -- to attempt this competition. You do need patience and a computer that doesn't mind some heat. The dataset includes 50,000 games between elite, ranked players. As a getting-started computational bonus, Kaggle has run these games through a chess engine to score each move.
Good luck finding Elo!",4 files,21.8 MB,This competition did not award ranking points,This competition did not count towards tiers,Playground Prediction Competition
Tradeshift Text Classification,"$5,000 ",8 years ago,,['logloss'],374,395,"5,637","In the late 90's, Yann LeCun's team pioneered the successful application of machine learning to optical character recognition. 25 years later, machine learning continues to be an invaluable tool for text processing downstream from the OCR process.
Tradeshift has created a dataset with thousands of documents, representing millions of words. In each document, several bounding boxes containing text are selected. For each piece of text, many features are extracted and certain labels are assigned.
In this competition, participants are asked to create and open source an algorithm that correctly predicts the probability that a piece of text belongs to a given class.",4 files,883.31 MB,This competition awarded ranking points,This competition counted towards tiers,Featured Prediction Competition
Africa Soil Property Prediction Challenge,"$8,000 ",8 years ago,,['mcrmse'],"1,231","1,308","18,273","Advances in rapid, low cost analysis of soil samples using infrared spectroscopy, georeferencing of soil samples, and greater availability of earth remote sensing data provide new opportunities for predicting soil functional properties at unsampled locations. Soil functional properties are those properties related to a soil’s capacity to support essential ecosystem services such as primary productivity, nutrient and water retention, and resistance to soil erosion. Digital mapping of soil functional properties, especially in data sparse regions such as Africa, is important for planning sustainable agricultural intensification and natural resources management.
Diffuse reflectance infrared spectroscopy has shown potential in numerous studies to provide a highly repeatable, rapid and low cost measurement of many soil functional properties. The amount of light absorbed by a soil sample is measured, with minimal sample preparation, at hundreds of specific wavebands across a range of wavelengths to provide an infrared spectrum (Fig. 1). The measurement can be typically performed in about 30 seconds, in contrast to conventional reference tests, which are slow and expensive and use chemicals.
Conventional reference soil tests are calibrated to the infrared spectra on a subset of samples selected to span the diversity in soils in a given target geographical area. The calibration models are then used to predict the soil test values for the whole sample set. The predicted soil test values from georeferenced soil samples can in turn be calibrated to remote sensing covariates, which are recorded for every pixel at a fixed spatial resolution in an area, and the calibration model is then used to predict the soil test values for each pixel. The result is a digital map of the soil properties.
This competition asks you to predict 5 target soil functional properties from diffuse reflectance infrared spectroscopy measurements.
Acknowledgements
This competition is sponsored by the Africa Soil Information Service.",3 files,21.7 MB,This competition awarded ranking points,This competition counted towards tiers,Research Prediction Competition
American Epilepsy Society Seizure Prediction Challenge,"$25,000 ",8 years ago,,['auc'],504,653,"17,777","Seizure forecasting systems hold promise for improving the quality of life for patients with epilepsy.
Epilepsy afflicts nearly 1% of the world's population, and is characterized by the occurrence of spontaneous seizures. For many patients, anticonvulsant medications can be given at sufficiently high doses to prevent seizures, but patients frequently suffer side effects. For 20-40% of patients with epilepsy, medications are not effective -- and even after surgical removal of epilepsy-causing brain tissue, many patients continue to experience spontaneous seizures. Despite the fact that seizures occur infrequently, patients with epilepsy experience persistent anxiety due to the possibility of a seizure occurring.
Seizure forecasting systems have the potential to help patients with epilepsy lead more normal lives. In order for EEG-based seizure forecasting systems to work effectively, computational algorithms must reliably identify periods of increased probability of seizure occurrence. If these seizure-permissive brain states can be identified, devices designed to warn patients of impeding seizures would be possible. Patients could avoid potentially dangerous activities like driving or swimming, and medications could be administered only when needed to prevent impending seizures, reducing overall side effects.
There is emerging evidence that the temporal dynamics of brain activity can be classified into 4 states: Interictal (between seizures, or baseline), Preictal (prior to seizure), Ictal (seizure), and Post-ictal (after seizures). Seizure forecasting requires the ability to reliably identify a preictal state that can be differentiated from the interictal, ictal, and postictal state. The primary challenge in seizure forecasting is differentiating between the preictal and interictal states. The goal of the competition is to demonstrate the existence and accurate classification of the preictal brain state in dogs and humans with naturally occurring epilepsy.
The Competition
Intracranial EEG was recorded from dogs with naturally occurring epilepsy using an ambulatory monitoring system. EEG was sampled from 16 electrodes at 400 Hz, and recorded voltages were referenced to the group average. These are long duration recordings, spanning multiple months up to a year and recording up to a hundred seizures in some dogs.
In addition, datasets from patients with epilepsy undergoing intracranial EEG monitoring to identify a region of brain that can be resected to prevent future seizures are included in the contest. These datasets have varying numbers of electrodes and are sampled at 5000 Hz, with recorded voltages referenced to an electrode outside the brain. The challenge is to distinguish between ten minute long data clips covering an hour prior to a seizure, and ten minute iEEG clips of interictal activity. Seizures are known to cluster, or occur in groups. Patients who typically have seizure clusters receive little benefit from forecasting follow-on seizures. For this contest only lead seizures, defined here as seizures occurring four hours or more after another seizure, are included in the training and testing data sets. In order to avoid any potential contamination between interictal, preictal, and post-ictal EEG signals interictal segments in the canine training and test data were restricted to be at least one week before or after any seizure. In the human data, where the entire monitoring session may last less than one week, interictal data segments were restricted to be at least four hours before or after any seizure. Interictal data segments were chosen at random within these restrictions for both canine and human subjects.
Participants are invited to visit the NIH-sponsored International Epilepsy Electrophysiology portal (http://ieeg.org) to review and download annotated interictal and preictal data from other patients and animal subjects. Using ieeg.org data for additional algorithm training is permitted.
Acknowledgements
This competition is sponsored by the National Institutes of Health (NINDS), the Epilepsy Foundation, and the American Epilepsy Society.
  References
Howbert JJ, Patterson EE, Stead SM, Brinkmann B, Vasoli V, Crepeau D, Vite CH, Sturges B, Ruedebusch V, Mavoori J, Leyde K, Sheffield WD, Litt B, Worrell GA (2014) Forecasting seizures in dogs with naturally occurring epilepsy. PLoS One 9(1):e81920.
Cook MJ, O'Brien TJ, Berkovic SF, Murphy M, Morokoff A, Fabinyi G, D'Souza W, Yerra R, Archer J, Litewka L, Hosking S, Lightfoot P, Ruedebusch V, Sheffield WD, Snyder D, Leyde K, Himes D (2013) Prediction of seizure likelihood with a long-term, implanted seizure advisory system in patients with drug-resistant epilepsy: a first-in-man study. LANCET NEUROL 12:563-571.
Park Y, Luo L, Parhi KK, Netoff T (2011) Seizure prediction with spectral power of EEG using cost-sensitive support vector machines. Epilepsia 52:1761-1770.
Davis KA, Sturges BK, Vite CH, Ruedebusch V, Worrell G, Gardner AB, Leyde K, Sheffield WD, Litt B (2011) A novel implanted device to wirelessly record and analyze continuous intracranial canine EEG. Epilepsy Res 96:116-122.
Andrzejak RG, Chicharro D, Elger CE, Mormann F (2009) Seizure prediction: Any better than chance? Clin Neurophysiol.
Snyder DE, Echauz J, Grimes DB, Litt B (2008) The statistics of a practical seizure warning system. J Neural Eng 5: 392–401.
Mormann F, Andrzejak RG, Elger CE, Lehnertz K (2007) Seizure prediction: the long and winding road. Brain 130: 314–333.
Haut S, Shinnar S, Moshe SL, O'Dell C, Legatt AD. (1999) The association between seizure clustering and status epilepticus in patients with intractable complex partial seizures. Epilepsia 40:1832–1834.",8003 files,113.62 GB,This competition awarded ranking points,This competition counted towards tiers,Research Prediction Competition
First Steps With Julia,,,,"['image', 'categorizationaccuracy']",56,64,252,"This competition is designed to help you get started with Julia. If you are looking for a good programming language for data science, or if you are already accustomed to one language, we encourage you to also try Julia. Julia is a relatively new language for technical computing that attempts to combine the strengths of other popular programming languages. 
Here we introduce two tutorials to highlight some of Julia's features. The first is focused on the basics of the language. In the second, a complete implementation of the K Nearest Neighbor algorithm is presented, highlighting features such as parallelization and speed. 
Both tutorials show that it is easy to write code in Julia, due to its intuitive syntax and design. The tutorials also describe some basics of image processing and some concepts of machine learning such as cross validation. After reviewing them, we hope you will be motivated to write your own machine learning algorithms in Julia.
This tutorial focuses on the task of identifying characters from Google Street View images. It differs from traditional character recognition because the data set contains different character fonts and the background is not the same for all images. 
Acknowledgements
The data was taken from the Chars74K dataset, which consists of images of characters selected from Google Street View images. We ask that you cite the following reference in any publication resulting from your work:
T. E. de Campos, B. R. Babu and M. Varma, Character recognition in natural images, Proceedings of the International Conference on Computer Vision Theory and Applications (VISAPP), Lisbon, Portugal, February 2009.
This tutorial was developed by Luis Tandalla during his summer 2014 internship at Kaggle.",8 files,150.23 MB,This competition did not award ranking points,This competition did not count towards tiers,Getting Started Prediction Competition
Liberty Mutual Group - Fire Peril Loss Cost,"$25,000 ",8 years ago,,['normalizedweightedgini'],632,716,"14,758","A Fortune 100 company, Liberty Mutual Insurance has provided a wide range of insurance products and services designed to meet our customers' ever-changing needs for over 100 years.
Within the business insurance industry, fire losses account for a significant portion of total property losses. High severity and low frequency, fire losses are inherently volatile, which makes modeling them difficult. In this challenge, your task is to predict the target, a transformed ratio of loss to total insured value, using the provided information. This will enable more accurate identification of each policyholder’s risk exposure and the ability to tailor the insurance coverage for their specific operation.
Because we seek to tap innovation both inside and outside the company, certain eligible Liberty Mutual employees are encouraged to participate in this challenge for development purposes. Refer to the competition rules for the full details.",3 files,1.16 GB,This competition awarded ranking points,This competition counted towards tiers,Featured Prediction Competition
Display Advertising Challenge,"$16,000 ",8 years ago,,['logloss'],717,831,"8,635","Display advertising is a billion dollar effort and one of the central uses of machine learning on the Internet. However, its data and methods are usually kept under lock and key. In this research competition, CriteoLabs is sharing a week’s worth of data for you to develop models predicting ad click-through rate (CTR). Given a user and the page he is visiting, what is the probability that he will click on a given ad?
The goal of this challenge is to benchmark the most accurate ML algorithms for CTR estimation. All winning models will be released under an open source license. As a participant, you are given a chance to access the traffic logs from Criteo that include various undisclosed features along with the click labels. ",1 files,50.5 MB,This competition awarded ranking points,This competition counted towards tiers,Research Prediction Competition
The Hunt for Prohibited Content,"$25,000 ",8 years ago,,['ap@{k}'],284,316,"4,991","Avito.ru is the largest general classified website in Russia that helps connect buyers with sellers across all Russian territories. There are more than 22 million active ads on Avito and each day a huge number of ads are added or modified. The efficiency of Avito depends heavily on the content quality -- when buyers can quickly find relevant content, sellers can sell their items in hours.
The larger and more popular Avito becomes the more attractive it becomes to sell illicit items or services. Some items that people try to sell are completely illegal while others might seem allowable but are still prohibited by our rules. This is why all new or modified ads are thoroughly moderated by our team of human moderators. The moderators can remove the ad if it conflicts with the Russian legislation or with the internal rules of AVITO.ru. However, with our growth it becomes more and more challenging to thoroughly moderate all ads. This is where machine learning comes into play.
The objective of this challenge is to create a predictive model that will learn from moderators' answers how to classify if an ad contains illicit content or not.",4 files,1.04 GB,This competition awarded ranking points,This competition counted towards tiers,Featured Prediction Competition
MLSP 2014 Schizophrenia Classification Challenge,,9 years ago,,['auc'],313,348,"2,242","Schizophrenia is a severe and disabling mental illnesses which has no well-established, non-invasive diagnosis biomarker. Currently, due to its symptom overlap with other mental illnesses (like bipolar disorder) it can only be diagnosed subjectively, by process of elimination.
This competition invites you to automatically diagnose subjects with schizophrenia based on multimodal features derived from their brain magnetic resonance imaging (MRI) scans.
The features made available in this competition are a result from current state-of-the art developments in neuroimaging and MRI data processing. Two modalities of MRI scans are used to obtain these features: functional and structural MRI. One challenge in this competition is how to optimally combine this type of multimodal information and select features that enhance diagnosis. Optional additional information is provided that could be helpful with this particular aspect of the task.
This is an official competition of the IEEE International Workshop on Machine Learning for Signal Processing (MLSP 2014)
Acknowledgements
Collection of this dataset was made at the Mind Research Network under an NIH NIGMS Centers of Biomedical Research Excellence (COBRE) grant 5P20RR021938/P20GM103472 to Vince Calhoun (PI).",5 files,217.12 MB,This competition awarded ranking points,This competition counted towards tiers,Research Prediction Competition
Greek Media Monitoring Multilabel Classification (WISE 2014),$680 ,9 years ago,,['fscoremicro'],120,127,"1,207","In the past, gathering information was paramount only for top-tier companies. In the information age, mining and categorization of relevant information is necessary for all companies. Media monitoring - the activity of monitoring the output of the print, online and broadcast media - allows every company to search a wide range of media, from printed media to internet publications, and be informed on their area of expertise and remain competitive.
This is a multi-label classification competition for articles coming from Greek printed media. Raw data comes from the scanning of print media, article segmentation, and optical character segmentation, and therefore is quite noisy. Each article is examined by a human annotator and categorized to one or more of the topics being monitored. Topics range from specific persons, products, and companies that can be easily categorized based on keywords, to more general semantic concepts, such as environment or economy. Building multi-label classifiers for the automated annotation of articles into topics can support the work of human annotators by suggesting a list of all topics by order of relevance, or even automate the annotation process for media and/or categories that are easier to predict. This saves valuable time and allows a media monitoring company to expand the portfolio of media being monitored.  
Organizers
The competition is organized by media monitoring solutions company DataScouting, media monitoring services company ENIMEROSI and the Deparment of Informatics of the Aristotle University of Thessaloniki. It is the challenge accompanying the 15th International Conference on Web Information System Engineering (WISE 2014) that will be held in Thessaloniki, Greece on 12-14 October 2014.
   ARISTOTLE
   UNIVERSITY OF
   THESSALONIKI ",5 files,335.1 MB,This competition awarded ranking points,This competition counted towards tiers,Research Prediction Competition
Random Acts of Pizza,,9 years ago,8 years ago,"['internet', 'binary classification', 'text', 'auc']",462,503,"3,570","Get started on this competition through Kaggle Scripts
In machine learning, it is often said there are no free lunches. How wrong we were.
This competition contains a dataset with 5671 textual requests for pizza from the Reddit community Random Acts of Pizza together with their outcome (successful/unsuccessful) and meta-data. Participants must create an algorithm capable of predicting which requests will garner a cheesy (but sincere!) act of kindness.
""I'll write a poem, sing a song, do a dance, play an instrument, whatever! I just want a pizza,"" says one hopeful poster. What about making an algorithm?
Kaggle is hosting this competition for the machine learning community to use for fun and practice. This data was collected and graciously shared by Althoff et al. (Buy them a pizza -- data collection is a thankless and tedious job!) We encourage participants to explore their accompanying paper and ask that you cite the following reference in any publications that result from your work:
Tim Althoff, Cristian Danescu-Niculescu-Mizil, Dan Jurafsky. How to Ask for a Favor: A Case Study on the Success of Altruistic Requests, Proceedings of ICWSM, 2014.",5 files,17.97 MB,This competition did not award ranking points,This competition did not count towards tiers,Playground Prediction Competition
Bike Sharing Demand,,9 years ago,8 years ago,"['tabular', 'time series analysis', 'cycling', 'rmsle']","3,242","3,559","32,809","Get started on this competition through Kaggle Scripts
Bike sharing systems are a means of renting bicycles where the process of obtaining membership, rental, and bike return is automated via a network of kiosk locations throughout a city. Using these systems, people are able rent a bike from a one location and return it to a different place on an as-needed basis. Currently, there are over 500 bike-sharing programs around the world.
The data generated by these systems makes them attractive for researchers because the duration of travel, departure location, arrival location, and time elapsed is explicitly recorded. Bike sharing systems therefore function as a sensor network, which can be used for studying mobility in a city. In this competition, participants are asked to combine historical usage patterns with weather data in order to forecast bike rental demand in the Capital Bikeshare program in Washington, D.C.
Acknowledgements
Kaggle is hosting this competition for the machine learning community to use for fun and practice. This dataset was provided by Hadi Fanaee Tork using data from Capital Bikeshare. We also thank the UCI machine learning repository for hosting the dataset. If you use the problem in publication, please cite:
Fanaee-T, Hadi, and Gama, Joao, Event labeling combining ensemble detectors and background knowledge, Progress in Artificial Intelligence (2013): pp. 1-15, Springer Berlin Heidelberg.",3 files,1.12 MB,This competition did not award ranking points,This competition did not count towards tiers,Playground Prediction Competition
UPenn and Mayo Clinic's Seizure Detection Challenge,"$8,000 ",9 years ago,,['mcauc'],200,241,"4,503","For individuals with drug-resistant epilepsy, responsive neurostimulation systems hold promise for augmenting current therapies and transforming epilepsy care.
Of the more than two million Americans who suffer from recurrent, spontaneous epileptic seizures, 500,000 continue to experience seizures despite multiple attempts to control the seizures with medication. For these patients responsive neurostimulation represents a possible therapy capable of aborting seizures before they affect a patient's normal activities. 
In order for a responsive neurostimulation device to successfully stop seizures, a seizure must be detected and electrical stimulation applied as early as possible. A seizure that builds and generalizes beyond its area of origin will be very difficult to abort via neurostimulation. Current seizure detection algorithms in commercial responsive neurostimulati
In addition, physicians and researchers working in epilepsy must often review large quantities of continuous EEG data to identify seizures, which in some patients may be quite subtle. Automated algorithms to detect seizures in large EEG datasets with low false positive and false negative rates would greatly assist clinical care and basic research.
The Competition:
Intracranial EEG was recorded from dogs with naturally occurring epilepsy using an ambulatory monitoring system. EEG was sampled from 16 electrodes at 400 Hz, and recorded voltages were referenced to the group average. 
In addition, datasets from patients with epilepsy undergoing intracranial EEG monitoring to identify a region of brain that can be resected to prevent future seizures are included in the contest. These datasets have varying numbers of electrodes and are sampled at 500 Hz or 5000 Hz, with recorded voltages referenced to an electrode outside the brain.
Acknowledgements
This competition is sponsored by the National Institues of Health (NINDS), and the American Epilepsy Society.",3 files,11.62 GB,This competition awarded ranking points,This competition counted towards tiers,Research Prediction Competition
Forest Cover Type Prediction,,9 years ago,8 years ago,"['multiclass classification', 'forestry', 'categorizationaccuracy']","1,692","1,860","15,404","Get started on this competition with Kaggle Scripts. No data download or local environment needed!
Random forests? Cover trees? Not so fast, computer nerds. We're talking about the real thing.
In this competition you are asked to predict the forest cover type (the predominant kind of tree cover) from strictly cartographic variables (as opposed to remotely sensed data). The actual forest cover type for a given 30 x 30 meter cell was determined from US Forest Service (USFS) Region 2 Resource Information System data. Independent variables were then derived from data obtained from the US Geological Survey and USFS. The data is in raw form (not scaled) and contains binary columns of data for qualitative independent variables such as wilderness areas and soil type.
This study area includes four wilderness areas located in the Roosevelt National Forest of northern Colorado. These areas represent forests with minimal human-caused disturbances, so that existing forest cover types are more a result of ecological processes rather than forest management practices.
Acknowledgements
Kaggle is hosting this competition for the machine learning community to use for fun and practice. This dataset was provided by Jock A. Blackard and Colorado State University. We also thank the UCI machine learning repository for hosting the dataset. If you use the problem in publication, please cite:
Bache, K. & Lichman, M. (2013). UCI Machine Learning Repository. Irvine, CA: University of California, School of Information and Computer Science",7 files,96.64 MB,This competition did not award ranking points,This competition did not count towards tiers,Playground Prediction Competition
KDD Cup 2014 - Predicting Excitement at DonorsChoose.org,"$2,000 ",9 years ago,,['auc'],472,645,"12,521","DonorsChoose.org is an online charity that makes it easy to help students in need through school donations. At any time, thousands of teachers in K-12 schools propose projects requesting materials to enhance the education of their students. When a project reaches its funding goal, they ship the materials to the school.
The 2014 KDD Cup asks participants to help DonorsChoose.org identify projects that are exceptionally exciting to the business, at the time of posting. While all projects on the site fulfill some kind of need, certain projects have a quality above and beyond what is typical. By identifying and recommending such projects early, they will improve funding outcomes, better the user experience, and help more students receive the materials they need to learn.
Successful predictions may require a broad range of analytical skills, from natural language processing on the need statements to data mining and classical supervised learning on the descriptive factors around each project.
About KDD
KDD 2014 is a premier interdisciplinary conference that brings together researchers and practitioners from all aspects of data science, data mining, knowledge discovery, large-scale data analytics, and big data. This year's KDD features 4 keynotes, 151 Research Track papers, 44 Industry & Government Track papers, 24 workshops, 12 tutorials, and more.
Acknowledgements
Data and logistical support has been graciously provided by DonorsChoose.org. DonorsChoose.org is an online charity and 501(c)(3) nonprofit organization that makes it easy for anyone to help students in need. Public school teachers from every corner of America post classroom project requests, and donors can give any amount to the project that most inspires them.",6 files,970.86 MB,This competition awarded ranking points,This competition counted towards tiers,Research Prediction Competition
Higgs Boson Machine Learning Challenge,"$13,000 ",9 years ago,,['custom metric'],"1,784","1,942","35,769","The evaluation metric is the approximate median significance (AMS):
AMS=
√
2((s+b+br)log(1+
s
b+br
)−s)
where
s, b : unnormalized true positive and false positive rates, respectively,
b_r =10 is the constant regularization term,
\\(\log\\) is the natural log.
More precisely, let \\((y_1, \ldots, y_n) \in \{\text{b},\text{s}\}^n\\) be the vector of true test labels, let \\((\hat{y}_1, \ldots, \hat{y}_n) \in \{\text{b},\text{s}\}^n\\) be the vector of predicted (submitted) test labels, and let \\((w_1, \ldots, w_n) \in {\mathbb{R}^+}^n\\) be the vector of weights. Then
s=
n
∑
i=1
wi1{yi=s}1{
ˆ
y
i=s}
and
b=
n
∑
i=1
wi1{yi=b}1{
ˆ
y
i=s},
where the indicator function \\(\mathbb{1}\{A\}\\) is 1 if its argument \\(A\\) is true and 0 otherwise.
For more information on the statistical model and the derivation of the metric, see the technical documentation. We have provided python code for the metric is available from the Data page and a Python starting kit.
Submission Instructions
The submission file format is 
EventId,RankOrder,Class
1,2,b
2,541234,s
3,5,b
4,1,b
5,542456,s
...
Your submission file should have a header row and three columns
EventId is a unique identifier for each event. The list of EventIds must correspond to the exact list of EventIds in test.csv, but the ordering can be arbitrary.
RankOrder is a permutation of the integer list [1,550000] . The higher the rank, the more signal-like is the event. Most predictors output a real-valued score for each event in the test set, in which case RankOrder is just the ordering of the test points according to the score. The RankOrder is not used for computing the AMS, but it allows the organizers to compute other metrics (e.g., ROC) related to the classification task, which is not captured entirely by the classification alone.
Class is either ""b"" or ""s"", and it indicates if your prediction (yi above in the formal definition) for the event is background or signal. The AMS will be calculated based on the (hidden) weights of events that you mark ""s""",4 files,56.9 MB,This competition awarded ranking points,This competition counted towards tiers,Featured Prediction Competition
Billion Word Imputation,,9 years ago,8 years ago,"['text', 'linguistics', 'levenshteinmean']",87,125,629,"This competition uses the billion-word benchmark corpus provided by Chelba et al. for language modeling. Rather than ask participants to create a classic language model and evaluate sentence probabilities -- a task which is difficult to faithfully score in Kaggle's supervised ML setting -- we have introduced a variation on the language modeling task.
For each sentence in the test set, we have removed exactly one word. Participants must create a model capable of inserting back the correct missing word at the correct location in the sentence. Submissions are scored using an edit distance to allow for partial credit.
We extend our thanks to authors who created this corpus and shared it for the research community to use. Please cite this paper if you use this dataset in your research: Ciprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge, Thorsten Brants, Phillipp Koehn: One Billion Word Benchmark for Measuring Progress in Statistical Language Modeling, CoRR, 2013.
Note: the train/test split used in this competition is different than the published version used for language modeling. If you are creating full language models and scoring perplexity, you should download the official version of the corpus from the authors' website.",2 files,1.7 GB,This competition did not award ranking points,This competition did not count towards tiers,Playground Prediction Competition
Learning Social Circles in Networks,,9 years ago,,['custom metric'],202,230,"1,751","Social Circles help users organize their personal social networks.  These are implemented as ""circles"" on Google+, and as ""lists"" on Facebook and Twitter. Each circle consists of a subset of a particular user's friends. Such circles may be disjoint, overlap, or be hierarchically nested.
The goal of this competition is to automatically infer users' social circles. You are provided a set of users, each of whose circles must be inferred. To do this, participants have access to:
A list of the user's friends
Anonymized Facebook profiles of each of those friends
A network of connections between those friends (their ""ego network"")
To give you an idea of how to use this data, the problem of detecting social circles has been discussed (in an academic setting) in http://i.stanford.edu/~julian/pdfs/nips2012.pdf
Those of you who've been around Kaggle for a while may remember that we called upon you to create this data set. We extend our thanks to those of you who helped out. As a reward, you might be able to find yourself in the data and gain a one-row advantage?!",8 files,2.57 MB,This competition awarded ranking points,This competition counted towards tiers,Playground Prediction Competition
DecMeg2014 - Decoding the Human Brain,"$5,000 ",9 years ago,,['categorizationaccuracy'],267,301,"4,505","Understanding how the human brain works is a primary goal in neuroscience research. Non-invasive functional neuroimaging techniques, such as magnetoencephalography (MEG), are able to capture the brain activity as multiple timeseries. When a subject is presented a stimulus and the concurrent brain activity is recorded, the relation between the pattern of recorded signal and the category of the stimulus may provide insights on the underlying mental process. Among the approaches to analyse the relation between brain activity and stimuli, the one based on predicting the stimulus from the concurrent brain recording is called brain decoding.
The goal of this competition is to predict the category of a visual stimulus presented to a subject from the concurrent brain activity. The brain activity is captured with an MEG device which records 306 timeseries at 1KHz of the magnetic field associated with the brain currents. The categories of the visual stimulus for this competition are two: face and scrambled face. A stimulus and the concurrent MEG recording is called trial and thousands of randomized trials were recorded from multiple subjects. The trials of some of the subjects, i.e. the train set, are provided to create prediction models. The remaining trials, i.e. the test set, belong to different subjects and they will be used to score the prediction models. Because of the variability across subjects in brain anatomy and in the patterns of brain activity, a certain degree of difference is expected between the data of different subjects and thus between the train set and the test set.
Bibliography
Full details of the neuroscientific experiment in which the data were collected are described in:
Front. Hum. Neurosci.
 http://www.frontiersin.org/Journal/10.3389/fnhum.2011.00076/abstract
A brief survey of the scientific literature on the problem of decoding across subjects, together with the description of the train set of this competition and a preliminary solution in terms of transfer learning, are described in:
Pattern Recognition in Neuroimaging, 2014 International Workshop on
ieeexplore.ieee.orghttp://arxiv.org/abs/1404.4175
Conference
This competition is associated with the the 19th International Conference on Biomagnetism, Biomag 2014. The Biomag conference will be held in Halifax, Canada, August 24-28, 2014.
Organization
This competition is organized by Emanuele Olivetti, Mostafa Kia and Paolo Avesani (NeuroInformatics Lab, Fondazione Bruno Kessler and Università di Trento, IT).
Acknowledgements
The awards of this competition are funded by Elekta Oy, MEG International Services Ltd (MISL), Fondazione Bruno Kessler, and Besa. We would also like to thank Daniel Wakeman (Martinos Center, MGH, USA), Richard Henson (MRC/CBU, Cambridge, UK), Ole Jensen (Donders Institute, NL), Nathan Weisz (University of Trento, IT) and Alexandre Gramfort (Telecom ParisTech, CNRS, CEA / Neurospin) for their contributions in preparing this competition.
                   ",5 files,5.84 GB,This competition awarded ranking points,This competition counted towards tiers,Research Prediction Competition
The Analytics Edge (15.071x),,9 years ago,9 years ago,['auc'],"1,684","1,684","19,634","(Please note: this competition is only open to students of https://www.edx.org/course/mitx/mitx-15-071x-analytics-edge-1416)
What predicts happiness? In this competition, you'll be using data from Show of Hands, an informal polling platform for use on mobile devices and the web, to see what aspects and characteristics of people's lives predict happiness.
Show of Hands has been downloaded over 300,000 times across Apple and Android app stores, and users have cast more than 75 million votes. In this problem, we'll use data from thousands of users and one hundred different questions to see which responses predict happiness.
Acknowledgements
This competition is brought to you by 15.071x, edX, and Show of Hands.",,,This competition did not award ranking points,This competition did not count towards tiers,Playground Prediction Competition
Acquire Valued Shoppers Challenge,"$30,000 ",9 years ago,,['auc'],949,"1,162","25,191","Consumer brands often offer discounts to attract new shoppers to buy their products. The most valuable customers are those who return after this initial incented purchase.  With enough purchase history, it is possible to predict which shoppers, when presented an offer, will buy a new item. However, identifying the shopper who will become a loyal buyer -- prior to the initial purchase -- is a more challenging task.
The Acquire Valued Shoppers Challenge asks participants to predict which shoppers are most likely to repeat purchase. To aid with algorithmic development, we have provided complete, basket-level, pre-offer shopping history for a large set of shoppers who were targeted for an acquisition campaign. The incentive offered to that shopper and their post-incentive behavior is also provided.
This challenge provides almost 350 million rows of completely anonymised transactional data from over 300,000 shoppers. It is one of the largest problems run on Kaggle to date.",5 files,3.07 GB,This competition awarded ranking points,This competition counted towards tiers,Featured Prediction Competition
Risky Business,"$100,000 ",9 years ago,9 years ago,['custom metric'],44,52,984,"Improve credit risk models by predicting the probability of default on a consumer credit product in the next 18 months. More accurate credit risk evaluations allow issuers of credit to be able to responsibly extend and manage credit lines for their The goal of this contest is to make the most accura
Enter Now!
This competition is only open to Masters-level participants who meet the eligibility criteria. Visit the Enter the Competition page to view the eligibility criteria and request entrance.",,,This competition awarded ranking points,This competition counted towards tiers,Masters Prediction Competition
Personalized Web Search Challenge,"$9,000 ",9 years ago,9 years ago,['ndcg@{k}'],194,261,"3,570","The Personalized Web Search Challenge provides a unique opportunity to consolidate and scrutinize the work from industrial labs on personalizing web search using user-logged search behavior context. It provides a fully anonymized dataset shared by Yandex, which has anonymized user ids, queries, query terms, urls, url domains and clicks.
This Challenge and the shared dataset will enable a whole new set of researchers to study the problem of personalizing web search experience. The Personalized Web Search Challenge is a part of series of contests organized by Yandex over many years. This year’s event is the eighth since 2004. In previous years, participants tried to learn to rank documents, predict traffic jams, find similar images, predict relevance of documents using search logs and detect search engine switchings in search sessions.
The Challenge is intended as a logical follow-up to the previous two challenges. We ask participants to re-rank URLs of each SERP returned by the search engine according to the personal preferences of the users. In other words, participants need to personalize search using the long-term (user history based) and short-term (session-based) user context. The evaluation relies on a variant of a dwell-time based model of personal relevance and is data-driven, as it is presently accepted in the state-of the-art research on personalized search.
The Challenge is a part of the Web Search Click Data workshop (WSCD 2014) and the reports of the best teams are welcome to be presented at this workshop, to be held at WSDM 2014 conference, on 28th February in New York, USA. The workshop is organized by Pavel Serdyukov (Yandex), Georges Dupret (Yahoo!) and Nick Craswell (Microsoft Research/Bing). ",7 files,5.86 GB,This competition awarded ranking points,This competition counted towards tiers,Featured Prediction Competition
See Click Predict Fix,"$4,000 ",9 years ago,9 years ago,['rmsle'],530,607,"5,543","This competition is the successor to the See Click Predict Fix Hackathon. The purpose of both competitions is to quantify and predict how people will react to a specific 311 issue. What makes an issue urgent? What do citizens really care about? How much does location matter? Being able to predict the most pressing 311 topics will allow governments to focus their efforts on fixing the most important problems. The data set for the competitions contains several hundred thousand 311 issues from four cities.
For those who are more interested in using the data for visualization or ""non-predictive"" data mining, we have added a $500 visualization prize. You may submit as many entries as you wish via the Visualization page. If you're plotting issues on maps, displaying the text in some meaningful way, or making any other creative use of the data, save it and post it!
About 311
311 is a mechanism by which citizens can express their desire to solve a problem the city or government by submitting a description of what needs to be done, fixed, or changed. In effect, this provides a high degree of transparency between government and its constituents. Once an issue has been established, citizens can vote and make comments on the issue so that government officials have some degree of awareness about what is the most important issue to address.
Sponsors
The meeting space has been provided by Microsoft.  Prize money is graciously offered by our sponsors:
On the citizen side, SeeClickFix leverages crowdsourcing to help both maintain the flow of incoming requests but show the public how effective you can be. When anyone in the community can report or comment on any issue, the entire group has a better perspective on what's happening--and how to fix it effectively.
For governments, SeeClickFix acts as a completely-customizable CRM that plugs into your existing request management tools. From types of service requests to managing different watch areas, SeeClickFix helps better m
A public policy entrepreneur and open innovation expert David advises numerous governments on open government and open data and works with leading non-profits and businesses on strategy, open innovation and community management. In addition to his work, David is an affiliate with the Berkman Centre for Internet and Society at Harvard where he is looking at issues surrounding the politics of data.
You can find David's writing on open innovation, public policy, public sector renewal and open source systems at his blog, or at TechPresident. In addition to his writing, David is frequently invited to speak on open government, policy making, negotiation and strategy to executives, policymakers, and students.
You can read a background on how this challenge came to be here.",3 files,55.75 MB,This competition awarded ranking points,This competition counted towards tiers,Featured Prediction Competition
See Click Predict Fix - Hackathon,"$1,000 ",9 years ago,9 years ago,['rmsle'],80,107,"1,051",">> View the San Francisco venue live <<
Competition ends:
Save the date! You're invited. We are organizing not one but two exciting competitions: a 24-hour hackathon followed by a deeper, two-month dive. The purpose of both competitions is to quantify and predict how people will react to a specific 311 issue. What makes an issue urgent? What do citizens really care about? How much does location matter? Being able to predict the most pressing 311 topics will allow governments to focus their efforts on fixing the most important problems. The data set for both competitions contains several hundred thousand 311 issues from four cities.
For those in the Bay Area: on the evening of September 27 to the evening of September 28, we will convene at Microsoft San Francisco (835 Market St Ste 700, San Francisco, CA), in the heart of the shopping district.  At that place and time, we will release the password to the competition data.  Please register on the meetup page if you plan to attend. Come see, predict, and fix by working with actual city data on real city problems.
Those outside the Bay Area can still participate in the Hackathon, and also in the ensuing longer competition.
For those who are more interested in using the data for visualization or ""non-predictive"" data mining, we have added a $500 visualization prize to both the hackathon and the longer competition.  When the competition opens, you may submit as many entries as you wish via the Visualization page.  If you're plotting issues on maps, displaying the text in some meaningful way, or making any other creative use of the data, save it and post it!
Note: the data will be uploaded prior to the hackathon   The data will be encrypted and the key released to participants at the start of the hackathon. Because this is a live event and delays are possible, the start time is approximate. Please be sure you have a program capable of handling encrypted .7z files.
About 311
311 is a mechanism by which citizens can express their desire to solve a problem the city or government by submitting a description of what needs to be done, fixed, or changed. In effect, this provides a high degree of transparency between government and its constituents. Once an issue has been established, citizens can vote and make comments on the issue so that government officials have some degree of awareness about what is the most important issue to address.
Sponsors
The meeting space has been provided by Microsoft.  Prize money is graciously offered by our sponsors:
On the citizen side, SeeClickFix leverages crowdsourcing to help both maintain the flow of incoming requests but show the public how effective you can be. When anyone in the community can report or comment on any issue, the entire group has a better perspective on what's happening--and how to fix it effectively.
For governments, SeeClickFix acts as a completely-customizable CRM that plugs into your existing request management tools. From types of service requests to managing different watch areas, SeeClickFix helps better m
A public policy entrepreneur and open innovation expert David advises numerous governments on open government and open data and works with leading non-profits and businesses on strategy, open innovation and community management. In addition to his work, David is an affiliate with the Berkman Centre for Internet and Society at Harvard where he is looking at issues surrounding the politics of data.
You can find David's writing on open innovation, public policy, public sector renewal and open source systems at his blog, or at TechPresident. In addition to his writing, David is frequently invited to speak on open government, policy making, negotiation and strategy to executives, policymakers, and students.
You can read a background on how this challenge came to be here.",2 files,13.02 MB,This competition awarded 0.25X ranking points,This competition did not count towards tiers,Featured Prediction Competition
Partly Sunny with a Chance of Hashtags,$500 ,9 years ago,9 years ago,['rmse'],258,299,"3,590","In this competition you are provided a set of tweets related to the weather. The challenge is to analyze the tweet and determine whether it has a positive, negative, or neutral sentiment, whether the weather occurred in the past, present, or future, and what sort of weather the tweet references. It's a lot to mine from so few characters, but if the going gets tough you can always blame the weather...
""Please knock out the power giant storm that is passing thru....please."" -Tweet #74096
CrowdFlower
We are excited to team up with CrowdFlower on the first of what we hope will be many fun machine learning projects. CrowdFlower is debuting a new open data library and we're always looking for an excuse to have a competition. Why is this exciting? Sweet, sweet Labels.
Data repositories sometimes have more in common with a landfill than a library. They're home to tattered piles of spreadsheets in odd formats with nary a shred of documentation to tell the GDP of Chile from the migratory patterns of North American goldfinches. If creating value from this digital exhaust is a defining theme of the big data explosion, most repositories leave you choking on the diesel fumes of data disappointment. Such data is great if you are doing a report on the GDP of Chile, but not so useful if you are doing machine learning, or its red-headed step child, data science.
Crowdflower's data sets provide the thing that makes so many repositories fall short - data paired with labels. One can decide whether two English sentences are related, make judgments about yogurt chatter, or rank emotions on tweets about nuclear energy. It's all about the (wo)manpower to label what these bytes actually mean.
The Open Data Library
CrowdFlower Open Data Library is a repository of real data set samples that developers, researchers and data scientists can download and use to test and improve algorithms. Our mission is to encourage users to explore the possibilities and power of crowdsourcing. Open Data is free, available to anyone, and ready-to-use with CrowdFlower’s Platform.
New data sets are continuously added to CrowdFlower Open Data Library as users of the CrowdFlower Platform opt-in to share their data with the crowdsourcing community. Sample data sets currently available include tweets for sentiment and topic analysis, word combinations to test similarities, sentence combinations to test related topics, and more. Learn more at www.crowdflower.com.",4 files,25.4 MB,This competition awarded ranking points,This competition counted towards tiers,Playground Prediction Competition
"Flight Quest 2: Flight Optimization, Main Phase","$220,000 ",9 years ago,9 years ago,['custom metric'],121,146,"2,107","These pages describe the Main phase of this competition, which is now closed. Click here to visit the final phase of Flight Quest 2.
Think you can change the future of flight?
Did you know airlines are constantly looking for ways to make flights more efficient? From gate conflicts to operational challenges to air traffic management, the dynamics of a flight can change quickly and lead to costly delays.
There is good news. Advancements in real-time big data analysis are changing the course of flight as we know it. Imagine if the pilot could augment their decision-making process with “real time business intelligence” — information available in the cockpit that would allow them to make adjustments to their flight patterns. 
Your challenge, should you decide to accept it: 
Use the different data sets found on this page under Get the Data to develop a usable and scalable algorithm that delivers a real-time flight profile to the pilot, helping them make flights more efficient and reliably on time. For background on the competition structure, visit the Basic Structure page.
Be sure to check the Forums regularly to stay on top of the latest competition news.
Download data »
Make a submission »",18 files,27.15 GB,This competition awarded ranking points,This competition counted towards tiers,GE Quests Prediction Competition
Dogs vs. Cats,,9 years ago,9 years ago,['categorizationaccuracy'],213,232,"1,218","In this competition, you'll write an algorithm to classify whether images contain either a dog or a cat.  This is easy for humans, dogs, and cats. Your computer will find it a bit more difficult.
Deep Blue beat Kasparov at chess in 1997.
Watson beat the brightest trivia minds at Jeopardy in 2011.
Can you tell Fido from Mittens in 2013?
The Asirra data set
Web services are often protected with a challenge that's supposed to be easy for people to solve, but difficult for computers. Such a challenge is often called a CAPTCHA (Completely Automated Public Turing test to tell Computers and Humans Apart) or HIP (Human Interactive Proof). HIPs are used for many purposes, such as to reduce email and blog spam and prevent brute-force attacks on web site passwords.
Asirra (Animal Species Image Recognition for Restricting Access) is a HIP that works by asking users to identify photographs of cats and dogs. This task is difficult for computers, but studies have shown that people can accomplish it quickly and accurately. Many even think it's fun! Here is an example of the Asirra interface:
Asirra is unique because of its partnership with Petfinder.com, the world's largest site devoted to finding homes for homeless pets. They've provided Microsoft Research with over three million images of cats and dogs, manually classified by people at thousands of animal shelters across the United States. Kaggle is fortunate to offer a subset of this data for fun and research. 
Image recognition attacks
While random guessing is the easiest form of attack, various forms of image recognition can allow an attacker to make guesses that are better than random. There is enormous diversity in the photo database (a wide variety of backgrounds, angles, poses, lighting, etc.), making accurate automatic classification difficult. In an informal poll conducted many years ago, computer vision experts posited that a classifier with better than 60% accuracy would be difficult without a major advance in the state of the art. For reference, a 60% classifier improves the guessing probability of a 12-image HIP from 1/4096 to 1/459.
State of the art
The current literature suggests machine classifiers can score above 80% accuracy on this task [1]. Therfore, Asirra is no longer considered safe from attack.  We have created this contest to benchmark the latest computer vision and deep learning approaches to this problem. Can you crack the CAPTCHA? Can you improve the state of the art? Can you create lasting peace between cats and dogs?
Okay, we'll settle for the former. 
Acknowledgements
We extend our thanks to Microsoft Research for providing the data for this competition.",3 files,853.96 MB,This competition awarded ranking points,This competition counted towards tiers,Playground Prediction Competition
Personalize Expedia Hotel Searches - ICDM 2013,"$25,000 ",9 years ago,9 years ago,['ndcg@{k}'],336,408,"3,487","Expedia is the world’s largest online travel agency (OTA) and powers search results for millions of travel shoppers every day. In this competitive market matching users to hotel inventory is very important since users easily jump from website to website. As such, having the best ranking of hotels (“sort”) for specific users with the best integration of price competitiveness gives an OTA the best chance of winning the sale.
For this contest, Expedia has provided a dataset that includes shopping and purchase data as well as information on price competitiveness. The data are organized around a set of “search result impressions”, or the ordered list of hotels that the user sees after they search for a hotel on the Expedia website. In addition to impressions from the existing algorithm, the data contain impressions where the hotels were randomly sorted, to avoid the position bias of the existing algorithm. The user response is provided as a click on a hotel or/and a purchase of a hotel room.
Appended to impressions are the following:
1) Hotel characteristics
2) Location attractiveness of hotels
3) User’s aggregate purchase history
4) Competitive OTA information
Models will be scored via performance on a hold-out set.",4 files,518.75 MB,This competition awarded ranking points,This competition counted towards tiers,Featured Prediction Competition
Facebook Recruiting III - Keyword Extraction,,9 years ago,9 years ago,['fscoremicro'],366,366,"2,663","Looking for a data science position at Facebook?  After two successful prior Kaggle competitions, Facebook continues their mission to identify the best data scientists and software engineers that Kaggle has to offer. In this third installment, they seek candidates who have experience text mining large amounts of data.
This competition tests your text skills on a large dataset from the Stack Exchange sites.  The task is to predict the tags (a.k.a. keywords, topics, summaries), given only the question text and its title. The dataset contains content from disparate stack exchange sites, containing a mix of both technical and non-technical questions.
Positions are available in Menlo Park, Seattle, New York City, and London; candidates must have, or be eligible to obtain, authorization to work in the US or UK.
Please note: you must compete as an individual in recruiting competitions. You may only use the data provided to make your predictions. Crawling stack exchange sites to look up answers is not permitted. Facebook will review the code of the top participants before deciding whether to offer an interview. 
This competition counts towards rankings & achievements.  If you wish to be considered for an interview at Facebook, check the box ""Allow host to contact me"" when you make your first entry.
Acknowledgements
We thank Stack Exchange (and its users) for generously releasing the source dataset through its Creative Commons Data Dumps. All data is licensed under the cc-by-sa license.",3 files,3.19 GB,This competition awarded ranking points,This competition counted towards tiers,Recruitment Prediction Competition
StumbleUpon Evergreen Classification Challenge,"$5,000 ",9 years ago,9 years ago,"['tabular', 'internet', 'text', 'auc']",624,624,"7,486","StumbleUpon is a user-curated web content discovery engine that recommends relevant, high quality pages and media to its users, based on their interests. While some pages we recommend, such as news articles or seasonal recipes, are only relevant for a short period of time, others maintain a timeless quality and can be recommended to users long after they are discovered. In other words, pages can either be classified as ""ephemeral"" or ""evergreen"". The ratings we get from our community give us strong signals that a page may no longer be relevant - but what if we could make this distinction ahead of time? A high quality prediction of ""ephemeral"" or ""evergreen"" would greatly improve a recommendation system like ours.
Many people know evergreen content when they see it, but can an algorithm make the same determination without human intuition? Your mission is to build a classifier which will evaluate a large set of URLs and label them as either evergreen or ephemeral. Can you out-class(ify) StumbleUpon? As an added incentive to the prize, a strong performance in this competition may lead to a career-launching internship at one of the best places to work in San Francisco.",4 files,196.18 MB,This competition awarded ranking points,This competition counted towards tiers,Featured Prediction Competition
The Big Data Combine Engineered by BattleFin,"$18,500 ",9 years ago,9 years ago,['mae'],424,424,"8,915","The Big Data Combine engineered by BattleFin are rapid fire, live tryouts for computer scientists with elite predictive analytic skills intent on monetizing their models. The first stage of the competition is a predictive modeling competition that requires participants to develop a model that predicts stock price movements using sentiment data provided by RavenPack. Traders, analysts and investors are always looking for techniques to better predict price movements.  Knowing whether a security will increase or decrease allows traders to make better investment decisions and manage risk more effectively. 
This competition is designed to identify people with the talent to create a predictive model using financial data. Competitors are given intraday trading data showing stock price movements at 5 minute intervals and asked to predict the change two hours in the future. The winners of the predictive modeling phase are invited to the ""live"" Big Data Combine tryouts in Miami, FL. Up to 12 finalists will be selected to compete in the live event in Miami. The lucky few will pitch their predictive model to expert judges and an engaged audience. They have only three minutes to present in non-technical terms three items: personal background, predictive model description, and how they would use there model to make money in finance. If their model and presentation impresses our judges, they will be eligible to work with BattleFin and Deltix to convert their predictive model into a trading strategy.
Master of Ceremonies (""MC"")
Matt Iseman - Actor, Comedian, Host of American Ninja Warrior
Mr. Iseman has hosted the game shows Scream Play on E! and Casino Night on the GSN. He currently appears as a regular cast member on the home makeover show Clean House, and its companion outtakes show, Clean House Comes Clean, both on the Style Network. Additionally, he hosted season 2 and 3 of American Ninja Warrior on the channel G4. He also has worked episodically in television shows including The Drew Carey Show, NCIS, and General Hospital. He has appeared on the syndicated MAD TV, Comedy Central’s Premium Blend, Fox’s The Best Damn Sports Show Period, and Fox News Channel’s Red Eye w/ Greg Gutfeld. Mr. Iseman was also the host of Sports Soup, a spin-off of E!'s The Soup, on Versus. Style Network and Versus are owned by Comcast. Iseman began working with American Ninja Warrior (G4) in 2010. He uses his great athleticism and work as a comedian to add his style to the show with Johnny Moseley (American Professional Freestyle Skier), and Angela Sun (Sideline Correspondent).
Judges
Ilya Gorelik, CEO of Deltix Inc. CEO, Ilya Gorelik is responsible for setting the strategic direction of the company, as well as overseeing global product development, sales and marketing.Ilya has more than 15 years of experience managing large-scale software projects and teams. He was one of the key development leaders of PTC, where he worked from 1989 to 1998, attaining the position of Senior VP of Engineering and Chief Technology Officer. From 1998 until 2000, Ilya was Senior VP of Product Strategy and Development and Chief Scientist for FirePond. From 2000 until founding Deltix in 2005, Ilya worked as Advisory CTO for HighRoads and several other software technology companies. Ilya has a Ph.D. in ComputationalMechanics from Moscow Technical University, he received an MS in Mechanical Engineering from Minsk Technical University.
Peter Hafez, Head Quantitative Research at RavenPack
Peter is an award-winning expert in the field of applied news analytics and has consulted numerous leading trading and investment firms on how to take advantage of news analytics in financial markets. Peter has more than 10 years of experience in quantitative finance with companies such as Standard & Poor's, Credit Suisse First Boston, and Saxo Bank. He is a recognized speaker at conferences on behavioral finance and algorithmic trading. Peter holds a Master's degree in Quantitative Finance from City University's Cass Business School along with an undergraduate degree in Economics from Copenhagen University.
Nabyl Charania, Managing Director at Rokk3r Labs
Nabyl is a Co-Founder and Managing Director at Rokk3r Labs.  Rokk3r Labs is Miami based Venture Capital firm with 35 employees.  Rokk3r Labs fuses entrepreneurial and professional talent to help entrepreneurs create ideas, prototypes, products and generally invests in disruptive companies designed for the modern hyper-connected world. Prior, he was a Founder and Managing Director at Decipher Labs Inc. He graduated from University of Waterloo in 2000.
Zeid Barakat, Co-Founder at Flyberry Capital LLC 
Zeid is a Co-Founder of Flyberry Capital a Boston based hedge fund that utilizes a Big Data strategy. Zeid works with ‘best-in class’ MIT & Harvard computer scientists and machine learning experts to develop proprietary trading strategies, focused on event-based arbitrage. In charge of dfining corporate growth strategy,business development, marketing, managing Board of Advisers, and fundraising. He is an Entrepreneur in high-tech companies, focused on novel approaches to biotechnology, healthcare and financial services. He earned MBA degree in General Management and Entrepreneurship from MIT in 2008.
About BattleFin
BattleFin is a tournament platform that crowdsources the world's best investment talent. The firm uses rapid fire, real capital tournaments to democratically identify up and coming investment talent. BattleFin has recently been featured in Bloomberg Business Week in an article titles ""The Hedge Fund Hunger Games"". The firm is passionate about leveling the playing field in finance so that anyone with an internet connection can participate in its tournaments. The firm specializes in finding the hedge fund managers of tomorrow. To learn more about BattleFin visit BattleFin.com.
About Deltix
Founded in 2005 and with more than 50 staff, Deltix has established itself as a leader in the growing domain of software and services for quantitative research and systematic automated trading. The Deltix Product Suite is an end-to-end platform for all phases of the alpha discovery and trading life-cycle; including data collection and aggregation, model development, back-testing, optimization, simulation, and deployment to production trading.
About RavenPack
Financial firms are overloaded with information and have turned to computers to read news and other media. What would take days for an investment professional to read and interpret takes computers only a few milliseconds. Now financial institutions can react much faster to the ever increasing amounts of news and information available for making investment decisions. Powered by a proprietary text analysis platform, RavenPack the tournament data sponsor, analyzes novel and relevant stories published by major news sources to look for key scheduled and unexpected geopolitical, macro-economic and corporate events, topics and opinions that indicate changes in market sentiment. Sampled news sources represent the most reliable and authoritative publishers of business and financial news.RavenPack continuously analyzes relevant information from Dow Jones Newswires, regional editions for the Wall Street Journal, and Barron’s to produce real time news sentiment scores and events from entities across multiple asset classes, including currencies, commodities, organizations, companies, sectors, and industries.",3 files,30.23 MB,This competition awarded ranking points,This competition counted towards tiers,Research Prediction Competition
"Flight Quest 2: Flight Optimization, Milestone Phase","$250,000 ",9 years ago,9 years ago,['custom metric'],129,156,"1,681","These pages describe the first Milestone phase of this competition, which is now closed. Click here to visit the current phase of Flight Quest 2.
Think you can change the future of flight?
Did you know airlines are constantly looking for ways to make flights more efficient? From gate conflicts to operational challenges to air traffic management, the dynamics of a flight can change quickly and lead to costly delays.
There is good news. Advancements in real-time big data analysis are changing the course of flight as we know it. Imagine if the pilot could augment their decision-making process with “real time business intelligence” — information available in the cockpit that would allow them to make adjustments to their flight patterns. 
Your challenge, should you decide to accept it: 
Use the different data sets found on this page under Get the Data to develop a usable and scalable algorithm that delivers a real-time flight profile to the pilot, helping them make flights more efficient and reliably on time. For background on the competition structure, visit the Basic Structure page.
Be sure to check the Forums regularly to stay on top of the latest competition news.
Download data »
Make a submission »",13 files,7.41 GB,This competition awarded ranking points,This competition did not count towards tiers,GE Quests Prediction Competition
MasterCard - Data Cleansing Competition,"$100,000 ",9 years ago,9 years ago,['custom metric'],6,7,21,"This is a private, invitation-only competition. The relevant information is provided only to contestants. The competition is closed to new entrants.
Qualification for future private competitions is based solely on objective leaderboard performance in competitions.",,,This competition did not award ranking points,This competition counted towards tiers,Masters Prediction Competition
The ICML 2013 Bird Challenge,$500 ,10 years ago,9 years ago,['auc'],76,87,545,"We're aware the competition deadline is tight, but wanted to give Kagglers the chance to work on this interesting problem. The results will be presented at the Workshop on Machine Learning for Bioacoustics at ICML 2013.
Acknowledgements
http://sabiod.org",11 files,1.85 GB,This competition awarded ranking points,This competition counted towards tiers,Research Prediction Competition
Facial Keypoints Detection,,,,"['image', 'rmse']",175,208,"1,220","The objective of this task is to predict keypoint positions on face images. This can be used as a building block in several applications, such as:
tracking faces in images and video
analysing facial expressions
detecting dysmorphic facial signs for medical diagnosis
biometrics / face recognition
Detecing facial keypoints is a very challenging problem.  Facial features vary greatly from one individual to another, and even for a single individual, there is a large amount of variation due to 3D pose, size, position, viewing angle, and illumination conditions. Computer vision research has come a long way in addressing these difficulties, but there remain many opportunities for improvement.
This getting-started competition provides a benchmark data set and an R tutorial to get you going on analysing face images. Get started with R >>
Acknowledgements
The data set for this competition was graciously provided by Dr. Yoshua Bengio of the University of Montreal. James Petterson.",4 files,80.86 MB,This competition did not award ranking points,This competition did not count towards tiers,Getting Started Prediction Competition
RecSys2013: Yelp Business Rating Prediction,$500 ,10 years ago,9 years ago,['rmse'],158,209,"2,108","We are pleased to announce the 2013 Recommender Systems Challenge associated with ACM RecSys 2013.  Known as RecSysChallenge 2013, this is a LBS contest organized by Yelp. The theme of this year’s competition is personalized business recommendations for Yelp users. We provide a detailed snapshot of Yelp data: over 10,000 businesses, 8,000 check-in sites, 40,000 users, and 200,000 reviews from the Phoenix, AZ metropolitan area.
Contest
At the heart of any recommender system is an algorithm to predict ratings.  Contestants are asked to predict the users’ future ratings of businesses. Participants will create a model to predict the rating a user would assign to a business. Models will be graded on accuracy using the root mean squared error metric. See the Evaluation page for more details.
The competition starts on May 3, 2013 and ends on August 31, 2013. Submissions to the workshop must be submitted by September 8, 2013.
A total of $500 in prize money will be presented to the winners. Top ranking teams will also be recognized at the RecSys banquet.
Workshop
After the competition closes, we will also hold a full-day workshop, co-located with the ACM RecSys 2013 conference, to discuss interesting approaches to the competition problem as well as lessons learned. We invite all contest participants to present their approach to building business recommendations. Contributions may focus on any individual steps such as feature extraction or model building; or describe an end-to-end system to predict business ratings.  
Workshop participation is independent of the contest: researchers are welcome to participate in either (or, hopefully, both!).
Looking for last year's challenge?  Check out RecSys Challenge 2012.",7 files,179.46 MB,This competition awarded ranking points,This competition counted towards tiers,Research Prediction Competition
KDD Cup 2013 - Author Disambiguation Challenge (Track 2),"$7,500 ",10 years ago,9 years ago,['fscoremicro'],237,325,"2,304","The ability to search literature and collect/aggregate metrics around publications is a central tool for modern research. Both academic and industry researchers across hundreds of scientific disciplines, from astronomy to zoology, increasingly rely on search to understand what has been published and by whom.
Microsoft Academic Search is an open platform that provides a variety of metrics and experiences for the research community, in addition to literature search. It covers more than 50 million publications and over 19 million authors across a variety of domains, with updates added each week. One of the main challenges of providing this service is caused by author-name ambiguity. This KDD Cup task challenges participants to determine which authors in a given data set are duplicates.",3 files,654.81 MB,This competition awarded ranking points,This competition counted towards tiers,Featured Prediction Competition
KDD Cup 2013 - Author-Paper Identification Challenge (Track 1),"$7,500 ",10 years ago,9 years ago,['custom metric'],552,754,"9,420","THIS COMPETITION IS COMPLETE. CONGRATULATIONS TO THE PRELIMINARY WINNERS!
The ability to search literature and collect/aggregate metrics around publications is a central tool for modern research. Both academic and industry researchers across hundreds of scientific disciplines, from astronomy to zoology, increasingly rely on search to understand what has been published and by whom.
Microsoft Academic Search is an open platform that provides a variety of metrics and experiences for the research community, in addition to literature search. It covers more than 50 million publications and over 19 million authors across a variety of domains, with updates added each week. One of the main challenges of providing this service is caused by author-name ambiguity. On one hand, there are many authors who publish under several variations of their own name.  On the other hand, different authors might share a similar or even the same name.
As a result, the profile of an author with an ambiguous name tends to contain noise, resulting in papers that are incorrectly assigned to him or her. This KDD Cup task challenges participants to determine which papers in an author profile were truly written by a given author.",9 files,657.8 MB,This competition awarded ranking points,This competition counted towards tiers,Featured Prediction Competition
Influencers in Social Networks,"$2,350 ",10 years ago,10 years ago,['auc'],132,177,"2,105","Data Science London and the UK Windows Azure Users Group in partnership with Microsoft and Peerindex, announce the Influencers in Social Networks competition as part of The Big Data Hackathon.  
The dataset, provided by Peerindex, comprises a standard, pair-wise preference learning task. Each datapoint describes two individuals, A and B. For each person, 11 p
The binary label represents a human judgement about which one of the two individuals is more influential. A label '1' means A is more influential than B. 0 means B is more influential than A. The goal of the challenge is to train a machine learning model which, for pairs of individuals, predicts the human judgement on who is more influential with high accuracy. Labels for the dataset have been collected by PeerIndex using an application similar to the one described in this post.
A python script computing a sample benchmark solution is available here: 
Competition begins: Saturday, Apr 13, 1pm BST (12 noon UTC)
This competition awards 25% the ranking points of a standard competition, but does not count towards tiers. ",3 files,2.72 MB,This competition awarded 0.25X ranking points,This competition did not count towards tiers,Featured Prediction Competition
Challenges in Representation Learning: The Black Box Learning Challenge,$500 ,10 years ago,10 years ago,['categorizationaccuracy'],211,232,"1,919","We are also providing a dataset of approx. 130,000 unsupervised examples that contestants can use to improve their models. The unsupervised data is a CSV file in the same format as the private test set (i.e. without the labels). The extra data comes from a distribution that is very similar to the training/test set distribution.
We provide example code for this contest as part of the pylearn2 package at https://github.com/lisa-lab/pylearn2
For this contest, look at the pylearn2/scripts/icml_2013_wrepl/black_box directory.",5 files,4.13 GB,This competition awarded ranking points,This competition counted towards tiers,Research Prediction Competition
Challenges in Representation Learning: Multi-modal Learning,$500 ,10 years ago,10 years ago,['auc'],24,31,126,"In this contest, competitors will design systems to learn about two modalities of data: images and text. The provided training data is Louis von Ahn's Small ESP Game Dataset, containing images and word tags for these images. Competitors should train their system to associate images to sets of word tags. At test time, the system is presented with two possible sets of word tags for an image, and must determine which is the correct set of word tags.
Because this task is very easy for humans to do, we will not provide the final test inputs until one week before the contest closes. Preliminary winners will need to release their winning code and demonstrate that they did not manually label the test set. We reserve the right to disqualify entries that may involve any manual labeling of the test set.",7 files,2.15 GB,This competition awarded ranking points,This competition counted towards tiers,Research Prediction Competition
Challenges in Representation Learning: Facial Expression Recognition Challenge,$500 ,10 years ago,10 years ago,['categorizationaccuracy'],56,63,190,"Example baseline submissions are available as part of the pylearn2 python package available at https://github.com/lisa-lab/pylearn2
The baseline submissions for this contest are in pylearn2/scripts/icml_2013_wrepl/emotions
Because this task is very easy for humans to do, we will not provide the final test inputs until one week before the contest closes. Preliminary winners will need to release their winning code and demonstrate that they did not manually label the test set. We reserve the right to disqualify entries that may involve any manually labeling of the test set.
Preliminary winners will need to release their winning code and demonstrate that they did not manually label the test set. We reserve the right to disqualify entries that may involve any manually labeling of the test set.",5 files,698.34 MB,This competition awarded ranking points,This competition counted towards tiers,Research Prediction Competition
Cause-effect pairs,"$10,000 ",10 years ago,9 years ago,['custom metric'],266,313,"4,576","                              The problem of attributing causes to effects is pervasive in science, medicine, economy and almost every aspects of our everyday life involving human reasoning and decision making. What affects your health? the economy? climate changes? The gold standard to establish causal relationships is to perform randomized controlled experiments. However, experiments are costly while non-experimental ""observational"" data collected routinely around the world are readily available. Unraveling potential cause-effect relationships from such observational data could save a lot of time and effort.
Consider for instance a target variable B, like occurence of ""lung cancer"" in patients. The goal would be to find whether a factor A, like ""smoking"", might cause B. The objective of the challenge is to rank pairs of variables {A, B} to prioritize experimental verifications of the conjecture that A causes B.
As is known, ""correlation does not mean causation"". More generally, observing a statistical dependency between A and B does not imply that A causes B or that B causes A;  A and B could be consequences of a common cause. But, is it possible to determine from the joint observation of samples of two variables A and B that A should be a cause of B? There are new algorithms that have appeared in the literature in the past few years that tackle this problem. This challenge is an opportunity to evaluate them and propose new techniques to improve on them.
We provide hundreds of pairs of real variables with known causal relationships from domains as diverse as chemistry, climatology, ecology, economy, engineering, epidemiology, genomics, medicine, physics. and sociology. Those are intermixed with controls (pairs of independent variables and pairs of variables that are dependent but not causally related) and semi-artificial cause-effect pairs (real variables mixed in various ways to produce a given outcome).
This challenge is limited to pairs of variables deprived of their context. Thus constraint-based methods relying on conditional independence tests and/or graphical models are not applicable. The goal is to push the state-of-the art in complementary methods, which can eventually disambiguate Markov equivalence classes. If you are skeptical that this is possible, try this quiz: Examine the plot below of values of variable B plotted as a function of values of variable A. Can you guess which one is a cause of the other? Hint: Some non-linear functions are non-invertible.
July 1: A new data release was made to address a normalization problem and the deadline was extended. Scores on the public leaderboard prior to July 1 were decreased by 0.5. Please make new submissions with the new validation set. The competition is open to new teams.",30 files,1 GB,This competition awarded ranking points,This competition counted towards tiers,Research Prediction Competition
Yelp Recruiting Competition,,10 years ago,9 years ago,['rmsle'],350,350,"3,831","Here at Yelp, we really love the quality of our data. We're grateful that so many of our users take the time to write such great reviews. We track 3 community-powered metrics of review quality: Useful, Funny, Cool. Over time, a good review will accumulate lots of votes in these categories from the community. However, another extremely important quality feature is the freshness of a review. What if we didn't have to wait for the community to vote on the best reviews to know which ones are high quality?
The goal of this competition is to estimate the number of Useful votes a review will receive. Yelp isn't only looking for the answer to this question; we're looking for an engineer that can solve this problem and push their code to production. The prize is a fast track through the recruiting process -- straight to an interview and the opportunity to show Yelp Engineers just what you've got.
For more information about the exciting opportunities at Yelp, check out http://www.yelp.com/careers!

This competition counts towards rankings & achievements.",7 files,278.41 MB,This competition awarded ranking points,This competition counted towards tiers,Recruitment Prediction Competition
ICDAR2013 - Handwriting Stroke Recovery from Offline Data,"$1,000 ",10 years ago,10 years ago,['rmse'],40,41,241,"IAPR/ICDAR award to be presented during ICDAR2013 that will be held in Washington, DC.",7 files,40.62 MB,This competition awarded ranking points,This competition did not count towards tiers,Research Prediction Competition
Data Science London + Scikit-learn,,,,['categorizationaccuracy'],190,191,"1,472","hosting a meetup on Scikit-learn
We encourage participants to post code via the ""Tutorials"" link on the left.  Don't worry about accuracy or whether your code is perfect.  The aim here is to explore sklearn by using it. 
 Its implementation is high quality due to s
Meetup Information
Thursday, March 7, 2013, 
“Learning in Python with scikit-learn"" by Andreas Mueller
""Parallel and large scale learning with scikit-learn"" by Olivier Grisel
notebook interface
How to perform scalable text feature extraction with the Hashing Trick
and hyper parameters tuning
How to optimize memory usage with memory mapping
How to approximate kernel Support Vector Machines for large scale datasets
A short introduction to Ensembles with model averaging and Random Forests
 by day and a Python machine learning hacker by night. He is interested in applications to Natural Language Processing, Computer Vision and predictive modelling.",3 files,8.01 MB,This competition did not award ranking points,This competition did not count towards tiers,Getting Started Prediction Competition
ICDAR2013 - Gender Prediction from Handwriting,"$1,000 ",10 years ago,10 years ago,['logloss'],189,205,"1,877","In addition to $1000, the ",21 files,9.2 GB,This competition awarded ranking points,This competition counted towards tiers,Research Prediction Competition
Just the Basics - Strata 2013 After-party,,,,['auc'],48,51,817,"Missed the one hour Just the Basics tutorial competition? Didn't get to implement that method you had in mind? Too many coffee breaks has your brain in Beautiful Mind mode?
This is the after-party competition. Same data. Same problem. More time! You have until the close of Strata to have fun with the problem.
Competition Starts: approximately 12:30 PM PT (3:30 PM ET), 02/26/2013
Competition Ends: 5:00 PM PT (8:00 PM ET), 02/28/2013",3 files,3.62 MB,This competition did not award ranking points,This competition did not count towards tiers,Getting Started Prediction Competition
Just the Basics - Strata 2013,,,,['auc'],49,52,290,"Two of Kaggle's very own are presenting an introductory tutorial at Strata 2013. Targeted for those with basic programming experience, it will cover the end-to-end analysis of predictive data problems. The tutorial is comprised of four sections, the last of which is this, a hands-on Kaggle competition in which participants can experience firsthand the joys of creating a model and the sorrows of overfitting.
Competition ends:
9:00am Tuesday, 02/26/2013
Location: Ballroom AB
Competition Starts: Approximately 11:15 AM PT (2:15PM ET), 02/26/2013
Competition Ends: 12:30 PM PT (3:30 PM ET), 02/26/2013
Open to the public!
Yes, you can participate in this for-fun competition without attending the tutorial.  
For fun?
You heard correctly; there's no Kaggle points or money up for grabs here. Isn't getting to lunch early a big enough motivation? 
Unlimited* submissions!
Show our servers who's boss! *for small values of unlimited
Where's the data?
To prevent head starts, the data will be available at the start of the competition.
About the presenters
Ben Hamner has worked with machine learning problems in a variety of different domains, including natural language processing, computer vision, web classification, and neuroscience. Prior to joining Kaggle, he applied machine learning to improve brain-computer interfaces as a Whitaker Fellow at the École Polytechnique Fédérale de Lausanne in Lausanne, Switzerland. He graduated with BSE in Biomedical Engineering, Electrical Engineering, and Math from Duke University.
William Cukierski has a bachelor’s degree in physics from Cornell University and a Ph.D. in biomedical engineering from Rutgers University, where he studied applications of machine learning in cancer research.",6 files,3.65 MB,This competition did not award ranking points,This competition did not count towards tiers,Getting Started Prediction Competition
Job Salary Prediction,"$6,000 ",10 years ago,10 years ago,['mae'],289,336,"2,863","Kaggle Startup Programplease apply
Successful models will incorporate some analysis of the impact of including different keywords or phrases, as well as making use of the structured data fields like location, hours or company.  Some of the structured data shown (such as category) is 'inferred' by Adzuna's own processes, based on where an ad came from or its contents, and may not be ""correct"" but is representative of the real data.
You will be provided with a training data set on which to build your model, which will include all variables including salary.  A second data set will be used to provide feedback on the public leaderboard.  After approximately 6 weeks, Kaggle will release a final data set that does not include the salary field to participants, who will then be required to submit their salary predictions against each job for evaluation.",10 files,388.14 MB,This competition awarded ranking points,This competition counted towards tiers,Featured Prediction Competition
The Marinexplore and Cornell University Whale Detection Challenge,"$10,000 ",10 years ago,10 years ago,['auc'],245,309,"3,293","Read the summary of the competition for a quick overview of the impact of the results.
We depend on shipping industry's uninterrupted ability to transport goods across long distances. Navigation technologies combine accurate position and environmental data to calculate optimal transport routes. Accounting for and reducing the impact of commercial shipping on the ocean’s environment, while achieving commercial sustainability, is of increasing importance, especially as it relates to the influence of cumulative noise “footprints” on the great whales.
Illustration of ships navigating safely around the habitat of whales.
Right whales make a half-dozen types of sounds, but the characteristic up-call is the one identified by the auto-detection buoys. 
Right whale up-call
Marinexplore and Cornell researchers challenge YOU to beat the existing whale detection algorithm identifying the right whale calls. This will advance ship routing decisions in the region.
[For details on the buoy network see a paper published by Acoustical Society of America.]
Read the summary of the competition for a quick overview of the impact of the results.",4 files,546.06 MB,This competition awarded ranking points,This competition counted towards tiers,Featured Prediction Competition
Predicting Parkinson's Disease Progression with Smartphone Data,"$10,000 ",10 years ago,10 years ago,['rmse'],,,,"Prizes will be given in the form of cash grants, with no strings attached.
Prizes include:
A grand prize of $10,000 for the team judged to have the most impactful use of the data.",13 files,18.07 GB,,,Research Prediction Competition
Blue Book for Bulldozers,"$10,000 ",10 years ago,10 years ago,['rmsle'],474,585,"4,852","There is a $10,000 prize pool for this competition, with prizes awarded to the top 3 places:
1st place: $6,500 
2nd place: $2,500 
3rd place: $1,000",14 files,213.81 MB,This competition awarded ranking points,This competition counted towards tiers,Featured Prediction Competition
Event Recommendation Engine Challenge,"$5,000 ",10 years ago,10 years ago,['custom metric'],223,281,"3,019","We (the competition hosts) are excited to sponsor the Event Recommendation Engine Challenge, which asks you to predict what events our users will be interested in based on events they’ve responded to in the past, user demographic information, and what events they’ve seen and clicked on in our app. The insights you discover from this data, and the algorithms the winners create, will allow us to improve our event recommendation algorithm, a core part of our applications and a key element in improving user experience.
This is the first competition launching under the Kaggle Startup Program!",10 files,387.48 MB,This competition awarded ranking points,This competition counted towards tiers,Featured Prediction Competition
Leaping Leaderboard Leapfrogs,$900 ,10 years ago,10 years ago,['rmse'],,,,"The leaderboard is a central fixture of the Kaggle experience. It provides context to the incredible work accomplished by the Kaggle data science community. To a competitor, the leaderboard is a dynamic, living, action-filled battle. Tactics come to life. Individuals leapfrog over each other.  Teams merge and blend submissions.  Some submit early and often, attempting to build up insurmountable leads. Others bide time, waiting to pounce minutes before the buzzer with their finest of forests.  We see the joys of regularization and the agony of overfitting.  It's raw. It's beautiful. It's thousands of hours of collective human toil.
It's boring.
To an observer, the leaderboard is a spreadsheet.  They see funny team names, numbers with too many decimals, strange column titles, and none of the history behind the battle. We run a veritable nerd olympics, but instead of smashing the 100m world record, we're elbowing for a few decimal places of some esoteric quantity called a capped binomial deviance. It's faceless. It's cold. It fails to tell the story of the battle. And you know what that means?
This means war.
We're calling on you to bring the leaderboard to life.  Break out the D3. Sacrifice an old PC to the javascript gods. Abandon all text, ye who enter here.  We're bootstrapping our own community to do what they do best, and that is doing things better.
What kinds of submissions do we hope result from this competition?
Maybe you know an API or two and can create a motion chart?
Maybe you know the hot, new HTML5 canvas tricks?
Maybe you know of an R package that styles plots like The Economist or XKCD?
Maybe you know Edward Tufte and can call in a favor?
Be creative. Scrape profile photos. Examine team formation. Examine relative scores. Watch for edge cases, cluttered text, and all the gotchas that crop up when you juggle a leaderboard of 10 vs. 1000 teams.  We're looking for entries that convey the storyline behind the leaderboard.  Style and substance counts, as does reproducibility (sorry to the Bob Rosses of the world who want to hand draw their submission).  Web-readiness is appreciated, but we know better than to put such constraints on the Kaggle community.  Use whatever brush you wish to paint this masterpiece.
Credits:
We'd like to acknowledge Chris Mulligan at Columbia University for providing the impetus that put this prospect in motion. You can see his blog post or even check out a git repository of the code he used to do it.
Image:",3 files,124.49 MB,,,Research Prediction Competition
Traveling Santa Problem,"$3,000 ",10 years ago,10 years ago,['custom metric'],353,389,"3,595","This competition will launch at midnight UTC on Saturday, December 15.
Santa Claus was excited to learn about the Kaggle competition platform, and wanted to use it for a slightly different purpose. Rather than a predictive modeling problem, he has an optimization problem for you: a very, very important optimization problem.
Santa needs help choosing the route he takes when delivering presents around the globe. Every year, Santa has to visit every boy and girl on his list.  It's a tough challenge, and Santa admits he scored a B- on his combinatorical optimization final. He's hoping you can develop algorithms that will solve his problem year after year.
Santa asked that we give you one particular instance of his TSP (Traveling Santa Problem). However, Santa's dilemma isn't quite the same as the Traveling Salesman Problem with which you may be familiar. Santa likes to see new terrain every year--don't ask, it's a reindeer thing--and doesn't want his route to be predictable.
You're looking for shortest-distance paths through a set of chimneys, but instead of providing one path, Santa asks you to provide two disjoint paths. If one of your paths contains an edge from A to B, your other path must not contain an edge from A to B or from B to A (either order still counts as using that edge). Your score is the larger of the two distances.  Santa asks competition winners to publish and open source the algorithms they use (for his future use, of course).
Rudolph was very adament about minimizing his workload. Trust us, you don't want to be on Rudolph's bad side.
Important note about prizes: We believe that Kaggle's public leaderboard is very important for both the fun of the competition and achieving great results, and we want to provide an incentive for everyone to submit to the public leaderboard all along the way (even though you can easily determine your submission's score all by yourself). So the competition will have two sets of prizes, one based on the scores at the end of the competition, and one based on the scores at the end of a randomly chosen day (UTC) between December 23 and January 17.  The day will not be revealed (or even chosen) until after the competition ends. (The competition will end at the end of the day UTC on January 18.)
  Attributions:
Data generation and lots of help framing the problem (including coming up with this TSP variant): Robert Bosch of Oberlin College Math Department
Santa photo: AurélienS
Sleigh photo: Creative Tools
Globe: William Cook
   ",2 files,4.75 MB,This competition awarded ranking points,This competition counted towards tiers,Featured Prediction Competition
Visualize the State of Public Education in Colorado,"$5,000 ",10 years ago,10 years ago,['rmse'],,,,"Winning Analysis and Visualization will receive cash awards for the following ranked entries:
1st: $3,500
2nd:$1,000
3rd: $ 500
All submissions will be evaluated by the Visual.ly team and qualified visualizers will be given admission into the Visual.ly Marketplace for future work.",27 files,7.04 MB,,,Research Prediction Competition
Prescription Volume Prediction,,10 years ago,10 years ago,['mcap@k'],12,13,416,"This is a private, invitation-only competition. The relevant information is provided only to contestants.",,,This competition did not award ranking points,This competition counted towards tiers,Masters Prediction Competition
Merck Molecular Activity Challenge,"$40,000 ",10 years ago,10 years ago,['custom metric'],236,269,"2,979","Help enable the development of safe, effective medicines.
When developing new medicines it is important to identify molecules that are highly active toward their intended targets but not toward other targets that might cause side effects. The objective of this competition is to identify the best statistical techniques for predicting biological activities of different molecules, both on- and off-target, given numerical descriptors generated from their chemical structures
The challenge is based on 15 molecular activity data sets, each for a biologically relevant target. Each row corresponds to a molecule and contains descriptors derived from that molecule's chemical structure.
In addition to the prediction competition, Merck is also hosting a visualization challenge with a $2,000 prize for the most insightful and elegant graphical representations of the data.
Prizes total $40,000.",7 files,123.55 MB,This competition awarded ranking points,This competition counted towards tiers,Featured Prediction Competition
Job Recommendation Challenge,"$20,000 ",10 years ago,10 years ago,['custom metric'],81,92,686,"This competition has completed. Congratulations to the winners along with all the other participants!
CareerBuilder.com is proud to sponsor the Job Recommendation Engine Challenge, which asks you to predict what jobs its users will apply to based on their previous applications, demographic information, and work history. The insights you discover from this data, and the algorithms the winners create, will allow CareerBuilder to improve its job recommendation algorithm, a core part of its website and a key element in improving user experience.   

There will also be a data visualization prospect towards the end of this contest.",13 files,1.66 GB,This competition awarded ranking points,This competition counted towards tiers,Prospect Prediction Competition
Digit Recognizer,,,,"['tabular', 'image', 'multiclass classification', 'categorizationaccuracy']","1,238","1,238","4,386","Start here if...
You have some experience with R or Python and machine learning basics, but you’re new to computer vision. This competition is the perfect introduction to techniques like neural networks using a classic dataset including pre-extracted features.
Competition Description
MNIST (""Modified National Institute of Standards and Technology"") is the de facto “hello world” dataset of computer vision. Since its release in 1999, this classic dataset of handwritten images has served as the basis for benchmarking classification algorithms. As new machine learning techniques emerge, MNIST remains a reliable resource for researchers and learners alike.
In this competition, your goal is to correctly identify digits from a dataset of tens of thousands of handwritten images. We’ve curated a set of tutorial-style kernels which cover everything from regression to neural networks. We encourage you to experiment with different algorithms to learn first-hand what works well and how techniques compare.
Practice Skills
Computer vision fundamentals including simple neural networks
Classification methods such as SVM and K-nearest neighbors
Acknowledgements 
More details about the dataset, including algorithms that have been tried on it and their levels of success, can be found at http://yann.lecun.com/exdb/mnist/index.html. The dataset is made available under a Creative Commons Attribution-Share Alike 3.0 license.",3 files,128.13 MB,This competition does not award ranking points,This competition does not count towards tiers,Getting Started Prediction Competition
EMI Music Data Science Hackathon - July 21st - 24 hours,"$10,000 ",10 years ago,10 years ago,['rmse'],133,170,"1,319","(Data will be made available 24 hours prior to the start of the contest)
For more info http://musicdatascience.com/
hashtag #musicdata #ds_ldn #DSGhack
  Proudly brought to you by
 ",11 files,46.44 MB,This competition awarded 0.25X ranking points,This competition did not count towards tiers,Featured Prediction Competition
Raising Money to Fund an Organizational Mission,"$10,000 ",10 years ago,10 years ago,['averageamongtopp'],27,29,205,"Many organizations prospect for loyal supporters and donors by sending direct mail appeals. This is an effective way to build a large base, but can be very expensive and have a low efficiency. Eliminating likely non-donors is the key to running an efficient Prospecting program and ultimately to pursuing the mission of the organization. Help us help these organizations to target the best prospective donors and fund organizational goals! 
Please note: This competition has rules that we have not used previously restricting what kinds of models are acceptable. Please see the rules page for more information.",14 files,9.71 GB,This competition awarded ranking points,This competition counted towards tiers,Featured Prediction Competition
Practice Fusion Diabetes Classification,"$10,000 ",10 years ago,10 years ago,['logloss'],145,167,"2,161","In the first phase of this prediction challenge Practice Fusion invited anyone with an interest in using electronic medical record data to improve public health to submit and vote on ideas for prediction problems based on a new dataset of 10,000 de-identified medical records. The votes are in and Shea Parkes' top voted submission has won.
Practice Fusion is now sponsoring the second and final phase of the challenge inspired by the winning problem: Identify patients diagnosed with Type 2 Diabetes Mellitus.
Over 25 million people, or nearly 8.3% of the entire United States population, have diabetes. Diabetes is also associated with a wide range of complications from heart disease and stroke to blindness and kidney disease. Predicting who has diabetes will lead to a better understanding of these complications and the common comorbidities that diabetics suffer.
The Challenge: Given a de-identified data set of patient electronic health records, build a model to determine who has a diabetes diagnosis, as defined by ICD9 codes 250, 250.0, 250.*0 or 250.*2 (e.g., 250, 250.0, 250.00, 250.10, 250.52, etc).",,,This competition awarded ranking points,This competition counted towards tiers,Research Prediction Competition
Predicting a Biological Response,"$20,000 ",11 years ago,10 years ago,['logloss'],698,791,"8,819","The objective of the competition is to help us build as good a model as possible so that we can, as optimally as this data allows, relate molecular information, to an actual biological response.
We have shared the data in the comma separated values (CSV) format. Each row in this data set represents a molecule. The first column contains experimental data describing an actual biological response; the molecule was seen to elicit this response (1), or not (0). The remaining columns represent molecular descriptors (d1 through d1776), these are calculated properties that can capture some of the characteristics of the molecule - for example size, shape, or elemental constitution. The descriptor matrix has been normalized.",3 files,31.06 MB,This competition awarded ranking points,This competition counted towards tiers,Featured Prediction Competition
ICFHR 2012 - Arabic Writer Identification,"$1,000 ",11 years ago,11 years ago,['categorizationaccuracy'],42,45,576,"Writer identification is a very active research field. It is of a primordial importance in forensic document examination when it helps experts in delibirating on the authenticity of a certain document.
This is a follow-up contest of the last year' Arabic Writer Identification Contest.
As we mentioned in the last edition, 
we have significantly augmented the number of writers (we have more than 200 writers in this new database).
we will not be providing any side-information (eg. number of documents per writer), as this is not necessarly known in a real forensic casework.
we will only provide binary images, as color and gray-level images might transform this into a pen identification task.
This competition is organized in conjunction with the International Conference of Frontiers in Handwriting Recognition ICFHR2012 which will be held in Bari, Italy in September 18-20.",7 files,18.99 MB,This competition awarded ranking points,This competition counted towards tiers,Research Prediction Competition
"KDD Cup 2012, Track 2","$8,000 ",11 years ago,11 years ago,['custom metric'],163,275,"5,276","TASK 2 DESCRIPTION
Search advertising has been one of the major revenue sources of the Internet industry for years. A key technology behind search advertising is to predict the click-through rate (pCTR) of ads, as the economic model behind search advertising requires pCTR values to rank ads and to price clicks. In this task, given the training instances derived from session logs of the Tencent proprietary search engine, soso.com, participants are expected to accurately predict the pCTR of ads in the testing instances.
TRAINING DATA FILE   
The training data file is a text file, where each line is a training instance derived from search session log messages. To understand the training data, let us begin with a description of search sessions.   
A search session refers to an interaction between a user and the search engine. It contains the following ingredients: the user, the query issued by the user, some ads returned by the search engine and thus impressed (displayed) to the user, and zero or more ads that were clicked by the user. For clarity, we introduce a terminology here. The number of ads impressed in a session is known as the ’depth’. The order of an ad in the impression list is known as the ‘position’ of that ad. An Ad, when impressed, would be displayed as a short text known as ’title’, followed by a slightly longer text known as the ’description’, and a URL (usually shortened to save screen space) known as ’display URL’.   
We divide each session into multiple instances, where each instance describes an impressed ad under a certain setting  (i.e., with certain depth and position values).  We aggregate instances with the same user id, ad id, query, and setting in order to reduce the dataset size. Therefore, schematically, each instance contains at least the following information:
UserID 
AdID 
Query 
Depth 
Position 
Impression 
the number of search sessions in which the ad (AdID) was impressed by the user (UserID) who issued the query (Query).
Click 
the number of times, among the above impressions, the user (UserID) clicked the ad (AdID).   
Moreover, the training, validation and testing data contain more information than the above list, because each ad and each user have some additional properties. We include some of these properties into the training, validation  and the testing instances, and put other properties in separate data files that can be indexed using ids in the instances. For more information about these data files, please refer to the section ADDITIONAL DATA FILES. 
Finally, after including additional features, each training instance is a line consisting of fields delimited by the TAB character: 
1. Click: as described in the above list. 
2. Impression: as described in the above list. 
3. DisplayURL: a property of the ad. 
The URL is shown together with the title and description of an ad. It is usually the shortened landing page URL of the ad, but not always. In the data file,  this URL is hashed for anonymity. 
4. AdID: as described in the above list. 
5. AdvertiserID: a property of the ad. 
Some advertisers consistently optimize their ads, so the title and description of their ads are more attractive than those of others’ ads. 
6. Depth: a property of the session, as described above.   
7. Position: a property of an ad in a session, as described above. 
8. QueryID:  id of the query. 
This id is a zero‐based integer value. It is the key of the data file 'queryid_tokensid.txt'.
9. KeywordID: a property of ads. 
This is the key of  'purchasedkeyword_tokensid.txt'. 
10. TitleID: a property of ads. 
This is the key of 'titleid_tokensid.txt'. 
11. DescriptionID: a property of ads. 
 This is the key of 'descriptionid_tokensid.txt'. 
12. UserID 
This is the key of 'userid_profile.txt'.  When we cannot identify the user, this field has a special value of 0.
 ADDITIONAL DATA FILES
There are five additional data files, as mentioned in the above section: 
1. queryid_tokensid.txt 
2. purchasedkeywordid_tokensid.txt 
3. titleid_tokensid.txt 
4. descriptionid_tokensid.txt 
5. userid_profile.txt 
Each line of the first four files maps an id to a list of tokens, corresponding to the query, keyword, ad title, and ad description, respectively. In each line, a TAB character separates the id and the token set.  A token can basically be a word in a natural language. For anonymity, each token is represented by its hash value.  Tokens are delimited by the character ‘|’. 
Each line of ‘userid_profile.txt’ is composed of UserID, Gender, and Age, delimited by the TAB character. Note that not every UserID in the training and the testing set will be present in ‘userid_profile.txt’. Each field is described below: 
1. Gender: 
'1'  for male, '2' for female,  and '0'  for unknown. 
2. Age: 
'1'  for (0, 12],  '2' for (12, 18], '3' for (18, 24], '4'  for  (24, 30], '5' for (30,  40], and '6' for greater than 40. 
TESTING DATASET
The testing dataset shares the same format as the training dataset, except for the counts of ad impressions and ad clicks that are needed for computing the empirical CTR. A subset of the testing dataset is used to consistently rank submitted/updated results on the leaderboard. The testing dataset is used for picking the final winners.
The log for forming the training dataset corresponds to earlier time than that of the testing dataset.
EVALUATION
Teams are expected to submit their result file in text format, in which each line corresponds to a line in the downloaded file with the same order, and there is only one field in each line: the predicted CTR. In the result file, the lines corresponding to the lines from validation dataset will be used to score for the ranking on the leaderboard during the competition except the last day (June 1, 2012), and the lines corresponding to the lines from testing dataset will be used for the ranking on the leaderboard on the day of June 1, 2012, and for picking the final winners.
The performance of the prediction will be scored in terms of the AUC (for more details about AUC, please see ‘ROC graphs: Notes and practical considerations for researchers‘ by Tom Fawcett). For a detailed definition of the metric, please refer to the tab ‘Evalaution’.
PRIZES
Teams with the best performance scores will be the winners. The prizes for the 1, 2 and 3 winners for task 2 are US Dollars $5000, $2000, and $1000, respectively.
 ",16 files,5.96 GB,This competition awarded ranking points,This competition counted towards tiers,Featured Prediction Competition
"KDD Cup 2012, Track 1","$8,000 ",11 years ago,11 years ago,['map@3'],656,863,"13,075"," BACKGROUND
Online social networking services have become tremendously popular in recent years, with popular social networking sites like Facebook, Twitter, and Tencent Weibo adding thousands of enthusiastic new users each day to their existing billions of actively engaged users. Since its launch in April 2010, Tencent Weibo, one of the largest micro-blogging websites in China, has become a major platform for building friendship and sharing interests online. Currently, there are more than 200 million registered users on Tencent Weibo, generating over 40 million messages each day. This scale benefits the Tencent Weibo users but it can also flood users with huge volumes of information and hence puts them at risk of information overload. Reducing the risk of information overload is a priority for improving the user experience and it also presents opportunities for novel data mining solutions. Thus, capturing users’ interests and accordingly serving them with potentially interesting items (e.g. news, games, advertisements, products), is a fundamental and crucial feature social networking websites like Tencent Weibo. 
TASK 1 DESCRIPTION 
The prediction task involves predicting whether or not a user will follow an item that has been recommended to the user. Items can be persons, organizations, or groups and will be defined more thoroughly below. 
DATASETS
First, we define some notations as follows:
“Item”: An item is a specific user in Tencent Weibo, which can be a person, an organization, or a group, that was selected and recommended to other users. Typically, celebrities, famous organizations, or some well-known groups were selected to form the ‘items set’ for recommendation. The size of this is about 6K items in the dataset. 
Items are organized in categories; each category belongs to another category, and all together they form a hierarchy. For example, an item, a vip user Dr. Kaifu LEE,
vip user: http://t.qq.com/kaifulee (wikipedia: http://en.wikipedia.org/wiki/Kai-Fu_Lee)
represented as
science-and-technology.internet.mobile
We can see that categories in different levels are separated by a dot ‘.’, and the category information about an item can help enhance your model prediction. For example, if a user Peter follows kaifulee, he may be interested in the other items of the category that kaifulee belongs to, and might also be interested in the items of the parent category of kaifulee’s category.
“Tweet”: a “tweet” is the action of a user posting a message to the microblog system, or the posted message itself. So when one user is “tweeting“, his/her followers will see the “tweet”.
“Retweet”: a user can repost a tweet and append some comments (or do nothing), to share it with more people (my followers).
“Comment”: a user can add some comments to a tweet. The contents of the comments  will not be automatically pushed to his/her followers as ‘tweeting’ or ‘retweeting’,but will appear at the ‘comment history’ of the commented tweet.
“Followee/follower”: If User B is followed by User A, B is a followee to A, and A is a follower to B.
We describe the datasets as follows:
The dataset represents a sampled snapshot of Tencent Weibo users’ preferences for various items –– the recommendation of items to users and the history of users’ ‘following’ history. It is of a larger scale compared to other publicly available datasets ever released. Also it provides richer information in multiple domains such as user profiles, social graph, item category, which may hopefully evoke deeply thoughtful ideas and methodology.
The users in the dataset, numbered in millions, are provided with rich information (demographics, profile keywords, follow history, etc.) for generating a good prediction model. To protect the privacy of the users, the IDs of both the users and the recommended items are anonymized as random numbers such that no identification is revealed. Furthermore, their information, when in Chinese, will be encoded as random strings or numbers, thus no contestant who understands Chinese would get advantages. Timestamps for recommendation are given for performing session analysis.
Two datasets in 7 text files, downloadable:
a) Training dataset : some fields are in the file rec_log_train.txt 
b) Testing dataset: some fields are in the file rec_log_test.txt
Format of the above 2 files:
(UserId)\t(ItemId)\t(Result)\t(Unix-timestamp)
Result: values are 1 or -1, where 1 represents the user UserId accepts the recommendation of item ItemId and follows it (i.e., adds it to his/her social network), and -1 represents the user rejects the recommended item.
We provide the true values of the ‘Result’ field in rec_log_train.txt, whereas in  rec_log_test.txt, the true values of the ‘Result’ field are withheld (for simplicity, in the file they are always 0). Another difference from rec_log_test.txt to rec_log_train.txt is that repeated recommended (UserId,ItemId) pairs were removed.
c)      More fields of the training and the testing datasets about the user and the item are in the following 5 files:
          i.              User profile data: user_profile.txt
Each line contains the following information of a user: the year of birth, the gender, the number of tweets and the tag-Ids. It is important to note that information about the users to be recommended is also in this file.
Format:
(UserId)\t(Year-of-birth)\t(Gender)\t(Number-of-tweet)\t(Tag-Ids)
Year of birth is selected by user when he/she registered.
Gender has an integer value of 0, 1, or 2, which represents “unknown”, “male”, or “female”, respectively.
Number-of-tweet is an integer that represents the amount of tweets the user has posted.
Tags are selected by users to represent their interests. If a user likes mountain climbing and swimming, he/she may select ""mountain climbing"" or ""swimming"" to be his/her tag. There are some users who select nothing. The original tags in natural languages are not used here, each unique tag is encoded as an unique integer.
Tag-Ids are in the form “tag-id1;tag-id2;...;tag-idN”. If a user doesn’t have tags, Tag-Ids will be ""0"".
        ii.              Item data: item.txt
Each line contains the following information of an item: its category and keywords.
Format:
(ItemId)\t(Item-Category)\t(Item-Keyword)
Item-Category is a string “a.b.c.d”, where the categories in the hierarchy are delimited by the character “.”, ordered in top-down fashion (i.e., category ‘a’ is a parent category of ‘b’, and category ‘b’ is a parent category of ‘c’, and so on.
Item-Keyword contains the keywords extracted from the corresponding Weibo profile of the person, organization, or group. The format is a string “id1;id2;…;idN”, where each unique keyword is encoded as an unique integer such that no real term is revealed.
      iii.              User action data: user_action.txt
The file user_action.txt contains the statistics about the ‘at’ (@) actions between the users in a certain number of recent days.
Format:
(UserId)\t(Action-Destination-UserId)\t(Number-of-at-action)\t(Number-of-retweet )\t(Number-of-comment)
If user A wants to notify another user about his/her tweet/retweet/comment, he/she would use an ‘at’ (@) action to notify the other user, such as ‘@tiger’ (here the user to be notified is ‘tiger’)..
For example, user A has retweeted user B 5 times, has “at” B 3 times, and has commented user B 6 times, then there is one line “A   B     3     5     6” in user_action.txt.
       iv.              User sns data: user_sns.txt
The file user_sns.txt contains each user’s follow history (i.e., the history of following another user). Note that the following relationship can be reciprocal.
Format:
(Follower-userid)\t(Followee-userid)
         v.              User key word data: user_key_word.txt
The file user_key_word.txt contains the keywords extracted from the tweet/retweet/comment by each user.
Format:
(UserId)\t(Keywords)
Keywords is in the form “kw1:weight1;kw2:weight2;…kw3:weight3”.
Keywords are extracted from the tweet/retweet/comment of a user, and can be used as features to better represent the user in your prediction model. The greater the weight, the more interested the user is with regards to the keyword.
Every keyword is encoded as a unique integer, and the keywords of the users are from the same vocabulary as the Item-Keyword. 
EVALUATION 
Teams’ scores and ranks on the leaderboard are based on a metric calculated from the predicted results in submitted result file and the held out ground truth of a validation dataset whose instances were a fixed set sampled from the testing dataset in the beginning and, until the last day of the competition (June 1, 2012) by then the scores and associated ranks on leaderboard are based on the predicted results and that of the rest of the testing dataset. This entails that the top-3 ranked teams at the time when the competition ends are the winners. The log for forming the training dataset corresponds to earlier time than that of the testing dataset.
The evaluation metric is average precision. For a detailed definition of the metric, please refer to the tab ‘Evaluation’. 
PRIZES 
The prizes for the 1, 2 and 3 winners for task 1 are US Dollars $5000, $2000, and $1000, respectively.
 ",7 files,1.58 GB,This competition awarded ranking points,This competition counted towards tiers,Featured Prediction Competition
The Hewlett Foundation: Automated Essay Scoring,"$100,000 ",11 years ago,11 years ago,['custom metric'],153,195,"2,493","The William and Flora Hewlett Foundation (Hewlett) is sponsoring the Automated Student Assessment Prize (ASAP).  Hewlett is appealing to data scientists and machine learning specialists to help solve an important social problem.  We need fast, effective and affordable solutions for automated grading of student-written essays.
Hewlett is sponsoring the following prizes:
$60,000:  1 place
$30,000:  2 place
$10,000:  3 place
You are provided access to hand scored essays, so that you can build, train and test scoring engines against a wide field of competitors.  Your success depends upon how closely you can deliver scores to those of human expert graders.  While we believe that these financial incentives are important, we also intend to introduce top performers both to leading vendors in the industry and/or an established base of interested buyers.  Hewlett is opening the field of automated student assessment to you.  We want to induce a breakthrough that is both personally satisfying and game-changing for improving public education.
Today, state departments of education are developing new forms of testing and grading methods, to assess the new common core standards.  In this environment the need for more sophisticated and affordable options is vital.  For example, we know that essays are an important expression of academic achievement, but they are expensive and time consuming for states to grade them by hand.  So, we are frequently limited to multiple-choice standardized tests.  We believe that automated scoring systems can yield fast, effective and affordable solutions that would allow states to introduce essays and other sophisticated testing tools.  We believe that you can help us pave the way towards a breakthrough.  ASAP is designed to achieve the following goals:
Challenge developers of automated student assessment systems to demonstrate their current capabilities.
Compare the efficacy and cost of automated scoring to that of human graders.
Reveal product capabilities to state departments of education and other key decision makers interested in adopting them.
The graded essays are selected according to specific data characteristics.  On average, each essay is approximately 150 to 550 words in length.  Some are more dependent upon source materials than others.  This range of essay type is provided so that we can better understand the strengths of your solution.  It is our intent to showcase quality and reliability, based on how well you can match expert human graders for each essay.
You will be provided with training data for each essay prompt.  The number of training essays does vary.  For example, the lowest amount of training data is 1,190 essays, randomly selected from a total of 1,982.  The data will contain ASCII formatted text for each essay followed by one or more human scores, and (where necessary) a final resolved human score.  Where it is relevant, you are provided with more than one human score, so that you may evaluate the reliability of the human scorers, but - keep in mind - that you will be predicting to the resolved score.  Also, please note that most essays are scored using a holistic scoring rubric.  However, one data set uses a trait scoring rubric.  The variability is intended to test the limits of your scoring engine’s capabilities.
Following a period of 3 months to build and/or train your engine, you will be provided with test data that will contain new essays, randomly selected for blind evaluation.  However, you will notice that the rater and resolved score columns will be blank.  You will be asked to supply, based on your engine's predictions for each essay, your score in the resolved score column and then submit your new data set on this site.
As part of the file that you will submit with your predictive scores, you will be asked to submit additional information.  We would like to understand both the time and capital that you’ve spent developing your engine, the profile of your team (or you as an individual if you are working alone) and the projected cost to implement your solution on a larger scale, along with any known limitations.  Basically, you will have the opportunity to present your case for who you are, why your model is commercially viable and to what extent you can use your model to satisfy the interests of potential buyers.  This other information will not be used to determine any prize rewards, and it is optional.  But, if you provide it, it will be used to evaluate whether or not your model should be presented to state departments of education and others who stand to benefit from your work.
Also, please note that it is our intention to stage other follow-on ASAP phases in the months ahead.  We are starting with graded essays and will follow with new data:
Phase 1: Demonstration for long-form constructed response (essays);
Phase 2: Demonstration for short-form constructed response (short answers);
Phase 3: Demonstration for symbolic mathematical/logic reasoning (charts/graphs).
In every instance, we seek to drive innovation for new solutions to automated student assessment.  We hope that you will enjoy this process.  May the best model win!",13 files,120.15 MB,This competition awarded ranking points,This competition counted towards tiers,Featured Prediction Competition
Benchmark Bond Trade Price Challenge,"$17,500 ",11 years ago,11 years ago,['wmae'],263,316,"2,778","The Benchmark Bond Trade Price Challenge is a competition to predict the next price that a US corporate bond might trade at. Contestants are given information on the bond including current coupon, time to maturity and a reference price computed by Benchmark Solutions.  Details of the previous 10 trades are also provided.  ",11 files,910.54 MB,This competition awarded ranking points,This competition counted towards tiers,Featured Prediction Competition
Getting Started,"$10,000 ",11 years ago,11 years ago,['rmse'],,,,Insert your description here,,,,,Featured Prediction Competition
ICDAR 2011 - Arabic Writer Identification,"$1,000 ",12 years ago,12 years ago,['mae'],30,34,139,"The dataset used for the competition is a sub-set of a large dataset that has been collected in Qatar University thanks to several volunteers with different backgrounds. The collection of data has been done under the supervision of Dr. Somaya Al-Maadeed.

The dataset has been digitized by the research assistant Wael Al-Ayoubi using an HP officejet 6500A Plus scanner. It has been cropped and preprocessed by Dr. Abdelâali Hassaïne.

Using this dataset is free of charge for academic purposes (please mention the competition article that will be published in ICDAR2011 to cite this dataset). For industrials purposes, please contact Dr. Somaya Al-Maadeed.
This competition is supervised by all of Dr. Somaya Al-Maadeed, Prof. Jihad Mohamad Alja’am, Prof. Ali Mohamed Jaoua from Qatar University and Prof. Ahmed Bouridane from Northumbria University.",6 files,552.65 MB,This competition awarded ranking points,This competition counted towards tiers,Featured Prediction Competition
Deloitte/FIDE Chess Rating Challenge,"$10,000 ",12 years ago,12 years ago,['custom metric'],181,191,"1,563","The Elo rating system was developed by the Hungarian physicist Arpad Elo in the 1950's and adopted by the world chess federation (FIDE) in 1970.  For more than four decades the FIDE Elo system has served as the primary yardstick in the world for measuring the strength of chess players.  FIDE ratings are used for determining invitations to chess tournaments including the world championship cycle, calculating specific pairings in most chess tournaments, and granting titles such as International Master (IM) or Grandmaster (GM).  In fact the Elo system is so popular that it has been adapted to many other applications beyond chess, including team sports rankings, other board games, and online video game systems.

However, despite the popularity of the Elo system, it has never really been demonstrated that the Elo approach is technically superior to other approaches.  Much of the appeal of the Elo system comes from its simplicity and familiarity, and it was ideally suited to a time when the computation of ratings was a significant practical challenge even for an annual list of a few hundred players.  Elo's formula was derived theoretically, in an era without large amounts of historical data or anything approaching today's computing power.  With the benefit of powerful computers and large game databases, we can easily investigate approaches that might do better than Elo at predicting chess results.  Such an investigation could have major implications on the theory and practice of ratings methodology, both for chess and also for the world beyond chess.


As an initial step in this process, Kaggle held an ""Elo versus the Rest of the World"" contest in the fall of 2010, requiring participants to develop predictive models that could forecast the results of chess games with great accuracy.  The contest was immensely popular among both chess enthusiasts and data scientists, drawing more than 3,500 submissions from 258 participating teams across 41 countries.  Although the prize fund was minimal, there was tremendous competitive drive spurring many participants to sustained effort.  For instance, despite a restriction keeping teams from making more than two submissions per day, there were twenty teams that made more than 50 submissions across the entire duration of the contest.  The underlying data is so simple and straightforward that it is very easy to begin playing with prediction models, but the overall problem of maximizing accuracy is so challenging that even the massive efforts of the first contest were not sufficient to identify a clearly superior approach.  There was a wide variety in the methodologies of just the top ten prizewinners, all of whom documented their approaches in significant detail after the completion of the contest.  The benchmark submission of the Elo formula finished far, far, behind, in 141st place out of 258, and there were 39 teams whose predictions were at least 5% more accurate than the Elo system.  It is clear that the Elo system is not the most accurate one, but it remains unclear which system is superior.

It is no longer a question of ""Elo versus the Rest of the World""; we must now hold a second contest and focus on finding a suitable replacement for Elo.  Possibly the best approach will be a modification of the Elo approach, or perhaps it will be something completely novel.  This second contest will have several significant improvements relative to the first contest.  It will have a more robust scoring function, one selected after extensive analysis of the results from the first contest as well as consultation with worldwide leaders in chess rating theory and categorical data analysis.  Further, FIDE has provided a complete dataset of multiple years of game-by-game results that were used for calculating the official FIDE ratings.  Until now it has never been possible to perform this level of analysis, because the dataset had not been assembled, and this contest website is the only place in the world where the data is available  The second contest provides more than 30 times as many games as the first contest did, and a much larger population of chess players as well, reflecting the whole distribution of player strength rather than just the fraction of top players covered by the first contest.",12 files,55.81 MB,This competition awarded ranking points,This competition counted towards tiers,Featured Prediction Competition
Stay Alert! The Ford Challenge,$950 ,12 years ago,12 years ago,['auc'],176,189,"1,402","Driving while distracted, fatigued or drowsy may lead to accidents. Activities that divert the driver's attention from the road ahead, such as engaging in a conversation with other passengers in the car, making or receiving phone calls, sending or receiving text messages, eating while driving or events outside the car may cause driver distraction. Fatigue and drowsiness can result from driving long hours or from lack of sleep.

The objective of this challenge is to design a detector/classifier that will detect whether the driver is alert or not alert, employing any combination of vehicular, environmental and driver physiological data that are acquired while driving.



The winner receives free registration to the 2011 International Joint Conference on Neural Networks (San Jose, California July 31 - August 5, 2011), which is valued at $950. The winner will also be invited to present their solution at the conference.",4 files,108.84 MB,This competition awarded ranking points,This competition counted towards tiers,Featured Prediction Competition
Predict Grant Applications,"$5,000 ",12 years ago,12 years ago,['auc'],203,216,"2,792","Around the world, the pool of funds available for research grants is steadily shrinking (in a relative sense). In Australia, success rates have fallen to 20-25 per cent, meaning that most academics are spending valuable time making applications that end up being rejected.

With this problem in mind, the University of Melbourne is hosting a competition to predict the success of grant applications. The winning model will be used by the university to predict which grant applications are likely to be successful, so that less time is wasted on applications that are unlikely to succeed. The university hopes the competition will also shed some light on what factors are important in determining whether an application will succeed.

The university has provided a dataset containing 249 features, including variables that represent the size of the grant, the general area of study and de-identified information on the investigators who are applying for the grant. Participants train their models on 8,707 grant applications made between 2004 and 2008. They then make predictions on a further 2,176 applications made in 2009 and the first half of 2010.

The winner of this competition will receive US$5,000. To be eligible for the prize, the winning method must be implementable by the University of Melbourne.  ",3 files,4.53 MB,This competition awarded ranking points,This competition counted towards tiers,Featured Prediction Competition
RTA Freeway Travel Time Prediction,"$10,000 ",12 years ago,12 years ago,['rmse'],354,376,"3,114","Forecasting travel times helps improve road safety and efficiency. Accurate predictions help commuters make informed decisions about when to travel and on what routes. This helps to lower intensity on problem arterials by encouraging motorists to use underutilised parts of the grid, and where possible, by having them select alternative times and modes of travel. 

This competition requires participants to predict travel time on Sydney's M4 freeway from past travel time observations. In addition to better informing network managers and Australian motorists, insights from the competition will improve the general efficiency of the road transport system in Sydney and increase functionality on the government's live traffic website. 

Participants in this competition are required to forecast the travel time on the M4 freeway for 15 mins, 30 mins, 45 mins, one hour, 90mins, two hours, six hours, 12 hours, 18 hours and 24 hours ahead. The NSW Roads and Traffic Authority has made 2 years' worth of historical data on road use between 2008 and 2010  available for this competition. 

$10,000 is being offered for the winning model.

The competition is being hosted by Australia's NSW Roads and Traffic Authority and is being sponsored by the NSW Department of Premier and Cabinet.",6 files,93.85 MB,This competition awarded ranking points,This competition counted towards tiers,Featured Prediction Competition
IJCNN Social Network Challenge,$950 ,12 years ago,12 years ago,['auc'],117,141,"1,124","•    Look at popular inbound nodes
•    Investigate local networks
•    Investigate mutuality of edges
•    Start reading http://en.wikipedia.org/wiki/Directed_graph
•    Also check http://www.sigkdd.org/kddcup/index.php?section=2003&method=info
•    Social Networks http://www.sciencedirect.com/science/journal/03788733",2 files,394.84 kB,This competition awarded ranking points,This competition counted towards tiers,Featured Prediction Competition
R Package Recommendation Engine,$150 ,12 years ago,12 years ago,['auc'],57,58,558,"There are many obvious ways you can try to improve on our example model. For example, you might try:
Using nearest neighbor methods over both packages and users.
Building a metric of maintainer quality and adding this predictor to the example logistic model.
Incorporating out-degree information from the graphs we're providing. The example model only uses in-degree information.
Using regularization to deal with the considerable correlation in the default predictors.",4 files,15.41 MB,This competition awarded ranking points,This competition counted towards tiers,Featured Prediction Competition
Tourism Forecasting Part Two,$500 ,12 years ago,12 years ago,['custom metric'],42,46,695,"Tourism is one of the most rapidly growing global industries and tourism
forecasting is becoming an increasingly important activity in planning
and managing the industry. The time series in this competition have
already been studied in detail in a paper by Athanasopoulos, Hyndman, Song and Wu (2010)
(to be published in the International Journal of Forecasting). For part
two of this contest, entrants must submit forecasts of the next 24 monthly and 8 quarterly  observations for 793 time series. Forecasts will be evaluated against the actual future
observations for each series.

Results for part two will be evaluated on the basis of the average MASE across all series. The overall
result will be calculated as the average MASE across all time series (from part one and part two of this competition).

The overall winner will collect $AUD500
and will be invited to contribute a discussion paper to the
International Journal of Forecasting describing their methodology and
giving their results, provided either the monthly results are better
than 1.38, the quarterly results are better than 1.43 or the yearly
results are better than 2.28. These thresholds are the best performing
methods in the analysis of these data described in Athanasopoulos et al
(2010).  In other words, the winner has to beat the best results in this
paper for at least one of the three sets of series (note that 21 teams outdid the yearly threshold in part one of this competition). It will also be
necessary that the winner be able to describe their method clearly, in
sufficient detail to enable replication and in a form suitable for the
International Journal of Forecasting. The paper would appear in the
April 2011 issue of the IJF.

Update 22 October: a correction was made to the data file. Please ensure that you build your models using tourismdata2revision2.csv.",2 files,911.34 kB,This competition awarded 0.5X ranking points,This competition counted towards tiers,Featured Prediction Competition
Tourism Forecasting Part One,$500 ,12 years ago,12 years ago,['custom metric'],55,60,477,"Tourism is one of the most rapidly growing global industries and tourism forecasting is becoming an increasingly important activity in planning and managing the industry. The time series in this competition have already been studied in detail in a paper by Athanasopoulos, Hyndman, Song and Wu (2010) (to be published in the International Journal of Forecasting). For part one of this contest, entrants must submit forecasts of the next four yearly observations. Forecasts will be tested against the actual future observations for each series.

Results for part one will be evaluated on the basis of the average MASE across all series.

Note, this is just part one of the competition. Part two will require entrants to forecast monthly and quarterly time series. The overall result will be calculated as the average MASE across all time series and across all frequencies.

The overall winner will collect $AUD500 and will be invited to contribute a discussion paper to the International Journal of Forecasting describing their methodology and giving their results, provided either the monthly results are better than 1.38, the quarterly results are better than 1.43 or the yearly results are better than 2.28. These thresholds are the best performing methods in the analysis of these data described in Athanasopoulos et al (2010).  In other words, the winner has to beat the best results in this paper for at least one of the three sets of series. It will also be necessary that the winner be able to describe their method clearly, in sufficient detail to enable replication and in a form suitable for the International Journal of Forecasting. The paper would appear in the April 2011 issue of the IJF.

Update: The team Theta Benchmark (see the leaderboard), uses the best method from Athanasopoulos, Hyndman, Song and Wu (2010).",3 files,101.23 kB,This competition awarded 0.5X ranking points,This competition counted towards tiers,Featured Prediction Competition
Chess ratings - Elo versus the Rest of the World,$617 ,12 years ago,12 years ago,['rmse'],252,262,"3,228","When predicting the outcome of chess games, you typically need two things; a rating system wherein the current ability of each player is estimated based on past results, and a model for estimating the expected score for each player, once you know their ratings.

Most rating systems use some methodology to determine initial ""seed"" ratings for the pool of players, and then update those ratings based on ongoing results.  The most famous approach is the Elo approach, where the applied change to a player's rating is proportional to the amount by which they exceed their aggregate expected score across all their recent games.  The scaling factor is known as the ""K-factor"", and for the official ratings used throughout the world, the K-factor is highest for new players and lowest for topmost players.  But there are many other approaches: the Ken Thompson approach takes each player's most recent 100 games and calculates the rating that would be most likely to lead to that performance.  The Mark Glickman approach is similar to Elo but introduces additional parameters for each player, tracking the level of confidence and level of volatility for each player's rating, and then using these parameters to determine which K-factor to apply.

The initial seed ratings are typically determined through a simultaneous calculation: a start rating is assumed for each player, then a ""performance rating"" is calculated for each player based on their results and the ratings of their opponents, and then those performance ratings are fed back into another iteration as the start ratings.  This is allowed to run until it converges upon a stable set of ratings.  This was the methodology used to calculate initial ratings for most major rating systems.  In fact this is the overall approach taken by the Jeff Sonas Chessmetrics rating calculation, and is used not just to calculate initial ratings but in fact to calculate all ratings.

There is a general convention in chess rating systems whereby the difference in two ratings is used for calculating expected score when two players face each other.  Of course it could just as well be the ratio of the two ratings, or some other more complex relationship that depends on the magnitude of the ratings and not just their relative difference.

Here are some links to articles existing on rating systems:
Elo and Ken Thompson
Glicko
Chessmetrics
Microsoft TrueSkill

Jeff Moser has a C# implementation of Elo and TrueSkill on Github. has posted a Java implementation of Glicko on Github.",4 files,1.32 MB,This competition awarded ranking points,This competition counted towards tiers,Featured Prediction Competition
INFORMS Data Mining Contest 2010,,12 years ago,12 years ago,['auc'],145,153,"1,483","Traders, analysts, investors and hedge funds are always looking for techniques to better predict stock price movements. The 2010 INFORMS Data Mining Contest takes aim at this goal, requiring participants to build models that predict the movement of stock prices over the next 60 minutes.
Knowing whether a stock will increase or decrease allows traders to make better investment decisions. Moreover, good predictive models allow traders to better understand what drives stock prices, supporting better risk management. The results of this contest could have a big impact on the finance industry.",2 files,14.72 MB,This competition awarded ranking points,This competition counted towards tiers,Featured Prediction Competition
World Cup 2010 - Confidence Challenge,$100 ,13 years ago,12 years ago,['custom metric'],63,64,63,"We are also running a World Cup 2010 - Take on the Quants Challenge.

The Confidence Challenge requires competitors to predict how far each country will progress through the World Cup and then assign a level of confidence to each prediction. A competitor's score is weighted by their level of confidence. There are more details on the submission instructions page and the evaluation page.

The competition closes just before the first game kicks off on June 11th.

What is your incentive to enter?

The winner of this challenge wins $USD100 to bet on the winner of the FIFA Golden Ball award.",,,This competition awarded 0.5X ranking points,This competition did not count towards tiers,Featured Prediction Competition
